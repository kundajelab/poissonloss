{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-06-13 17:16:19--  http://hgdownload.cse.ucsc.edu/goldenPath/mm10/bigZips/mm10.chrom.sizes\r\n",
      "Resolving hgdownload.cse.ucsc.edu (hgdownload.cse.ucsc.edu)... 128.114.119.163\r\n",
      "Connecting to hgdownload.cse.ucsc.edu (hgdownload.cse.ucsc.edu)|128.114.119.163|:80... connected.\r\n",
      "HTTP request sent, awaiting response... 200 OK\r\n",
      "Length: 1405 (1.4K) [text/plain]\r\n",
      "Saving to: ‘mm10.chrom.sizes’\r\n",
      "\r\n",
      "\r",
      "mm10.chrom.sizes      0%[                    ]       0  --.-KB/s               \r",
      "mm10.chrom.sizes    100%[===================>]   1.37K  --.-KB/s    in 0s      \r\n",
      "\r\n",
      "2020-06-13 17:16:19 (81.7 MB/s) - ‘mm10.chrom.sizes’ saved [1405/1405]\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!wget http://hgdownload.cse.ucsc.edu/goldenPath/mm10/bigZips/mm10.chrom.sizes -O mm10.chrom.sizes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get 1kb around summits\n",
    "#!zcat Oct4/idr-optimal-set.summit.bed.gz Sox2/idr-optimal-set.summit.bed.gz Nanog/idr-optimal-set.summit.bed.gz Klf4/idr-optimal-set.summit.bed.gz | perl -lane 'print $F[0].\"\\t\".(($F[1]+$F[9])).\"\\t\".(($F[1]+$F[9]))' | bedtools slop -g mm10.chrom.sizes -b 500 | perl -lane 'if ($F[2]-$F[1]==1000) {print $F[0].\"\\t\".$F[1].\"\\t\".$F[2].\"\\t1\"}' | sortBed | gzip -c > 1k_around_summits.bed.gz\n",
    "!zcat Oct4/idr-optimal-set.summit.bed.gz | perl -lane 'print $F[0].\"\\t\".(($F[1]+$F[9])).\"\\t\".(($F[1]+$F[9]))' | bedtools slop -g mm10.chrom.sizes -b 500 | perl -lane 'if ($F[2]-$F[1]==1000) {print $F[0].\"\\t\".$F[1].\"\\t\".$F[2].\"\\t1\"}' | sortBed | gzip -c > 1k_around_summits.bed.gz\n",
    "#split into train, valid, test sets\n",
    "!zcat 1k_around_summits.bed.gz | egrep -w 'chr2|chr3|chr4' | gzip -c > test_1k_around_summits.bed.gz\n",
    "!zcat 1k_around_summits.bed.gz | egrep -w 'chr1|chr8|chr9' | gzip -c > valid_1k_around_summits.bed.gz\n",
    "!zcat 1k_around_summits.bed.gz | egrep -w -v 'chr1|chr2|chr3|chr4|chr9' | gzip -c > train_1k_around_summits.bed.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /users/avanti/anaconda3/envs/py376/lib/python3.7/site-packages/tensorflow_core/python/compat/v2_compat.py:68: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "from seqdataloader.batchproducers import coordbased\n",
    "from seqdataloader.batchproducers.coordbased import coordstovals\n",
    "from seqdataloader.batchproducers.coordbased import coordbatchproducers\n",
    "from seqdataloader.batchproducers.coordbased import coordbatchtransformers\n",
    "from seqdataloader.batchproducers.coordbased.coordstovals.core import CoordsToValsJoiner\n",
    "from seqdataloader.batchproducers.coordbased.coordstovals.bigwig import AbstractCountAndProfileTransformer \n",
    "from seqdataloader.batchproducers.coordbased.coordstovals.bigwig import LogCountsPlusOne\n",
    "from seqdataloader.batchproducers.coordbased.coordstovals.bigwig import SmoothProfiles\n",
    "from seqdataloader.batchproducers.coordbased.coordstovals.bigwig import BigWigReader \n",
    "from seqdataloader.batchproducers.coordbased.coordstovals.bigwig import smooth_profiles\n",
    "from seqdataloader.batchproducers.coordbased.coordstovals.bigwig import rolling_window\n",
    "from seqdataloader.batchproducers.coordbased.coordstovals.bigwig import MultiTrackCountsAndProfile\n",
    "import keras\n",
    "import keras.layers as kl\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "import tensorflow_probability as tfp\n",
    "import numpy as np\n",
    "from seqdataloader.batchproducers.coordbased.core import Coordinates, KerasBatchGenerator, apply_mask\n",
    "from keras.models import load_model\n",
    "import os\n",
    "from keras.utils import CustomObjectScope\n",
    "from seqdataloader.batchproducers.coordbased.core import Coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Written by Žiga Avsec\n",
    "import pyBigWig\n",
    "\n",
    "#loss function\n",
    "def multinomial_nll(true_counts, logits):\n",
    "    \"\"\"Compute the multinomial negative log-likelihood\n",
    "    Args:\n",
    "      true_counts: observed count values\n",
    "      logits: predicted logit values\n",
    "    \"\"\"\n",
    "    counts_per_example = tf.reduce_sum(true_counts, axis=-1)\n",
    "    dist = tfp.distributions.Multinomial(total_count=counts_per_example,\n",
    "                                         logits=logits)\n",
    "    return (-tf.reduce_sum(dist.log_prob(true_counts)) / \n",
    "            tf.to_float(tf.shape(true_counts)[0]))\n",
    "\n",
    "#Written by Žiga Avsec\n",
    "class MultichannelMultinomialNLL(object):\n",
    "    def __init__(self, n):\n",
    "        self.__name__ = \"MultichannelMultinomialNLL\"\n",
    "        self.n = n\n",
    "\n",
    "    def __call__(self, true_counts, logits):\n",
    "        for i in range(self.n):\n",
    "            loss = multinomial_nll(true_counts[..., i], logits[..., i])\n",
    "            if i == 0:\n",
    "                total = loss\n",
    "            else:\n",
    "                total += loss\n",
    "        return total\n",
    "\n",
    "    def get_config(self):\n",
    "        return {\"n\": self.n}\n",
    "    \n",
    "#poisson loss but we take the exponent of the\n",
    "# labels to convert them from log(counts+1)\n",
    "# to counts.\n",
    "def exponentiate_label_poisson(true, preds):\n",
    "    true_counts = tf.exp(true)-1\n",
    "    return tf.keras.losses.poisson(y_true=true_counts,\n",
    "                                   y_pred=preds)\n",
    "\n",
    "\n",
    "def poisson_loglinear_loss(true, preds):\n",
    "    true_counts = tf.exp(true)-1\n",
    "    preds_counts = tf.exp(preds)\n",
    "    return tf.keras.losses.poisson(y_true=true_counts,\n",
    "                                   y_pred=preds_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Heads up: coordinates in bed file are assumed to be on the positive strand; if strand in the bed file is improtant to you, please add that feature to SimpleCoordsBatchProducer\n",
      "Heads up: coordinates in bed file are assumed to be on the positive strand; if strand in the bed file is improtant to you, please add that feature to SimpleCoordsBatchProducer\n",
      "CHIPNexus.OCT4.logcount (128, 2)\n",
      "CHIPNexus.OCT4.profile (128, 1000, 2)\n"
     ]
    }
   ],
   "source": [
    "# If we want to avoid zero-padding, then the size of the output predictions\n",
    "# will depend on the size of the input sequence supplied. We define the\n",
    "# API for an AbstractProfileModel class which returns the length of the\n",
    "# output profile in addition to returning the model, given information\n",
    "# on the size of the input sequence and the model parameters.\n",
    "        \n",
    "class AbstractProfileModel(object):\n",
    "    \n",
    "    def get_output_profile_len(self):\n",
    "        raise NotImplementedError()\n",
    "  \n",
    "    def get_model(self):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "# The architecture by Žiga Avsec involves residual connections, which means\n",
    "# that the layers being added together in an elementwise fashion need\n",
    "# to have the same dimensions. To achieve this without zero-padding, we\n",
    "# have to trim away the flanks of earlier convolutional layers. That\n",
    "# is what this function is meant to do. (Note that the original BP-net\n",
    "# architecture zero-pads; this is a modification to avoid the zero\n",
    "# padding and use information on actual sequence instead)\n",
    "\n",
    "def trim_flanks_of_conv_layer(conv_layer, output_len, width_to_trim, filters):\n",
    "    layer = keras.layers.Lambda(\n",
    "        lambda x: x[:,\n",
    "          int(0.5*(width_to_trim)):-(width_to_trim-int(0.5*(width_to_trim)))],\n",
    "        output_shape=(output_len, filters))(conv_layer)\n",
    "    return layer\n",
    "\n",
    "\n",
    "#This model architecture is based on BP-Net by Žiga Avsec\n",
    "# https://drive.google.com/file/d/1kg6Ic0-FvJtVUva9Mh3FPnOAHJcN6VB-/view\n",
    "#It has been modified for this specific use-case.\n",
    "class BPnetArch(AbstractProfileModel):   \n",
    "\n",
    "    def __init__(self, counts_loss,\n",
    "                       log_space_preds,\n",
    "                       counts_hiddenlayer_size,\n",
    "                       num_counts_hidden_layers,\n",
    "                       input_seq_len, c_task_weight, filters,\n",
    "                       n_dil_layers, conv1_kernel_size,\n",
    "                       dil_kernel_size,\n",
    "                       outconv_kernel_size, lr,\n",
    "                       seed=1234):\n",
    "        self.counts_loss = counts_loss\n",
    "        self.log_space_preds = log_space_preds\n",
    "        print(\"LOG SPACE PREDS IS SET TO\",log_space_preds)\n",
    "        self.counts_hiddenlayer_size = counts_hiddenlayer_size\n",
    "        self.num_counts_hidden_layers = num_counts_hidden_layers\n",
    "        self.input_seq_len = input_seq_len\n",
    "        self.c_task_weight = c_task_weight\n",
    "        self.filters = filters\n",
    "        self.n_dil_layers = n_dil_layers\n",
    "        self.conv1_kernel_size = conv1_kernel_size\n",
    "        self.dil_kernel_size = dil_kernel_size\n",
    "        self.outconv_kernel_size = outconv_kernel_size\n",
    "        self.lr = lr\n",
    "        self.seed = seed\n",
    "    \n",
    "    def get_embedding_len(self):\n",
    "        embedding_len = self.input_seq_len\n",
    "        embedding_len -= (self.conv1_kernel_size-1)     \n",
    "        for i in range(1, self.n_dil_layers+1):\n",
    "            dilation_rate = (2**i)\n",
    "            embedding_len -= dilation_rate*(self.dil_kernel_size-1)\n",
    "        return embedding_len\n",
    "    \n",
    "    def get_output_profile_len(self):\n",
    "        embedding_len = self.get_embedding_len()\n",
    "        out_profile_len = embedding_len - (self.outconv_kernel_size - 1)\n",
    "        return out_profile_len\n",
    "    \n",
    "    def get_keras_model(self):\n",
    "      \n",
    "        np.random.seed(self.seed)\n",
    "        tf.set_random_seed(self.seed)\n",
    "\n",
    "        out_pred_len = self.get_output_profile_len()\n",
    "        \n",
    "        #'inp' is the one-hot encoded DNA sequence input\n",
    "        inp = kl.Input(shape=(self.input_seq_len, 4), name='sequence')\n",
    "        first_conv = kl.Conv1D(filters=self.filters,\n",
    "                               kernel_size=self.conv1_kernel_size,\n",
    "                               padding='valid',\n",
    "                               activation='relu')(inp)\n",
    "        \n",
    "        #Need to keep track of the layer size for trimming purposes when\n",
    "        # we get to the residual connections.\n",
    "        \n",
    "        curr_layer_size = self.input_seq_len - (self.conv1_kernel_size-1)\n",
    "        \n",
    "        #Define input layers for the control tracks - both counts and profile\n",
    "        #Dimension is '1' for the ChIP-seq control counts because the positive\n",
    "        # and negative strands are added together\n",
    "        \n",
    "        patchcap_logcount = kl.Input(\n",
    "            shape=(2,), name=\"patchcap.logcount\")\n",
    "        #if working with raw counts, go from logcount->count\n",
    "        if (self.log_space_preds==False):\n",
    "            patchcap_count_touse = kl.Lambda(lambda x: tf.exp(x)-1)(patchcap_logcount) \n",
    "        else:\n",
    "            patchcap_count_touse = patchcap_logcount\n",
    "        \n",
    "        patchcap_profile = kl.Input(\n",
    "            shape=(out_pred_len, 2), name=\"patchcap.profile\")\n",
    "        \n",
    "        #Gather together all the tensors representing the model inputs\n",
    "        model_inputs = [\n",
    "            inp,\n",
    "            patchcap_logcount,\n",
    "            patchcap_profile\n",
    "        ]\n",
    "        \n",
    "        #Prepare the stack of dilated convolutions with residual connections\n",
    "        prev_layer = first_conv\n",
    "        for i in range(1, self.n_dil_layers + 1):\n",
    "            dilation_rate = 2**i\n",
    "            conv_output = kl.Conv1D(filters=self.filters,\n",
    "                                  kernel_size=self.dil_kernel_size,\n",
    "                                  padding='valid',\n",
    "                                  activation='relu',\n",
    "                                  dilation_rate=dilation_rate)(prev_layer)          \n",
    "            width_to_trim = dilation_rate*(self.dil_kernel_size-1)\n",
    "            curr_layer_size = (curr_layer_size - width_to_trim)\n",
    "            prev_layer = trim_flanks_of_conv_layer(\n",
    "              conv_layer=prev_layer, output_len=curr_layer_size,\n",
    "              width_to_trim=width_to_trim, filters=self.filters)\n",
    "            prev_layer = kl.merge.Add()([prev_layer, conv_output])\n",
    "\n",
    "        combined_conv = prev_layer\n",
    "\n",
    "        #gap = GlobalAveragePooling. This layer is used as input for the\n",
    "        # counts prediction tasks.\n",
    "        gap_combined_conv = kl.GlobalAvgPool1D()(combined_conv)\n",
    "        \n",
    "        lossarr = []\n",
    "        lossweightsarr = []\n",
    "        model_outputs = []\n",
    "    \n",
    "        #Define the output layers for the counts prediction tasks\n",
    "        for countouttaskname, numunits, countcontrolinp in [\n",
    "                    #(\"CHIPNexus.NANOG.logcount\", 2, patchcap_count_touse),\n",
    "                    (\"CHIPNexus.OCT4.logcount\", 2, patchcap_count_touse),\n",
    "                    #(\"CHIPNexus.KLF4.logcount\", 2, patchcap_count_touse),\n",
    "                    #(\"CHIPNexus.SOX2.logcount\", 2, patchcap_count_touse)\n",
    "                ]:\n",
    "            precount_out = kl.concatenate([gap_combined_conv,\n",
    "                                           countcontrolinp], axis=-1)\n",
    "            for i in range(self.num_counts_hidden_layers):\n",
    "                precount_out = kl.Dense(\n",
    "                    units=self.counts_hiddenlayer_size,\n",
    "                    activation=\"relu\")(precount_out)\n",
    "            count_out = kl.Dense(\n",
    "                units=numunits,\n",
    "                name=countouttaskname,\n",
    "                activation=(\"linear\" if self.log_space_preds else \"softplus\"))(\n",
    "                    precount_out)\n",
    "            model_outputs.append(count_out)\n",
    "            lossarr.append(self.counts_loss)\n",
    "            lossweightsarr.append(self.c_task_weight)\n",
    "\n",
    "        #Define the output layers for the profile prediction tasks\n",
    "        for profileouttaskname, numunits, profilecontrolinp in [\n",
    "            #(\"CHIPNexus.NANOG.profile\", 2, patchcap_profile),\n",
    "            (\"CHIPNexus.OCT4.profile\", 2, patchcap_profile),\n",
    "            #(\"CHIPNexus.KLF4.profile\", 2, patchcap_profile),\n",
    "            #(\"CHIPNexus.SOX2.profile\", 2, patchcap_profile)\n",
    "        ]:\n",
    "                                                                \n",
    "        \n",
    "            profile_out_precontrol = kl.Conv1D(\n",
    "                                      filters=numunits,\n",
    "                                      kernel_size=self.outconv_kernel_size,\n",
    "                                      padding='valid')(combined_conv)\n",
    "            profile_out = kl.Conv1D(filters=numunits, kernel_size=1, name=profileouttaskname)(kl.concatenate([profile_out_precontrol,profilecontrolinp], axis=-1))\n",
    "\n",
    "            model_outputs.append(profile_out)\n",
    "            lossarr.append(MultichannelMultinomialNLL(numunits))\n",
    "            lossweightsarr.append(1.0)\n",
    "\n",
    "        #Compile the model and return it\n",
    "        model = keras.models.Model(inputs=model_inputs, outputs=model_outputs)\n",
    "        model.compile(keras.optimizers.Adam(lr=self.lr),\n",
    "                      loss=lossarr,\n",
    "                      loss_weights=lossweightsarr)\n",
    "        return model\n",
    "\n",
    "seq_len = 1346\n",
    "out_pred_len = 1000\n",
    "#out_pred_len = modelwrapper.get_output_profile_len()\n",
    "#print(out_pred_len, seq_len-out_pred_len)   \n",
    "    \n",
    "    \n",
    "# the code below is used to prepare instances of keras.utils.Sequence that\n",
    "# can be supplied to model.fit_generator(...)\n",
    "# Note that we log-transform our counts using np.log(counts+1)\n",
    "# Also note that the profiles for the control are smoothed by windows of\n",
    "# size 1 and 50 (smoothing by a window of size 1 just returns\n",
    "# the original profile)\n",
    "\n",
    "inputs_coordstovals = coordstovals.core.CoordsToValsJoiner(\n",
    "    coordstovals_list=[\n",
    "      coordbased.coordstovals.fasta.PyfaidxCoordsToVals(\n",
    "        genome_fasta_path=\"mm10_no_alt_analysis_set_ENCODE.fasta\",\n",
    "        mode_name=\"sequence\",\n",
    "        center_size_to_use=seq_len),\n",
    "      coordstovals.bigwig.PosAndNegSeparateLogCounts(\n",
    "        counts_mode_name=\"patchcap.logcount\",\n",
    "        profile_mode_name=\"patchcap.profile\",\n",
    "        pos_strand_bigwig_path=\"patchcap/counts.pos.bw\",\n",
    "        neg_strand_bigwig_path=\"patchcap/counts.neg.bw\",\n",
    "        center_size_to_use=out_pred_len),\n",
    "    ]\n",
    ")\n",
    "    \n",
    "\n",
    "targets_coordstovals = CoordsToValsJoiner(\n",
    "    coordstovals_list=[\n",
    "      #coordstovals.bigwig.PosAndNegSeparateLogCounts(\n",
    "      #  counts_mode_name=\"CHIPNexus.NANOG.logcount\",\n",
    "      #  profile_mode_name=\"CHIPNexus.NANOG.profile\",\n",
    "      #  pos_strand_bigwig_path=\"Nanog/counts.pos.bw\",\n",
    "      #  neg_strand_bigwig_path=\"Nanog/counts.neg.bw\",\n",
    "      #  center_size_to_use=out_pred_len),\n",
    "      coordstovals.bigwig.PosAndNegSeparateLogCounts(\n",
    "        counts_mode_name=\"CHIPNexus.OCT4.logcount\",\n",
    "        profile_mode_name=\"CHIPNexus.OCT4.profile\",\n",
    "        pos_strand_bigwig_path=\"Oct4/counts.pos.bw\",\n",
    "        neg_strand_bigwig_path=\"Oct4/counts.neg.bw\",\n",
    "        center_size_to_use=out_pred_len),\n",
    "      #coordstovals.bigwig.PosAndNegSeparateLogCounts(\n",
    "      #  counts_mode_name=\"CHIPNexus.KLF4.logcount\",\n",
    "      #  profile_mode_name=\"CHIPNexus.KLF4.profile\",\n",
    "      #  pos_strand_bigwig_path=\"Klf4/counts.pos.bw\",\n",
    "      #  neg_strand_bigwig_path=\"Klf4/counts.neg.bw\",\n",
    "      #  center_size_to_use=out_pred_len),\"\"\"\n",
    "      #coordstovals.bigwig.PosAndNegSeparateLogCounts(\n",
    "      #  counts_mode_name=\"CHIPNexus.SOX2.logcount\",\n",
    "      #  profile_mode_name=\"CHIPNexus.SOX2.profile\",\n",
    "      #  pos_strand_bigwig_path=\"Sox2/counts.pos.bw\",\n",
    "      #  neg_strand_bigwig_path=\"Sox2/counts.neg.bw\",\n",
    "      #  center_size_to_use=out_pred_len),\n",
    "    ]\n",
    ")\n",
    "\n",
    "keras_train_batch_generator = KerasBatchGenerator(\n",
    "  coordsbatch_producer=coordbatchproducers.SimpleCoordsBatchProducer(\n",
    "      bed_file=\"train_1k_around_summits.bed.gz\",\n",
    "      batch_size=64,\n",
    "      shuffle_before_epoch=True, \n",
    "      seed=1234),\n",
    "  inputs_coordstovals=inputs_coordstovals,\n",
    "  targets_coordstovals=targets_coordstovals,\n",
    "  coordsbatch_transformer=\n",
    "          coordbatchtransformers.ReverseComplementAugmenter().chain(\n",
    "          coordbatchtransformers.UniformJitter(\n",
    "              maxshift=200, chromsizes_file=\"mm10.chrom.sizes\")),\n",
    ")\n",
    "\n",
    "keras_valid_batch_generator = KerasBatchGenerator(\n",
    "  coordsbatch_producer=coordbatchproducers.SimpleCoordsBatchProducer(\n",
    "            bed_file=\"valid_1k_around_summits.bed.gz\",\n",
    "            batch_size=64,\n",
    "            shuffle_before_epoch=False, \n",
    "            seed=1234),\n",
    "  inputs_coordstovals=inputs_coordstovals,\n",
    "  targets_coordstovals=targets_coordstovals\n",
    ")\n",
    "\n",
    "#As a sanity check, print out the dimensions of everything in individual batches\n",
    "sampinputs,samptargets = keras_train_batch_generator[0]\n",
    "# for key in sampinputs:\n",
    "#   print(key, sampinputs[key].shape)\n",
    "for key in samptargets:\n",
    "    print(key, samptargets[key].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extend_generator(generator):\n",
    "    samp_inputs, samp_targets = generator[0]\n",
    "    concat_inputs = OrderedDict([(key, []) for key in samp_inputs.keys()])\n",
    "    concat_targets = OrderedDict([(key, []) for key in samp_targets.keys()])\n",
    "\n",
    "    for batch_idx in range(len(generator)):\n",
    "        batch_inputs, batch_targets = generator[batch_idx]\n",
    "        for key in batch_inputs:\n",
    "            concat_inputs[key].extend(batch_inputs[key])\n",
    "        for key in batch_targets:\n",
    "            concat_targets[key].extend(batch_targets[key])\n",
    "    \n",
    "    for key in concat_inputs:\n",
    "        concat_inputs[key] = np.array(concat_inputs[key])\n",
    "    \n",
    "    for key in concat_targets:\n",
    "        concat_targets[key] = np.array(concat_targets[key])\n",
    "    \n",
    "    return (concat_inputs, concat_targets)\n",
    "\n",
    "from scipy.stats import spearmanr\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def plot_scatter(preds, targets, keys, takelogofpreds):\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=len(keys),\n",
    "                           figsize=(5*len(keys),5))\n",
    "    for i,key in enumerate(keys):\n",
    "        if takelogofpreds:\n",
    "            x = np.sum(np.log(preds[key]+1), axis=-1)\n",
    "        else:\n",
    "            x = np.sum(preds[key], axis=-1)\n",
    "        y = np.sum(targets[key], axis=-1)\n",
    "        ax[i].scatter(x, y, alpha=0.01)        \n",
    "        ax[i].set_xlabel(\"Preds: \"+key)\n",
    "        ax[i].set_ylabel(\"Targets: \"+key)\n",
    "        ax[i].plot(x, x, color='black')\n",
    "        print(key,spearmanr(x,y))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the unrolled validation set\n",
    "valid_concat_inputs, valid_concat_targets = extend_generator(keras_valid_batch_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed: 1234\n",
      "LOG SPACE PREDS IS SET TO True\n",
      "Oct4only_ctaskweight0_seed1234.h5\n",
      "Model: \"model_9\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "sequence (InputLayer)           (None, 1346, 4)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_94 (Conv1D)              (None, 1326, 64)     5440        sequence[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_61 (Lambda)              (None, 1322, 64)     0           conv1d_94[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_95 (Conv1D)              (None, 1322, 64)     12352       conv1d_94[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_61 (Add)                    (None, 1322, 64)     0           lambda_61[0][0]                  \n",
      "                                                                 conv1d_95[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_62 (Lambda)              (None, 1314, 64)     0           add_61[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_96 (Conv1D)              (None, 1314, 64)     12352       add_61[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add_62 (Add)                    (None, 1314, 64)     0           lambda_62[0][0]                  \n",
      "                                                                 conv1d_96[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_63 (Lambda)              (None, 1298, 64)     0           add_62[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_97 (Conv1D)              (None, 1298, 64)     12352       add_62[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add_63 (Add)                    (None, 1298, 64)     0           lambda_63[0][0]                  \n",
      "                                                                 conv1d_97[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_64 (Lambda)              (None, 1266, 64)     0           add_63[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_98 (Conv1D)              (None, 1266, 64)     12352       add_63[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add_64 (Add)                    (None, 1266, 64)     0           lambda_64[0][0]                  \n",
      "                                                                 conv1d_98[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_65 (Lambda)              (None, 1202, 64)     0           add_64[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_99 (Conv1D)              (None, 1202, 64)     12352       add_64[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add_65 (Add)                    (None, 1202, 64)     0           lambda_65[0][0]                  \n",
      "                                                                 conv1d_99[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_66 (Lambda)              (None, 1074, 64)     0           add_65[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_100 (Conv1D)             (None, 1074, 64)     12352       add_65[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add_66 (Add)                    (None, 1074, 64)     0           lambda_66[0][0]                  \n",
      "                                                                 conv1d_100[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_11 (Gl (None, 64)           0           add_66[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "patchcap.logcount (InputLayer)  (None, 2)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_101 (Conv1D)             (None, 1000, 2)      9602        add_66[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "patchcap.profile (InputLayer)   (None, 1000, 2)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_49 (Concatenate)    (None, 66)           0           global_average_pooling1d_11[0][0]\n",
      "                                                                 patchcap.logcount[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_50 (Concatenate)    (None, 1000, 4)      0           conv1d_101[0][0]                 \n",
      "                                                                 patchcap.profile[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "CHIPNexus.OCT4.logcount (Dense) (None, 2)            134         concatenate_49[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "CHIPNexus.OCT4.profile (Conv1D) (None, 1000, 2)      10          concatenate_50[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 89,298\n",
      "Trainable params: 89,298\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/200\n",
      "276/276 [==============================] - 97s 351ms/step - loss: 1700.8782 - CHIPNexus.OCT4.logcount_loss: 6.6618 - CHIPNexus.OCT4.profile_loss: 1700.8776 - val_loss: 1940.2539 - val_CHIPNexus.OCT4.logcount_loss: 6.5258 - val_CHIPNexus.OCT4.profile_loss: 1280.0967\n",
      "Epoch 2/200\n",
      "276/276 [==============================] - 88s 317ms/step - loss: 1645.0471 - CHIPNexus.OCT4.logcount_loss: 7.8423 - CHIPNexus.OCT4.profile_loss: 1645.0465 - val_loss: 1897.9113 - val_CHIPNexus.OCT4.logcount_loss: 9.5054 - val_CHIPNexus.OCT4.profile_loss: 1259.4553\n",
      "Epoch 3/200\n",
      "276/276 [==============================] - 90s 326ms/step - loss: 1614.6381 - CHIPNexus.OCT4.logcount_loss: 5.8040 - CHIPNexus.OCT4.profile_loss: 1614.6372 - val_loss: 1849.7936 - val_CHIPNexus.OCT4.logcount_loss: 7.9317 - val_CHIPNexus.OCT4.profile_loss: 1251.1909- los\n",
      "Epoch 4/200\n",
      "276/276 [==============================] - 85s 308ms/step - loss: 1590.1703 - CHIPNexus.OCT4.logcount_loss: 8.8848 - CHIPNexus.OCT4.profile_loss: 1590.1703 - val_loss: 1862.4799 - val_CHIPNexus.OCT4.logcount_loss: 9.4847 - val_CHIPNexus.OCT4.profile_loss: 1242.1558\n",
      "Epoch 5/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "276/276 [==============================] - 88s 319ms/step - loss: 1568.0944 - CHIPNexus.OCT4.logcount_loss: 7.0962 - CHIPNexus.OCT4.profile_loss: 1568.0935 - val_loss: 1852.7327 - val_CHIPNexus.OCT4.logcount_loss: 8.2385 - val_CHIPNexus.OCT4.profile_loss: 1232.0364\n",
      "Epoch 6/200\n",
      "276/276 [==============================] - 94s 339ms/step - loss: 1545.7920 - CHIPNexus.OCT4.logcount_loss: 10.8783 - CHIPNexus.OCT4.profile_loss: 1545.7914 - val_loss: 1892.7759 - val_CHIPNexus.OCT4.logcount_loss: 13.6696 - val_CHIPNexus.OCT4.profile_loss: 1225.7758\n",
      "Epoch 7/200\n",
      "276/276 [==============================] - 104s 376ms/step - loss: 1524.1965 - CHIPNexus.OCT4.logcount_loss: 8.0387 - CHIPNexus.OCT4.profile_loss: 1524.1963 - val_loss: 1844.6890 - val_CHIPNexus.OCT4.logcount_loss: 8.1427 - val_CHIPNexus.OCT4.profile_loss: 1216.1868\n",
      "Epoch 8/200\n",
      "276/276 [==============================] - 89s 321ms/step - loss: 1503.6295 - CHIPNexus.OCT4.logcount_loss: 8.4650 - CHIPNexus.OCT4.profile_loss: 1503.6290 - val_loss: 1877.1024 - val_CHIPNexus.OCT4.logcount_loss: 9.4737 - val_CHIPNexus.OCT4.profile_loss: 1210.5431\n",
      "Epoch 9/200\n",
      "276/276 [==============================] - 90s 326ms/step - loss: 1472.4475 - CHIPNexus.OCT4.logcount_loss: 9.4637 - CHIPNexus.OCT4.profile_loss: 1472.4470 - val_loss: 2077.2769 - val_CHIPNexus.OCT4.logcount_loss: 5.9228 - val_CHIPNexus.OCT4.profile_loss: 1291.3398\n",
      "Epoch 10/200\n",
      "276/276 [==============================] - 102s 368ms/step - loss: 1456.7043 - CHIPNexus.OCT4.logcount_loss: 8.6007 - CHIPNexus.OCT4.profile_loss: 1456.7039 - val_loss: 1888.5951 - val_CHIPNexus.OCT4.logcount_loss: 9.4439 - val_CHIPNexus.OCT4.profile_loss: 1202.6689\n",
      "Epoch 11/200\n",
      "276/276 [==============================] - 99s 360ms/step - loss: 1436.9813 - CHIPNexus.OCT4.logcount_loss: 6.3302 - CHIPNexus.OCT4.profile_loss: 1436.9813 - val_loss: 1922.3663 - val_CHIPNexus.OCT4.logcount_loss: 5.2474 - val_CHIPNexus.OCT4.profile_loss: 1200.2365\n",
      "Epoch 12/200\n",
      "276/276 [==============================] - 84s 306ms/step - loss: 1403.8974 - CHIPNexus.OCT4.logcount_loss: 5.6877 - CHIPNexus.OCT4.profile_loss: 1403.8970 - val_loss: 1898.1560 - val_CHIPNexus.OCT4.logcount_loss: 5.4616 - val_CHIPNexus.OCT4.profile_loss: 1192.41033s - loss\n",
      "Epoch 13/200\n",
      "276/276 [==============================] - 82s 296ms/step - loss: 1340.6083 - CHIPNexus.OCT4.logcount_loss: 5.5521 - CHIPNexus.OCT4.profile_loss: 1340.6086 - val_loss: 2264.9543 - val_CHIPNexus.OCT4.logcount_loss: 5.2772 - val_CHIPNexus.OCT4.profile_loss: 1370.7900\n",
      "Epoch 14/200\n",
      "276/276 [==============================] - 82s 295ms/step - loss: 1297.0443 - CHIPNexus.OCT4.logcount_loss: 6.1479 - CHIPNexus.OCT4.profile_loss: 1297.0442 - val_loss: 1952.2244 - val_CHIPNexus.OCT4.logcount_loss: 10.6062 - val_CHIPNexus.OCT4.profile_loss: 1188.2379\n",
      "Epoch 15/200\n",
      "276/276 [==============================] - 83s 302ms/step - loss: 1192.9855 - CHIPNexus.OCT4.logcount_loss: 7.3765 - CHIPNexus.OCT4.profile_loss: 1192.9851 - val_loss: 1903.6102 - val_CHIPNexus.OCT4.logcount_loss: 5.3947 - val_CHIPNexus.OCT4.profile_loss: 1162.2523\n",
      "Epoch 16/200\n",
      "276/276 [==============================] - 86s 313ms/step - loss: 1130.8428 - CHIPNexus.OCT4.logcount_loss: 10.0548 - CHIPNexus.OCT4.profile_loss: 1130.8428 - val_loss: 1890.4653 - val_CHIPNexus.OCT4.logcount_loss: 7.9693 - val_CHIPNexus.OCT4.profile_loss: 1156.9762\n",
      "Epoch 17/200\n",
      "276/276 [==============================] - 81s 295ms/step - loss: 1146.2299 - CHIPNexus.OCT4.logcount_loss: 2.4281 - CHIPNexus.OCT4.profile_loss: 1146.2297 - val_loss: 1888.4850 - val_CHIPNexus.OCT4.logcount_loss: 2.1519 - val_CHIPNexus.OCT4.profile_loss: 1152.5208\n",
      "val_loss 1844.68896484375\n",
      "val_CHIPNexus.OCT4.logcount_loss 8.142650604248047\n",
      "val_CHIPNexus.OCT4.profile_loss 1216.186767578125\n",
      "loss 1524.3099340971007\n",
      "CHIPNexus.OCT4.logcount_loss 8.038746\n",
      "CHIPNexus.OCT4.profile_loss 1524.1963\n",
      "Seed: 1234\n",
      "LOG SPACE PREDS IS SET TO True\n",
      "Oct4only_ctaskweight125_seed1234.h5\n",
      "Model: \"model_10\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "sequence (InputLayer)           (None, 1346, 4)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_102 (Conv1D)             (None, 1326, 64)     5440        sequence[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_67 (Lambda)              (None, 1322, 64)     0           conv1d_102[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_103 (Conv1D)             (None, 1322, 64)     12352       conv1d_102[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_67 (Add)                    (None, 1322, 64)     0           lambda_67[0][0]                  \n",
      "                                                                 conv1d_103[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_68 (Lambda)              (None, 1314, 64)     0           add_67[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_104 (Conv1D)             (None, 1314, 64)     12352       add_67[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add_68 (Add)                    (None, 1314, 64)     0           lambda_68[0][0]                  \n",
      "                                                                 conv1d_104[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_69 (Lambda)              (None, 1298, 64)     0           add_68[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_105 (Conv1D)             (None, 1298, 64)     12352       add_68[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add_69 (Add)                    (None, 1298, 64)     0           lambda_69[0][0]                  \n",
      "                                                                 conv1d_105[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_70 (Lambda)              (None, 1266, 64)     0           add_69[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_106 (Conv1D)             (None, 1266, 64)     12352       add_69[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add_70 (Add)                    (None, 1266, 64)     0           lambda_70[0][0]                  \n",
      "                                                                 conv1d_106[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_71 (Lambda)              (None, 1202, 64)     0           add_70[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_107 (Conv1D)             (None, 1202, 64)     12352       add_70[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add_71 (Add)                    (None, 1202, 64)     0           lambda_71[0][0]                  \n",
      "                                                                 conv1d_107[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_72 (Lambda)              (None, 1074, 64)     0           add_71[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_108 (Conv1D)             (None, 1074, 64)     12352       add_71[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add_72 (Add)                    (None, 1074, 64)     0           lambda_72[0][0]                  \n",
      "                                                                 conv1d_108[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_12 (Gl (None, 64)           0           add_72[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "patchcap.logcount (InputLayer)  (None, 2)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_109 (Conv1D)             (None, 1000, 2)      9602        add_72[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "patchcap.profile (InputLayer)   (None, 1000, 2)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_51 (Concatenate)    (None, 66)           0           global_average_pooling1d_12[0][0]\n",
      "                                                                 patchcap.logcount[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_52 (Concatenate)    (None, 1000, 4)      0           conv1d_109[0][0]                 \n",
      "                                                                 patchcap.profile[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "CHIPNexus.OCT4.logcount (Dense) (None, 2)            134         concatenate_51[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "CHIPNexus.OCT4.profile (Conv1D) (None, 1000, 2)      10          concatenate_52[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 89,298\n",
      "Trainable params: 89,298\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "276/276 [==============================] - 80s 290ms/step - loss: 1735.7508 - CHIPNexus.OCT4.logcount_loss: 0.3845 - CHIPNexus.OCT4.profile_loss: 1687.6826 - val_loss: 2102.0823 - val_CHIPNexus.OCT4.logcount_loss: 0.2913 - val_CHIPNexus.OCT4.profile_loss: 1286.1918\n",
      "Epoch 2/200\n",
      "276/276 [==============================] - 86s 312ms/step - loss: 1655.9860 - CHIPNexus.OCT4.logcount_loss: 0.3092 - CHIPNexus.OCT4.profile_loss: 1617.3412 - val_loss: 2103.2781 - val_CHIPNexus.OCT4.logcount_loss: 0.2841 - val_CHIPNexus.OCT4.profile_loss: 1259.6021\n",
      "Epoch 3/200\n",
      "276/276 [==============================] - 82s 298ms/step - loss: 1614.2273 - CHIPNexus.OCT4.logcount_loss: 0.3108 - CHIPNexus.OCT4.profile_loss: 1575.3774 - val_loss: 2085.8052 - val_CHIPNexus.OCT4.logcount_loss: 0.2792 - val_CHIPNexus.OCT4.profile_loss: 1239.8694\n",
      "Epoch 4/200\n",
      "276/276 [==============================] - 81s 294ms/step - loss: 1582.0063 - CHIPNexus.OCT4.logcount_loss: 0.2935 - CHIPNexus.OCT4.profile_loss: 1545.3149 - val_loss: 2105.9355 - val_CHIPNexus.OCT4.logcount_loss: 0.3133 - val_CHIPNexus.OCT4.profile_loss: 1237.3264\n",
      "Epoch 5/200\n",
      "276/276 [==============================] - 85s 309ms/step - loss: 1556.3973 - CHIPNexus.OCT4.logcount_loss: 0.2961 - CHIPNexus.OCT4.profile_loss: 1519.3799 - val_loss: 2028.1674 - val_CHIPNexus.OCT4.logcount_loss: 0.2822 - val_CHIPNexus.OCT4.profile_loss: 1218.8096\n",
      "Epoch 6/200\n",
      "276/276 [==============================] - 86s 312ms/step - loss: 1535.1577 - CHIPNexus.OCT4.logcount_loss: 0.2939 - CHIPNexus.OCT4.profile_loss: 1498.4235 - val_loss: 2005.9686 - val_CHIPNexus.OCT4.logcount_loss: 0.2646 - val_CHIPNexus.OCT4.profile_loss: 1210.1396\n",
      "Epoch 7/200\n",
      "276/276 [==============================] - 81s 293ms/step - loss: 1507.9870 - CHIPNexus.OCT4.logcount_loss: 0.2815 - CHIPNexus.OCT4.profile_loss: 1472.7992 - val_loss: 2153.5483 - val_CHIPNexus.OCT4.logcount_loss: 0.2903 - val_CHIPNexus.OCT4.profile_loss: 1239.4480s: 1057.0947 - CHIPNexus.OCT4.logcount_loss: 0.2725 - CHIPNexus.OCT4.profi - ETA: 46s - loss: 1055.1491 - CHIPNexus.OCT4.logcount_loss: 0.2705 - CHIPNexus.OCT4.profile_loss: 1021.3 - ETA: 45s - lo - ETA: 23s - loss: 1063.1444 - CHIPNexus.OCT4.logcount_loss: 0.2763 - CHIPNexus.OCT4.profile_loss: 1028.611 - ETA: 23s - loss: 1062.5074 - CHIPNexus.OCT4.logcount_loss: 0 - ETA: 12s - loss\n",
      "Epoch 8/200\n",
      "276/276 [==============================] - 88s 317ms/step - loss: 1492.1435 - CHIPNexus.OCT4.logcount_loss: 0.2979 - CHIPNexus.OCT4.profile_loss: 1454.8998 - val_loss: 1983.3385 - val_CHIPNexus.OCT4.logcount_loss: 0.2628 - val_CHIPNexus.OCT4.profile_loss: 1195.5334\n",
      "Epoch 9/200\n",
      "276/276 [==============================] - 85s 307ms/step - loss: 1453.7277 - CHIPNexus.OCT4.logcount_loss: 0.2711 - CHIPNexus.OCT4.profile_loss: 1419.8407 - val_loss: 2211.0420 - val_CHIPNexus.OCT4.logcount_loss: 0.2937 - val_CHIPNexus.OCT4.profile_loss: 1287.7543\n",
      "Epoch 10/200\n",
      "276/276 [==============================] - 85s 307ms/step - loss: 1439.6051 - CHIPNexus.OCT4.logcount_loss: 0.3491 - CHIPNexus.OCT4.profile_loss: 1395.9653 - val_loss: 2037.4677 - val_CHIPNexus.OCT4.logcount_loss: 0.2643 - val_CHIPNexus.OCT4.profile_loss: 1183.1704\n",
      "Epoch 11/200\n",
      "276/276 [==============================] - 86s 311ms/step - loss: 1450.3006 - CHIPNexus.OCT4.logcount_loss: 0.2992 - CHIPNexus.OCT4.profile_loss: 1412.9039 - val_loss: 2056.1223 - val_CHIPNexus.OCT4.logcount_loss: 0.2775 - val_CHIPNexus.OCT4.profile_loss: 1193.4586\n",
      "Epoch 12/200\n",
      "276/276 [==============================] - 83s 302ms/step - loss: 1410.7804 - CHIPNexus.OCT4.logcount_loss: 0.3127 - CHIPNexus.OCT4.profile_loss: 1371.6951 - val_loss: 2051.3862 - val_CHIPNexus.OCT4.logcount_loss: 0.2699 - val_CHIPNexus.OCT4.profile_loss: 1228.2690\n",
      "Epoch 13/200\n",
      "276/276 [==============================] - 89s 323ms/step - loss: 1354.8836 - CHIPNexus.OCT4.logcount_loss: 0.2944 - CHIPNexus.OCT4.profile_loss: 1318.0874 - val_loss: 2030.2009 - val_CHIPNexus.OCT4.logcount_loss: 0.2976 - val_CHIPNexus.OCT4.profile_loss: 1168.6416\n",
      "Epoch 14/200\n",
      "  4/276 [..............................] - ETA: 26s - loss: 1204.0151 - CHIPNexus.OCT4.logcount_loss: 0.4297 - CHIPNexus.OCT4.profile_loss: 1150.3073"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/avanti/anaconda3/envs/py376/lib/python3.7/site-packages/keras/callbacks/callbacks.py:95: RuntimeWarning: Method (on_train_batch_end) is slow compared to the batch update (0.118877). Check your callbacks.\n",
      "  % (hook_name, delta_t_median), RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "276/276 [==============================] - 89s 324ms/step - loss: 1390.4576 - CHIPNexus.OCT4.logcount_loss: 0.2989 - CHIPNexus.OCT4.profile_loss: 1353.0898 - val_loss: 2012.8521 - val_CHIPNexus.OCT4.logcount_loss: 0.2685 - val_CHIPNexus.OCT4.profile_loss: 1173.1462\n",
      "Epoch 15/200\n",
      "  3/276 [..............................] - ETA: 36s - loss: 1074.1100 - CHIPNexus.OCT4.logcount_loss: 0.2613 - CHIPNexus.OCT4.profile_loss: 1041.4535"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/avanti/anaconda3/envs/py376/lib/python3.7/site-packages/keras/callbacks/callbacks.py:95: RuntimeWarning: Method (on_train_batch_end) is slow compared to the batch update (0.109104). Check your callbacks.\n",
      "  % (hook_name, delta_t_median), RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "276/276 [==============================] - 85s 307ms/step - loss: 1323.3508 - CHIPNexus.OCT4.logcount_loss: 0.2934 - CHIPNexus.OCT4.profile_loss: 1286.6721 - val_loss: 2074.8000 - val_CHIPNexus.OCT4.logcount_loss: 0.2749 - val_CHIPNexus.OCT4.profile_loss: 1175.2646\n",
      "Epoch 16/200\n",
      "276/276 [==============================] - 82s 298ms/step - loss: 1198.4547 - CHIPNexus.OCT4.logcount_loss: 0.2846 - CHIPNexus.OCT4.profile_loss: 1162.8805 - val_loss: 1998.9551 - val_CHIPNexus.OCT4.logcount_loss: 0.2794 - val_CHIPNexus.OCT4.profile_loss: 1151.6464\n",
      "Epoch 17/200\n",
      "276/276 [==============================] - 87s 314ms/step - loss: 1287.5713 - CHIPNexus.OCT4.logcount_loss: 0.3099 - CHIPNexus.OCT4.profile_loss: 1248.8331 - val_loss: 2115.8938 - val_CHIPNexus.OCT4.logcount_loss: 0.2744 - val_CHIPNexus.OCT4.profile_loss: 1210.7144\n",
      "Epoch 18/200\n",
      "276/276 [==============================] - 86s 311ms/step - loss: 1234.2994 - CHIPNexus.OCT4.logcount_loss: 0.2902 - CHIPNexus.OCT4.profile_loss: 1198.0212 - val_loss: 1988.3037 - val_CHIPNexus.OCT4.logcount_loss: 0.2682 - val_CHIPNexus.OCT4.profile_loss: 1150.4908\n",
      "val_loss 1983.3385009765625\n",
      "val_CHIPNexus.OCT4.logcount_loss 0.26283252239227295\n",
      "val_CHIPNexus.OCT4.profile_loss 1195.533447265625\n",
      "loss 1492.2583555017095\n",
      "CHIPNexus.OCT4.logcount_loss 0.29794502\n",
      "CHIPNexus.OCT4.profile_loss 1454.8998\n",
      "Seed: 1234\n",
      "LOG SPACE PREDS IS SET TO True\n",
      "Oct4only_ctaskweight250_seed1234.h5\n",
      "Model: \"model_11\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "sequence (InputLayer)           (None, 1346, 4)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_110 (Conv1D)             (None, 1326, 64)     5440        sequence[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_73 (Lambda)              (None, 1322, 64)     0           conv1d_110[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_111 (Conv1D)             (None, 1322, 64)     12352       conv1d_110[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_73 (Add)                    (None, 1322, 64)     0           lambda_73[0][0]                  \n",
      "                                                                 conv1d_111[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_74 (Lambda)              (None, 1314, 64)     0           add_73[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_112 (Conv1D)             (None, 1314, 64)     12352       add_73[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add_74 (Add)                    (None, 1314, 64)     0           lambda_74[0][0]                  \n",
      "                                                                 conv1d_112[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_75 (Lambda)              (None, 1298, 64)     0           add_74[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_113 (Conv1D)             (None, 1298, 64)     12352       add_74[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add_75 (Add)                    (None, 1298, 64)     0           lambda_75[0][0]                  \n",
      "                                                                 conv1d_113[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_76 (Lambda)              (None, 1266, 64)     0           add_75[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_114 (Conv1D)             (None, 1266, 64)     12352       add_75[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add_76 (Add)                    (None, 1266, 64)     0           lambda_76[0][0]                  \n",
      "                                                                 conv1d_114[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_77 (Lambda)              (None, 1202, 64)     0           add_76[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_115 (Conv1D)             (None, 1202, 64)     12352       add_76[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add_77 (Add)                    (None, 1202, 64)     0           lambda_77[0][0]                  \n",
      "                                                                 conv1d_115[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_78 (Lambda)              (None, 1074, 64)     0           add_77[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_116 (Conv1D)             (None, 1074, 64)     12352       add_77[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add_78 (Add)                    (None, 1074, 64)     0           lambda_78[0][0]                  \n",
      "                                                                 conv1d_116[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_13 (Gl (None, 64)           0           add_78[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "patchcap.logcount (InputLayer)  (None, 2)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_117 (Conv1D)             (None, 1000, 2)      9602        add_78[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "patchcap.profile (InputLayer)   (None, 1000, 2)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_53 (Concatenate)    (None, 66)           0           global_average_pooling1d_13[0][0]\n",
      "                                                                 patchcap.logcount[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_54 (Concatenate)    (None, 1000, 4)      0           conv1d_117[0][0]                 \n",
      "                                                                 patchcap.profile[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "CHIPNexus.OCT4.logcount (Dense) (None, 2)            134         concatenate_53[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "CHIPNexus.OCT4.profile (Conv1D) (None, 1000, 2)      10          concatenate_54[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 89,298\n",
      "Trainable params: 89,298\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "276/276 [==============================] - 85s 306ms/step - loss: 1838.9985 - CHIPNexus.OCT4.logcount_loss: 0.3863 - CHIPNexus.OCT4.profile_loss: 1742.4230 - val_loss: 2388.1123 - val_CHIPNexus.OCT4.logcount_loss: 0.3491 - val_CHIPNexus.OCT4.profile_loss: 1329.2562loss: 1855.5873 - CHIPNexus.OCT4.logcount_loss: 0.3881 - CHIPNexus.OCT4.profile_l\n",
      "Epoch 2/200\n",
      "276/276 [==============================] - 83s 300ms/step - loss: 1763.1194 - CHIPNexus.OCT4.logcount_loss: 0.3108 - CHIPNexus.OCT4.profile_loss: 1685.4159 - val_loss: 2175.5854 - val_CHIPNexus.OCT4.logcount_loss: 0.3882 - val_CHIPNexus.OCT4.profile_loss: 1296.9340\n",
      "Epoch 3/200\n",
      "276/276 [==============================] - 82s 296ms/step - loss: 1711.9166 - CHIPNexus.OCT4.logcount_loss: 0.3065 - CHIPNexus.OCT4.profile_loss: 1635.2971 - val_loss: 2305.4456 - val_CHIPNexus.OCT4.logcount_loss: 0.3139 - val_CHIPNexus.OCT4.profile_loss: 1279.9650ount_loss: 0.3120 - CHIPNexus.OCT4.profile_lo - ETA: 18s - loss: 1140.4149 - CHIPNexus.OCT4.logcount_loss: 0.3143 - CHIPNexus.OC - ETA: 12s - loss: 1136.7021 - CHIPNexus.OCT4.logcount_loss: 0.3092 - CHIPNexus.OCT4.profile_loss: - ETA: 9s - loss: 1136.1230 - \n",
      "Epoch 4/200\n",
      "276/276 [==============================] - 82s 298ms/step - loss: 1672.1375 - CHIPNexus.OCT4.logcount_loss: 0.2988 - CHIPNexus.OCT4.profile_loss: 1597.4315 - val_loss: 2196.6455 - val_CHIPNexus.OCT4.logcount_loss: 0.2730 - val_CHIPNexus.OCT4.profile_loss: 1259.1299s - loss: 2934.7231 - CHIPNexus.OCT4.logcount_loss: 0.3134 - CHIPNexus.OCT4.profile_loss: 2856.371 - ETA:  - ETA: 22s - loss: 1942.3500 - CHIPNexus.OCT4.logcount_loss: 0.3008 - CH - ETA: 13s - lo\n",
      "Epoch 5/200\n",
      "276/276 [==============================] - 81s 295ms/step - loss: 1646.6543 - CHIPNexus.OCT4.logcount_loss: 0.2918 - CHIPNexus.OCT4.profile_loss: 1573.7064 - val_loss: 2159.1372 - val_CHIPNexus.OCT4.logcount_loss: 0.2721 - val_CHIPNexus.OCT4.profile_loss: 1247.6193\n",
      "Epoch 6/200\n",
      "276/276 [==============================] - 87s 314ms/step - loss: 1621.8215 - CHIPNexus.OCT4.logcount_loss: 0.2920 - CHIPNexus.OCT4.profile_loss: 1548.8275 - val_loss: 2142.9451 - val_CHIPNexus.OCT4.logcount_loss: 0.2628 - val_CHIPNexus.OCT4.profile_loss: 1241.6465\n",
      "Epoch 7/200\n",
      "276/276 [==============================] - 86s 313ms/step - loss: 1591.3491 - CHIPNexus.OCT4.logcount_loss: 0.2772 - CHIPNexus.OCT4.profile_loss: 1522.0415 - val_loss: 2249.3950 - val_CHIPNexus.OCT4.logcount_loss: 0.2916 - val_CHIPNexus.OCT4.profile_loss: 1252.7574\n",
      "Epoch 8/200\n",
      "276/276 [==============================] - 85s 309ms/step - loss: 1570.1427 - CHIPNexus.OCT4.logcount_loss: 0.2782 - CHIPNexus.OCT4.profile_loss: 1500.5951 - val_loss: 2227.7998 - val_CHIPNexus.OCT4.logcount_loss: 0.2894 - val_CHIPNexus.OCT4.profile_loss: 1243.5110\n",
      "Epoch 9/200\n",
      "276/276 [==============================] - 84s 303ms/step - loss: 1547.7113 - CHIPNexus.OCT4.logcount_loss: 0.2730 - CHIPNexus.OCT4.profile_loss: 1479.4629 - val_loss: 2180.2075 - val_CHIPNexus.OCT4.logcount_loss: 0.2685 - val_CHIPNexus.OCT4.profile_loss: 1219.9470- CHIPNexus.OCT4.profile_loss: 1687.878 - ETA: 23s - loss: 1753.9593 - CHIPNexus.OCT4 - ETA: 8s - loss: 1607.3419 - CHIPNexus.OCT\n",
      "Epoch 10/200\n",
      "276/276 [==============================] - 81s 293ms/step - loss: 1528.8938 - CHIPNexus.OCT4.logcount_loss: 0.2805 - CHIPNexus.OCT4.profile_loss: 1458.7649 - val_loss: 2057.0325 - val_CHIPNexus.OCT4.logcount_loss: 0.3051 - val_CHIPNexus.OCT4.profile_loss: 1214.2076\n",
      "Epoch 11/200\n",
      "276/276 [==============================] - 82s 296ms/step - loss: 1506.5972 - CHIPNexus.OCT4.logcount_loss: 0.2868 - CHIPNexus.OCT4.profile_loss: 1434.9023 - val_loss: 2128.3823 - val_CHIPNexus.OCT4.logcount_loss: 0.2623 - val_CHIPNexus.OCT4.profile_loss: 1208.2728ss: 1100.5641 - CHIPNexus.OCT4.logcount_loss: - ETA: 27s - loss: 1765.4223 - CHIPNexus.OCT4.logcount_loss: 0.2964  - ETA: 17s - loss - ETA: 2s - loss: 1522.4699 - CHIPNexus.OCT4.logcount_loss: 0.2882 - CHIPNexus.OCT4.pro\n",
      "Epoch 12/200\n",
      "276/276 [==============================] - 87s 314ms/step - loss: 1475.1638 - CHIPNexus.OCT4.logcount_loss: 0.2966 - CHIPNexus.OCT4.profile_loss: 1401.0209 - val_loss: 2186.0383 - val_CHIPNexus.OCT4.logcount_loss: 0.3003 - val_CHIPNexus.OCT4.profile_loss: 1211.89038.1114 - CHIPNexus. - ETA: 16s\n",
      "Epoch 13/200\n",
      "276/276 [==============================] - 87s 316ms/step - loss: 1447.5383 - CHIPNexus.OCT4.logcount_loss: 0.3168 - CHIPNexus.OCT4.profile_loss: 1368.3289 - val_loss: 2101.5942 - val_CHIPNexus.OCT4.logcount_loss: 0.3154 - val_CHIPNexus.OCT4.profile_loss: 1202.4758xus.OCT4.profile_loss: 137\n",
      "Epoch 14/200\n",
      "276/276 [==============================] - 82s 298ms/step - loss: 1456.4454 - CHIPNexus.OCT4.logcount_loss: 0.3015 - CHIPNexus.OCT4.profile_loss: 1381.0789 - val_loss: 2171.4905 - val_CHIPNexus.OCT4.logcount_loss: 0.2793 - val_CHIPNexus.OCT4.profile_loss: 1214.1084 0.3018 - CHIPNexus.OCT4.profi - ETA: 4s - loss: 1476.0729 - CHIPNexus.OCT4.logcount_loss: 0.3022 - CHIP\n",
      "Epoch 15/200\n",
      "276/276 [==============================] - 88s 317ms/step - loss: 1363.9626 - CHIPNexus.OCT4.logcount_loss: 0.3088 - CHIPNexus.OCT4.profile_loss: 1286.7587 - val_loss: 2093.3938 - val_CHIPNexus.OCT4.logcount_loss: 0.2728 - val_CHIPNexus.OCT4.profile_loss: 1198.7698 - CHIPNexus.OCT4.profile_loss: 13 - ETA: 20s - loss: 1441.6164 - CHIPNexus.OCT4.logcount_loss: 0.3173 - CHIPNexus.OCT4.profile_loss: 1362 - ETA: 18s - loss: 1436.0450 - CHIPNexu - ETA: 5s - loss: 1380.2366 - CHIPNexus.OCT4.logcount_loss: 0.3106 - \n",
      "Epoch 16/200\n",
      "  3/276 [..............................] - ETA: 34s - loss: 1075.4670 - CHIPNexus.OCT4.logcount_loss: 0.2525 - CHIPNexus.OCT4.profile_loss: 1012.3416"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/avanti/anaconda3/envs/py376/lib/python3.7/site-packages/keras/callbacks/callbacks.py:95: RuntimeWarning: Method (on_train_batch_end) is slow compared to the batch update (0.137460). Check your callbacks.\n",
      "  % (hook_name, delta_t_median), RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "276/276 [==============================] - 81s 293ms/step - loss: 1256.1681 - CHIPNexus.OCT4.logcount_loss: 0.2895 - CHIPNexus.OCT4.profile_loss: 1183.7936 - val_loss: 2126.6646 - val_CHIPNexus.OCT4.logcount_loss: 0.2777 - val_CHIPNexus.OCT4.profile_loss: 1172.1226\n",
      "Epoch 17/200\n",
      "276/276 [==============================] - 83s 302ms/step - loss: 1422.1985 - CHIPNexus.OCT4.logcount_loss: 0.2921 - CHIPNexus.OCT4.profile_loss: 1349.1785 - val_loss: 2113.6155 - val_CHIPNexus.OCT4.logcount_loss: 0.2660 - val_CHIPNexus.OCT4.profile_loss: 1172.2178\n",
      "Epoch 18/200\n",
      "276/276 [==============================] - 88s 317ms/step - loss: 1255.7532 - CHIPNexus.OCT4.logcount_loss: 0.2798 - CHIPNexus.OCT4.profile_loss: 1185.8114 - val_loss: 2124.5115 - val_CHIPNexus.OCT4.logcount_loss: 0.2716 - val_CHIPNexus.OCT4.profile_loss: 1165.2509\n",
      "Epoch 19/200\n",
      "276/276 [==============================] - 89s 324ms/step - loss: 1242.1014 - CHIPNexus.OCT4.logcount_loss: 0.2826 - CHIPNexus.OCT4.profile_loss: 1171.4426 - val_loss: 2129.1123 - val_CHIPNexus.OCT4.logcount_loss: 0.2682 - val_CHIPNexus.OCT4.profile_loss: 1161.7562\n",
      "Epoch 20/200\n",
      "276/276 [==============================] - 85s 308ms/step - loss: 1231.6103 - CHIPNexus.OCT4.logcount_loss: 0.2835 - CHIPNexus.OCT4.profile_loss: 1160.7234 - val_loss: 2111.9717 - val_CHIPNexus.OCT4.logcount_loss: 0.2678 - val_CHIPNexus.OCT4.profile_loss: 1152.0067\n",
      "val_loss 2057.032470703125\n",
      "val_CHIPNexus.OCT4.logcount_loss 0.3050827383995056\n",
      "val_CHIPNexus.OCT4.profile_loss 1214.2076416015625\n",
      "loss 1529.0364225454432\n",
      "CHIPNexus.OCT4.logcount_loss 0.28051206\n",
      "CHIPNexus.OCT4.profile_loss 1458.7649\n",
      "Seed: 1234\n",
      "LOG SPACE PREDS IS SET TO True\n",
      "Oct4only_ctaskweight500_seed1234.h5\n",
      "Model: \"model_12\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "sequence (InputLayer)           (None, 1346, 4)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_118 (Conv1D)             (None, 1326, 64)     5440        sequence[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_79 (Lambda)              (None, 1322, 64)     0           conv1d_118[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_119 (Conv1D)             (None, 1322, 64)     12352       conv1d_118[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_79 (Add)                    (None, 1322, 64)     0           lambda_79[0][0]                  \n",
      "                                                                 conv1d_119[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_80 (Lambda)              (None, 1314, 64)     0           add_79[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_120 (Conv1D)             (None, 1314, 64)     12352       add_79[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add_80 (Add)                    (None, 1314, 64)     0           lambda_80[0][0]                  \n",
      "                                                                 conv1d_120[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_81 (Lambda)              (None, 1298, 64)     0           add_80[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_121 (Conv1D)             (None, 1298, 64)     12352       add_80[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add_81 (Add)                    (None, 1298, 64)     0           lambda_81[0][0]                  \n",
      "                                                                 conv1d_121[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_82 (Lambda)              (None, 1266, 64)     0           add_81[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_122 (Conv1D)             (None, 1266, 64)     12352       add_81[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add_82 (Add)                    (None, 1266, 64)     0           lambda_82[0][0]                  \n",
      "                                                                 conv1d_122[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_83 (Lambda)              (None, 1202, 64)     0           add_82[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_123 (Conv1D)             (None, 1202, 64)     12352       add_82[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add_83 (Add)                    (None, 1202, 64)     0           lambda_83[0][0]                  \n",
      "                                                                 conv1d_123[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_84 (Lambda)              (None, 1074, 64)     0           add_83[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_124 (Conv1D)             (None, 1074, 64)     12352       add_83[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add_84 (Add)                    (None, 1074, 64)     0           lambda_84[0][0]                  \n",
      "                                                                 conv1d_124[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_14 (Gl (None, 64)           0           add_84[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "patchcap.logcount (InputLayer)  (None, 2)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_125 (Conv1D)             (None, 1000, 2)      9602        add_84[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "patchcap.profile (InputLayer)   (None, 1000, 2)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_55 (Concatenate)    (None, 66)           0           global_average_pooling1d_14[0][0]\n",
      "                                                                 patchcap.logcount[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_56 (Concatenate)    (None, 1000, 4)      0           conv1d_125[0][0]                 \n",
      "                                                                 patchcap.profile[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "CHIPNexus.OCT4.logcount (Dense) (None, 2)            134         concatenate_55[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "CHIPNexus.OCT4.profile (Conv1D) (None, 1000, 2)      10          concatenate_56[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 89,298\n",
      "Trainable params: 89,298\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "276/276 [==============================] - 82s 296ms/step - loss: 1915.9733 - CHIPNexus.OCT4.logcount_loss: 0.3884 - CHIPNexus.OCT4.profile_loss: 1721.7809 - val_loss: 2622.3879 - val_CHIPNexus.OCT4.logcount_loss: 0.2923 - val_CHIPNexus.OCT4.profile_loss: 1327.4126136 - CHIPNexus.OCT4.profile_loss: 1 - E\n",
      "Epoch 2/200\n",
      "276/276 [==============================] - 83s 300ms/step - loss: 1821.1559 - CHIPNexus.OCT4.logcount_loss: 0.3087 - CHIPNexus.OCT4.profile_loss: 1666.8104 - val_loss: 2610.1018 - val_CHIPNexus.OCT4.logcount_loss: 0.2944 - val_CHIPNexus.OCT4.profile_loss: 1299.7089\n",
      "Epoch 3/200\n",
      "276/276 [==============================] - 83s 301ms/step - loss: 1778.4933 - CHIPNexus.OCT4.logcount_loss: 0.3067 - CHIPNexus.OCT4.profile_loss: 1625.1366 - val_loss: 2380.0127 - val_CHIPNexus.OCT4.logcount_loss: 0.3147 - val_CHIPNexus.OCT4.profile_loss: 1269.5767\n",
      "Epoch 4/200\n",
      "276/276 [==============================] - 83s 301ms/step - loss: 1740.3435 - CHIPNexus.OCT4.logcount_loss: 0.2999 - CHIPNexus.OCT4.profile_loss: 1590.3988 - val_loss: 2539.0291 - val_CHIPNexus.OCT4.logcount_loss: 0.2927 - val_CHIPNexus.OCT4.profile_loss: 1257.0369 - \n",
      "Epoch 5/200\n",
      "276/276 [==============================] - 82s 296ms/step - loss: 1707.6164 - CHIPNexus.OCT4.logcount_loss: 0.2958 - CHIPNexus.OCT4.profile_loss: 1559.7295 - val_loss: 2463.6191 - val_CHIPNexus.OCT4.logcount_loss: 0.2830 - val_CHIPNexus.OCT4.profile_loss: 1243.4401\n",
      "Epoch 6/200\n",
      "276/276 [==============================] - 84s 305ms/step - loss: 1673.0786 - CHIPNexus.OCT4.logcount_loss: 0.2851 - CHIPNexus.OCT4.profile_loss: 1530.5176 - val_loss: 2284.2556 - val_CHIPNexus.OCT4.logcount_loss: 0.2765 - val_CHIPNexus.OCT4.profile_loss: 1230.2705\n",
      "Epoch 7/200\n",
      "276/276 [==============================] - 84s 303ms/step - loss: 1646.0912 - CHIPNexus.OCT4.logcount_loss: 0.2815 - CHIPNexus.OCT4.profile_loss: 1505.3492 - val_loss: 2402.6897 - val_CHIPNexus.OCT4.logcount_loss: 0.2687 - val_CHIPNexus.OCT4.profile_loss: 1223.0895xus.O\n",
      "Epoch 8/200\n",
      "276/276 [==============================] - 82s 297ms/step - loss: 1624.8496 - CHIPNexus.OCT4.logcount_loss: 0.2860 - CHIPNexus.OCT4.profile_loss: 1481.8323 - val_loss: 2470.2842 - val_CHIPNexus.OCT4.logcount_loss: 0.3127 - val_CHIPNexus.OCT4.profile_loss: 1215.5968\n",
      "Epoch 9/200\n",
      "276/276 [==============================] - 86s 313ms/step - loss: 1593.6325 - CHIPNexus.OCT4.logcount_loss: 0.2722 - CHIPNexus.OCT4.profile_loss: 1457.5217 - val_loss: 2322.3752 - val_CHIPNexus.OCT4.logcount_loss: 0.2520 - val_CHIPNexus.OCT4.profile_loss: 1202.9128\n",
      "Epoch 10/200\n",
      "276/276 [==============================] - 84s 305ms/step - loss: 1576.2002 - CHIPNexus.OCT4.logcount_loss: 0.2760 - CHIPNexus.OCT4.profile_loss: 1438.2015 - val_loss: 2241.4463 - val_CHIPNexus.OCT4.logcount_loss: 0.2581 - val_CHIPNexus.OCT4.profile_loss: 1202.8575\n",
      "Epoch 11/200\n",
      "276/276 [==============================] - 85s 308ms/step - loss: 1551.8358 - CHIPNexus.OCT4.logcount_loss: 0.2728 - CHIPNexus.OCT4.profile_loss: 1415.4512 - val_loss: 2227.0657 - val_CHIPNexus.OCT4.logcount_loss: 0.2564 - val_CHIPNexus.OCT4.profile_loss: 1194.7631\n",
      "Epoch 12/200\n",
      "276/276 [==============================] - 85s 309ms/step - loss: 1533.7123 - CHIPNexus.OCT4.logcount_loss: 0.2766 - CHIPNexus.OCT4.profile_loss: 1395.4127 - val_loss: 2288.0586 - val_CHIPNexus.OCT4.logcount_loss: 0.2587 - val_CHIPNexus.OCT4.profile_loss: 1191.8809\n",
      "Epoch 13/200\n",
      "276/276 [==============================] - 83s 302ms/step - loss: 1509.9818 - CHIPNexus.OCT4.logcount_loss: 0.2920 - CHIPNexus.OCT4.profile_loss: 1363.9583 - val_loss: 2220.3401 - val_CHIPNexus.OCT4.logcount_loss: 0.2716 - val_CHIPNexus.OCT4.profile_loss: 1186.8650\n",
      "Epoch 14/200\n",
      "276/276 [==============================] - 85s 307ms/step - loss: 1473.0705 - CHIPNexus.OCT4.logcount_loss: 0.3042 - CHIPNexus.OCT4.profile_loss: 1320.9829 - val_loss: 2366.5952 - val_CHIPNexus.OCT4.logcount_loss: 0.2763 - val_CHIPNexus.OCT4.profile_loss: 1243.7432\n",
      "Epoch 15/200\n",
      "276/276 [==============================] - 81s 293ms/step - loss: 1415.5904 - CHIPNexus.OCT4.logcount_loss: 0.2890 - CHIPNexus.OCT4.profile_loss: 1271.0977 - val_loss: 2411.2866 - val_CHIPNexus.OCT4.logcount_loss: 0.3164 - val_CHIPNexus.OCT4.profile_loss: 1177.8896\n",
      "Epoch 16/200\n",
      "276/276 [==============================] - 81s 294ms/step - loss: 1449.2375 - CHIPNexus.OCT4.logcount_loss: 0.2971 - CHIPNexus.OCT4.profile_loss: 1300.7090 - val_loss: 2254.9543 - val_CHIPNexus.OCT4.logcount_loss: 0.2659 - val_CHIPNexus.OCT4.profile_loss: 1168.3822s: 1450.4838 - CHIPNexus.OCT4.logcount_loss: 0.2968 - CHIPNexus.OCT4.profile_loss: 1302.\n",
      "Epoch 17/200\n",
      "276/276 [==============================] - 82s 297ms/step - loss: 1344.5941 - CHIPNexus.OCT4.logcount_loss: 0.2930 - CHIPNexus.OCT4.profile_loss: 1198.0720 - val_loss: 2324.3816 - val_CHIPNexus.OCT4.logcount_loss: 0.2678 - val_CHIPNexus.OCT4.profile_loss: 1199.8384\n",
      "Epoch 18/200\n",
      "276/276 [==============================] - 83s 302ms/step - loss: 1318.4703 - CHIPNexus.OCT4.logcount_loss: 0.2945 - CHIPNexus.OCT4.profile_loss: 1171.2080 - val_loss: 2229.5925 - val_CHIPNexus.OCT4.logcount_loss: 0.2681 - val_CHIPNexus.OCT4.profile_loss: 1165.5042\n",
      "Epoch 19/200\n",
      "276/276 [==============================] - 82s 297ms/step - loss: 1464.4358 - CHIPNexus.OCT4.logcount_loss: 0.2863 - CHIPNexus.OCT4.profile_loss: 1321.2667 - val_loss: 2219.7026 - val_CHIPNexus.OCT4.logcount_loss: 0.2658 - val_CHIPNexus.OCT4.profile_loss: 1155.2821loss: 0.2882 - CHIPNexus.OCT4. - ETA: 13s - loss: 1527.0226 - CHIPNexus.OCT4.logcount_loss: 0.2868 - CH - ETA: 6s - loss: 1496.0283 - CHIPNexus.OCT4.logcount\n",
      "Epoch 20/200\n",
      "276/276 [==============================] - 82s 297ms/step - loss: 1345.2289 - CHIPNexus.OCT4.logcount_loss: 0.2834 - CHIPNexus.OCT4.profile_loss: 1203.5446 - val_loss: 2258.6660 - val_CHIPNexus.OCT4.logcount_loss: 0.2677 - val_CHIPNexus.OCT4.profile_loss: 1166.9725\n",
      "Epoch 21/200\n",
      "276/276 [==============================] - 82s 298ms/step - loss: 1330.6069 - CHIPNexus.OCT4.logcount_loss: 0.2817 - CHIPNexus.OCT4.profile_loss: 1189.7336 - val_loss: 2211.4819 - val_CHIPNexus.OCT4.logcount_loss: 0.2651 - val_CHIPNexus.OCT4.profile_loss: 1129.6737\n",
      "Epoch 22/200\n",
      "  4/276 [..............................] - ETA: 29s - loss: 1302.2418 - CHIPNexus.OCT4.logcount_loss: 0.3589 - CHIPNexus.OCT4.profile_loss: 1122.7756"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/avanti/anaconda3/envs/py376/lib/python3.7/site-packages/keras/callbacks/callbacks.py:95: RuntimeWarning: Method (on_train_batch_end) is slow compared to the batch update (0.124633). Check your callbacks.\n",
      "  % (hook_name, delta_t_median), RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "276/276 [==============================] - 85s 307ms/step - loss: 1390.3296 - CHIPNexus.OCT4.logcount_loss: 0.2805 - CHIPNexus.OCT4.profile_loss: 1250.0751 - val_loss: 2250.0571 - val_CHIPNexus.OCT4.logcount_loss: 0.2628 - val_CHIPNexus.OCT4.profile_loss: 1132.7937CHIPNexus.\n",
      "Epoch 23/200\n",
      "276/276 [==============================] - 81s 293ms/step - loss: 1302.8089 - CHIPNexus.OCT4.logcount_loss: 0.2824 - CHIPNexus.OCT4.profile_loss: 1161.5981 - val_loss: 2301.4104 - val_CHIPNexus.OCT4.logcount_loss: 0.2705 - val_CHIPNexus.OCT4.profile_loss: 1127.6497\n",
      "Epoch 24/200\n",
      "276/276 [==============================] - 85s 309ms/step - loss: 1253.6190 - CHIPNexus.OCT4.logcount_loss: 0.2817 - CHIPNexus.OCT4.profile_loss: 1112.7548 - val_loss: 2258.3604 - val_CHIPNexus.OCT4.logcount_loss: 0.2635 - val_CHIPNexus.OCT4.profile_loss: 1116.6063\n",
      "Epoch 25/200\n",
      "276/276 [==============================] - 83s 300ms/step - loss: 1247.3250 - CHIPNexus.OCT4.logcount_loss: 0.2776 - CHIPNexus.OCT4.profile_loss: 1108.5254 - val_loss: 2237.1309 - val_CHIPNexus.OCT4.logcount_loss: 0.2634 - val_CHIPNexus.OCT4.profile_loss: 1116.4360\n",
      "Epoch 26/200\n",
      "276/276 [==============================] - 84s 303ms/step - loss: 1253.0111 - CHIPNexus.OCT4.logcount_loss: 0.2815 - CHIPNexus.OCT4.profile_loss: 1112.2458 - val_loss: 2222.1658 - val_CHIPNexus.OCT4.logcount_loss: 0.2630 - val_CHIPNexus.OCT4.profile_loss: 1111.9375\n",
      "Epoch 27/200\n",
      "276/276 [==============================] - 86s 312ms/step - loss: 1246.0314 - CHIPNexus.OCT4.logcount_loss: 0.2760 - CHIPNexus.OCT4.profile_loss: 1108.0079 - val_loss: 2276.3442 - val_CHIPNexus.OCT4.logcount_loss: 0.2636 - val_CHIPNexus.OCT4.profile_loss: 1116.0354\n",
      "Epoch 28/200\n",
      "276/276 [==============================] - 88s 319ms/step - loss: 1230.8698 - CHIPNexus.OCT4.logcount_loss: 0.2759 - CHIPNexus.OCT4.profile_loss: 1092.9391 - val_loss: 2220.6772 - val_CHIPNexus.OCT4.logcount_loss: 0.2593 - val_CHIPNexus.OCT4.profile_loss: 1102.8392\n",
      "Epoch 29/200\n",
      "276/276 [==============================] - 83s 301ms/step - loss: 1226.6434 - CHIPNexus.OCT4.logcount_loss: 0.2740 - CHIPNexus.OCT4.profile_loss: 1089.6373 - val_loss: 2189.0249 - val_CHIPNexus.OCT4.logcount_loss: 0.2701 - val_CHIPNexus.OCT4.profile_loss: 1109.4746\n",
      "Epoch 30/200\n",
      "276/276 [==============================] - 89s 323ms/step - loss: 1228.1271 - CHIPNexus.OCT4.logcount_loss: 0.2743 - CHIPNexus.OCT4.profile_loss: 1090.9591 - val_loss: 2204.9631 - val_CHIPNexus.OCT4.logcount_loss: 0.2599 - val_CHIPNexus.OCT4.profile_loss: 1098.5962CT4.logcount_los\n",
      "Epoch 31/200\n",
      "276/276 [==============================] - 86s 311ms/step - loss: 1283.0431 - CHIPNexus.OCT4.logcount_loss: 0.2773 - CHIPNexus.OCT4.profile_loss: 1144.3793 - val_loss: 2216.6125 - val_CHIPNexus.OCT4.logcount_loss: 0.2569 - val_CHIPNexus.OCT4.profile_loss: 1105.9569\n",
      "Epoch 32/200\n",
      "276/276 [==============================] - 81s 295ms/step - loss: 1215.5280 - CHIPNexus.OCT4.logcount_loss: 0.2686 - CHIPNexus.OCT4.profile_loss: 1081.2278 - val_loss: 2199.9004 - val_CHIPNexus.OCT4.logcount_loss: 0.2636 - val_CHIPNexus.OCT4.profile_loss: 1104.7352\n",
      "Epoch 33/200\n",
      "276/276 [==============================] - 81s 295ms/step - loss: 1211.2412 - CHIPNexus.OCT4.logcount_loss: 0.2689 - CHIPNexus.OCT4.profile_loss: 1076.8121 - val_loss: 2177.3882 - val_CHIPNexus.OCT4.logcount_loss: 0.2610 - val_CHIPNexus.OCT4.profile_loss: 1090.5869\n",
      "Epoch 34/200\n",
      "276/276 [==============================] - 83s 301ms/step - loss: 1216.7902 - CHIPNexus.OCT4.logcount_loss: 0.2614 - CHIPNexus.OCT4.profile_loss: 1086.0835 - val_loss: 2251.9797 - val_CHIPNexus.OCT4.logcount_loss: 0.2515 - val_CHIPNexus.OCT4.profile_loss: 1100.4315\n",
      "Epoch 35/200\n",
      "276/276 [==============================] - 82s 298ms/step - loss: 1204.8629 - CHIPNexus.OCT4.logcount_loss: 0.2610 - CHIPNexus.OCT4.profile_loss: 1074.3387 - val_loss: 2185.1074 - val_CHIPNexus.OCT4.logcount_loss: 0.2498 - val_CHIPNexus.OCT4.profile_loss: 1094.2418\n",
      "Epoch 36/200\n",
      "276/276 [==============================] - 83s 300ms/step - loss: 1190.1393 - CHIPNexus.OCT4.logcount_loss: 0.2559 - CHIPNexus.OCT4.profile_loss: 1062.1776 - val_loss: 2190.4392 - val_CHIPNexus.OCT4.logcount_loss: 0.2461 - val_CHIPNexus.OCT4.profile_loss: 1087.5940\n",
      "Epoch 37/200\n",
      "276/276 [==============================] - 85s 310ms/step - loss: 1179.7817 - CHIPNexus.OCT4.logcount_loss: 0.2497 - CHIPNexus.OCT4.profile_loss: 1054.9198 - val_loss: 2194.4319 - val_CHIPNexus.OCT4.logcount_loss: 0.2401 - val_CHIPNexus.OCT4.profile_loss: 1086.5582\n",
      "Epoch 38/200\n",
      "276/276 [==============================] - 85s 307ms/step - loss: 1206.7144 - CHIPNexus.OCT4.logcount_loss: 0.2484 - CHIPNexus.OCT4.profile_loss: 1082.4961 - val_loss: 2292.5483 - val_CHIPNexus.OCT4.logcount_loss: 0.2573 - val_CHIPNexus.OCT4.profile_loss: 1097.9441\n",
      "Epoch 39/200\n",
      "276/276 [==============================] - 84s 303ms/step - loss: 1197.2051 - CHIPNexus.OCT4.logcount_loss: 0.2427 - CHIPNexus.OCT4.profile_loss: 1075.8457 - val_loss: 2274.3579 - val_CHIPNexus.OCT4.logcount_loss: 0.2592 - val_CHIPNexus.OCT4.profile_loss: 1118.1036\n",
      "Epoch 40/200\n",
      "276/276 [==============================] - 85s 308ms/step - loss: 1196.8316 - CHIPNexus.OCT4.logcount_loss: 0.2467 - CHIPNexus.OCT4.profile_loss: 1073.4574 - val_loss: 2170.0000 - val_CHIPNexus.OCT4.logcount_loss: 0.2377 - val_CHIPNexus.OCT4.profile_loss: 1085.7821\n",
      "Epoch 41/200\n",
      "276/276 [==============================] - 85s 307ms/step - loss: 1164.5086 - CHIPNexus.OCT4.logcount_loss: 0.2343 - CHIPNexus.OCT4.profile_loss: 1047.3601 - val_loss: 2241.9683 - val_CHIPNexus.OCT4.logcount_loss: 0.2411 - val_CHIPNexus.OCT4.profile_loss: 1086.3027\n",
      "Epoch 42/200\n",
      "276/276 [==============================] - 82s 298ms/step - loss: 1158.2947 - CHIPNexus.OCT4.logcount_loss: 0.2358 - CHIPNexus.OCT4.profile_loss: 1040.3923 - val_loss: 2136.8838 - val_CHIPNexus.OCT4.logcount_loss: 0.2409 - val_CHIPNexus.OCT4.profile_loss: 1080.9749\n",
      "Epoch 43/200\n",
      "  3/276 [..............................] - ETA: 33s - loss: 1269.7900 - CHIPNexus.OCT4.logcount_loss: 0.3188 - CHIPNexus.OCT4.profile_loss: 1110.3757"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/avanti/anaconda3/envs/py376/lib/python3.7/site-packages/keras/callbacks/callbacks.py:95: RuntimeWarning: Method (on_train_batch_end) is slow compared to the batch update (0.119906). Check your callbacks.\n",
      "  % (hook_name, delta_t_median), RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "276/276 [==============================] - 85s 309ms/step - loss: 1157.5678 - CHIPNexus.OCT4.logcount_loss: 0.2347 - CHIPNexus.OCT4.profile_loss: 1040.1973 - val_loss: 2137.7505 - val_CHIPNexus.OCT4.logcount_loss: 0.2427 - val_CHIPNexus.OCT4.profile_loss: 1077.1814\n",
      "Epoch 44/200\n",
      "276/276 [==============================] - 95s 343ms/step - loss: 1178.8317 - CHIPNexus.OCT4.logcount_loss: 0.2304 - CHIPNexus.OCT4.profile_loss: 1063.6516 - val_loss: 2244.7004 - val_CHIPNexus.OCT4.logcount_loss: 0.2442 - val_CHIPNexus.OCT4.profile_loss: 1086.0970\n",
      "Epoch 45/200\n",
      "276/276 [==============================] - 87s 316ms/step - loss: 1166.0212 - CHIPNexus.OCT4.logcount_loss: 0.2292 - CHIPNexus.OCT4.profile_loss: 1051.4181 - val_loss: 2141.0852 - val_CHIPNexus.OCT4.logcount_loss: 0.2470 - val_CHIPNexus.OCT4.profile_loss: 1081.5562\n",
      "Epoch 46/200\n",
      "276/276 [==============================] - 84s 303ms/step - loss: 1158.9581 - CHIPNexus.OCT4.logcount_loss: 0.2264 - CHIPNexus.OCT4.profile_loss: 1045.7799 - val_loss: 2125.1233 - val_CHIPNexus.OCT4.logcount_loss: 0.2401 - val_CHIPNexus.OCT4.profile_loss: 1074.3602\n",
      "Epoch 47/200\n",
      "276/276 [==============================] - 85s 308ms/step - loss: 1155.1244 - CHIPNexus.OCT4.logcount_loss: 0.2281 - CHIPNexus.OCT4.profile_loss: 1041.0974 - val_loss: 2181.4888 - val_CHIPNexus.OCT4.logcount_loss: 0.2317 - val_CHIPNexus.OCT4.profile_loss: 1074.4064\n",
      "Epoch 48/200\n",
      "276/276 [==============================] - 87s 314ms/step - loss: 1153.9791 - CHIPNexus.OCT4.logcount_loss: 0.2238 - CHIPNexus.OCT4.profile_loss: 1042.0760 - val_loss: 2187.3582 - val_CHIPNexus.OCT4.logcount_loss: 0.2330 - val_CHIPNexus.OCT4.profile_loss: 1075.7057\n",
      "Epoch 49/200\n",
      "276/276 [==============================] - 84s 303ms/step - loss: 1153.3272 - CHIPNexus.OCT4.logcount_loss: 0.2244 - CHIPNexus.OCT4.profile_loss: 1041.1219 - val_loss: 2176.4070 - val_CHIPNexus.OCT4.logcount_loss: 0.2317 - val_CHIPNexus.OCT4.profile_loss: 1074.1313\n",
      "Epoch 50/200\n",
      "276/276 [==============================] - 84s 304ms/step - loss: 1140.5693 - CHIPNexus.OCT4.logcount_loss: 0.2229 - CHIPNexus.OCT4.profile_loss: 1029.1300 - val_loss: 2316.8176 - val_CHIPNexus.OCT4.logcount_loss: 0.3018 - val_CHIPNexus.OCT4.profile_loss: 1072.0062\n",
      "Epoch 51/200\n",
      "276/276 [==============================] - 85s 309ms/step - loss: 1146.8802 - CHIPNexus.OCT4.logcount_loss: 0.2247 - CHIPNexus.OCT4.profile_loss: 1034.5231 - val_loss: 2165.1001 - val_CHIPNexus.OCT4.logcount_loss: 0.2295 - val_CHIPNexus.OCT4.profile_loss: 1069.5508\n",
      "Epoch 52/200\n",
      "276/276 [==============================] - 85s 310ms/step - loss: 1134.6548 - CHIPNexus.OCT4.logcount_loss: 0.2224 - CHIPNexus.OCT4.profile_loss: 1023.4543 - val_loss: 2098.6438 - val_CHIPNexus.OCT4.logcount_loss: 0.2322 - val_CHIPNexus.OCT4.profile_loss: 1069.8754\n",
      "Epoch 53/200\n",
      "276/276 [==============================] - 87s 315ms/step - loss: 1135.2432 - CHIPNexus.OCT4.logcount_loss: 0.2197 - CHIPNexus.OCT4.profile_loss: 1025.3698 - val_loss: 2126.0408 - val_CHIPNexus.OCT4.logcount_loss: 0.2287 - val_CHIPNexus.OCT4.profile_loss: 1067.0104loss: 0.2196 - CHIPNexus.OCT4.profile_loss:\n",
      "Epoch 54/200\n",
      "  3/276 [..............................] - ETA: 34s - loss: 1103.9138 - CHIPNexus.OCT4.logcount_loss: 0.1921 - CHIPNexus.OCT4.profile_loss: 1007.8431"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/avanti/anaconda3/envs/py376/lib/python3.7/site-packages/keras/callbacks/callbacks.py:95: RuntimeWarning: Method (on_train_batch_end) is slow compared to the batch update (0.111000). Check your callbacks.\n",
      "  % (hook_name, delta_t_median), RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "276/276 [==============================] - 85s 308ms/step - loss: 1128.2441 - CHIPNexus.OCT4.logcount_loss: 0.2191 - CHIPNexus.OCT4.profile_loss: 1018.6880 - val_loss: 2105.7668 - val_CHIPNexus.OCT4.logcount_loss: 0.2288 - val_CHIPNexus.OCT4.profile_loss: 1064.9449\n",
      "Epoch 55/200\n",
      "276/276 [==============================] - 82s 297ms/step - loss: 1130.8572 - CHIPNexus.OCT4.logcount_loss: 0.2177 - CHIPNexus.OCT4.profile_loss: 1021.9854 - val_loss: 2106.5649 - val_CHIPNexus.OCT4.logcount_loss: 0.2265 - val_CHIPNexus.OCT4.profile_loss: 1064.3557\n",
      "Epoch 56/200\n",
      "276/276 [==============================] - 84s 304ms/step - loss: 1123.9799 - CHIPNexus.OCT4.logcount_loss: 0.2136 - CHIPNexus.OCT4.profile_loss: 1017.1821 - val_loss: 2176.6077 - val_CHIPNexus.OCT4.logcount_loss: 0.2270 - val_CHIPNexus.OCT4.profile_loss: 1063.9807\n",
      "Epoch 57/200\n",
      "276/276 [==============================] - 87s 315ms/step - loss: 1126.0556 - CHIPNexus.OCT4.logcount_loss: 0.2146 - CHIPNexus.OCT4.profile_loss: 1018.7565 - val_loss: 2110.3208 - val_CHIPNexus.OCT4.logcount_loss: 0.2275 - val_CHIPNexus.OCT4.profile_loss: 1066.2390\n",
      "Epoch 58/200\n",
      "276/276 [==============================] - 85s 309ms/step - loss: 1130.3207 - CHIPNexus.OCT4.logcount_loss: 0.2149 - CHIPNexus.OCT4.profile_loss: 1022.8638 - val_loss: 2207.7000 - val_CHIPNexus.OCT4.logcount_loss: 0.2443 - val_CHIPNexus.OCT4.profile_loss: 1062.2317 loss\n",
      "Epoch 59/200\n",
      "276/276 [==============================] - 84s 304ms/step - loss: 1129.1188 - CHIPNexus.OCT4.logcount_loss: 0.2156 - CHIPNexus.OCT4.profile_loss: 1021.2985 - val_loss: 2153.5251 - val_CHIPNexus.OCT4.logcount_loss: 0.2225 - val_CHIPNexus.OCT4.profile_loss: 1064.7307\n",
      "Epoch 60/200\n",
      "276/276 [==============================] - 83s 300ms/step - loss: 1127.4398 - CHIPNexus.OCT4.logcount_loss: 0.2108 - CHIPNexus.OCT4.profile_loss: 1022.0450 - val_loss: 2107.1873 - val_CHIPNexus.OCT4.logcount_loss: 0.2228 - val_CHIPNexus.OCT4.profile_loss: 1065.0358\n",
      "Epoch 61/200\n",
      "276/276 [==============================] - 91s 328ms/step - loss: 1126.3366 - CHIPNexus.OCT4.logcount_loss: 0.2098 - CHIPNexus.OCT4.profile_loss: 1021.4514 - val_loss: 2157.0369 - val_CHIPNexus.OCT4.logcount_loss: 0.2204 - val_CHIPNexus.OCT4.profile_loss: 1062.4321\n",
      "Epoch 62/200\n",
      "276/276 [==============================] - 85s 307ms/step - loss: 1126.4303 - CHIPNexus.OCT4.logcount_loss: 0.2112 - CHIPNexus.OCT4.profile_loss: 1020.8467 - val_loss: 2102.3022 - val_CHIPNexus.OCT4.logcount_loss: 0.2232 - val_CHIPNexus.OCT4.profile_loss: 1059.1824\n",
      "val_loss 2098.643798828125\n",
      "val_CHIPNexus.OCT4.logcount_loss 0.23218689858913422\n",
      "val_CHIPNexus.OCT4.profile_loss 1069.8753662109375\n",
      "loss 1134.672998711871\n",
      "CHIPNexus.OCT4.logcount_loss 0.22240081\n",
      "CHIPNexus.OCT4.profile_loss 1023.45435\n",
      "Seed: 1334\n",
      "LOG SPACE PREDS IS SET TO True\n",
      "Oct4only_ctaskweight0_seed1334.h5\n",
      "Model: \"model_13\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "sequence (InputLayer)           (None, 1346, 4)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_126 (Conv1D)             (None, 1326, 64)     5440        sequence[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_85 (Lambda)              (None, 1322, 64)     0           conv1d_126[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_127 (Conv1D)             (None, 1322, 64)     12352       conv1d_126[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_85 (Add)                    (None, 1322, 64)     0           lambda_85[0][0]                  \n",
      "                                                                 conv1d_127[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_86 (Lambda)              (None, 1314, 64)     0           add_85[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_128 (Conv1D)             (None, 1314, 64)     12352       add_85[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add_86 (Add)                    (None, 1314, 64)     0           lambda_86[0][0]                  \n",
      "                                                                 conv1d_128[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_87 (Lambda)              (None, 1298, 64)     0           add_86[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_129 (Conv1D)             (None, 1298, 64)     12352       add_86[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add_87 (Add)                    (None, 1298, 64)     0           lambda_87[0][0]                  \n",
      "                                                                 conv1d_129[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_88 (Lambda)              (None, 1266, 64)     0           add_87[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_130 (Conv1D)             (None, 1266, 64)     12352       add_87[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add_88 (Add)                    (None, 1266, 64)     0           lambda_88[0][0]                  \n",
      "                                                                 conv1d_130[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_89 (Lambda)              (None, 1202, 64)     0           add_88[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_131 (Conv1D)             (None, 1202, 64)     12352       add_88[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add_89 (Add)                    (None, 1202, 64)     0           lambda_89[0][0]                  \n",
      "                                                                 conv1d_131[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_90 (Lambda)              (None, 1074, 64)     0           add_89[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_132 (Conv1D)             (None, 1074, 64)     12352       add_89[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add_90 (Add)                    (None, 1074, 64)     0           lambda_90[0][0]                  \n",
      "                                                                 conv1d_132[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_15 (Gl (None, 64)           0           add_90[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "patchcap.logcount (InputLayer)  (None, 2)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_133 (Conv1D)             (None, 1000, 2)      9602        add_90[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "patchcap.profile (InputLayer)   (None, 1000, 2)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_57 (Concatenate)    (None, 66)           0           global_average_pooling1d_15[0][0]\n",
      "                                                                 patchcap.logcount[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_58 (Concatenate)    (None, 1000, 4)      0           conv1d_133[0][0]                 \n",
      "                                                                 patchcap.profile[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "CHIPNexus.OCT4.logcount (Dense) (None, 2)            134         concatenate_57[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "CHIPNexus.OCT4.profile (Conv1D) (None, 1000, 2)      10          concatenate_58[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 89,298\n",
      "Trainable params: 89,298\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "276/276 [==============================] - 84s 306ms/step - loss: 1712.5916 - CHIPNexus.OCT4.logcount_loss: 30.3288 - CHIPNexus.OCT4.profile_loss: 1712.5908 - val_loss: 2017.7437 - val_CHIPNexus.OCT4.logcount_loss: 29.6035 - val_CHIPNexus.OCT4.profile_loss: 1435.4811\n",
      "Epoch 2/200\n",
      "276/276 [==============================] - 87s 315ms/step - loss: 1654.8190 - CHIPNexus.OCT4.logcount_loss: 30.4894 - CHIPNexus.OCT4.profile_loss: 1654.8185 - val_loss: 1940.1420 - val_CHIPNexus.OCT4.logcount_loss: 29.1833 - val_CHIPNexus.OCT4.profile_loss: 1420.0924\n",
      "Epoch 3/200\n",
      "276/276 [==============================] - 83s 301ms/step - loss: 1628.3778 - CHIPNexus.OCT4.logcount_loss: 38.9427 - CHIPNexus.OCT4.profile_loss: 1628.3777 - val_loss: 1877.3481 - val_CHIPNexus.OCT4.logcount_loss: 40.1417 - val_CHIPNexus.OCT4.profile_loss: 1404.8956\n",
      "Epoch 4/200\n",
      "276/276 [==============================] - 82s 296ms/step - loss: 1609.9412 - CHIPNexus.OCT4.logcount_loss: 42.2299 - CHIPNexus.OCT4.profile_loss: 1609.9414 - val_loss: 1902.3116 - val_CHIPNexus.OCT4.logcount_loss: 40.4382 - val_CHIPNexus.OCT4.profile_loss: 1393.6592\n",
      "Epoch 5/200\n",
      "276/276 [==============================] - 82s 298ms/step - loss: 1592.5531 - CHIPNexus.OCT4.logcount_loss: 32.4883 - CHIPNexus.OCT4.profile_loss: 1592.5524 - val_loss: 1923.4360 - val_CHIPNexus.OCT4.logcount_loss: 27.5638 - val_CHIPNexus.OCT4.profile_loss: 1384.0088\n",
      "Epoch 6/200\n",
      "276/276 [==============================] - 83s 300ms/step - loss: 1575.4864 - CHIPNexus.OCT4.logcount_loss: 28.4259 - CHIPNexus.OCT4.profile_loss: 1575.4861 - val_loss: 1908.6198 - val_CHIPNexus.OCT4.logcount_loss: 33.3657 - val_CHIPNexus.OCT4.profile_loss: 1386.3506\n",
      "Epoch 7/200\n",
      "276/276 [==============================] - 86s 311ms/step - loss: 1559.6806 - CHIPNexus.OCT4.logcount_loss: 39.3304 - CHIPNexus.OCT4.profile_loss: 1559.6803 - val_loss: 1884.2749 - val_CHIPNexus.OCT4.logcount_loss: 38.0619 - val_CHIPNexus.OCT4.profile_loss: 1368.4036\n",
      "Epoch 8/200\n",
      "  3/276 [..............................] - ETA: 35s - loss: 1025.1561 - CHIPNexus.OCT4.logcount_loss: 37.7663 - CHIPNexus.OCT4.profile_loss: 1025.1561"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/avanti/anaconda3/envs/py376/lib/python3.7/site-packages/keras/callbacks/callbacks.py:95: RuntimeWarning: Method (on_train_batch_end) is slow compared to the batch update (0.125841). Check your callbacks.\n",
      "  % (hook_name, delta_t_median), RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "276/276 [==============================] - 85s 307ms/step - loss: 1545.9696 - CHIPNexus.OCT4.logcount_loss: 36.2572 - CHIPNexus.OCT4.profile_loss: 1545.9689 - val_loss: 1887.8815 - val_CHIPNexus.OCT4.logcount_loss: 31.9669 - val_CHIPNexus.OCT4.profile_loss: 1365.3221Nexus.OCT4.profile_loss: - ETA: 0s - loss: 1547.8499 - CHIPNexus.OCT4.logcount_loss: 36.2727 - CHIPNexus.OCT4.profile_loss: 1547.849\n",
      "Epoch 9/200\n",
      "276/276 [==============================] - 86s 313ms/step - loss: 1530.4002 - CHIPNexus.OCT4.logcount_loss: 32.5129 - CHIPNexus.OCT4.profile_loss: 1530.3999 - val_loss: 1895.3862 - val_CHIPNexus.OCT4.logcount_loss: 34.5895 - val_CHIPNexus.OCT4.profile_loss: 1359.6957\n",
      "Epoch 10/200\n",
      "276/276 [==============================] - 84s 306ms/step - loss: 1507.7794 - CHIPNexus.OCT4.logcount_loss: 34.1784 - CHIPNexus.OCT4.profile_loss: 1507.7792 - val_loss: 1986.3206 - val_CHIPNexus.OCT4.logcount_loss: 30.6131 - val_CHIPNexus.OCT4.profile_loss: 1389.8102\n",
      "Epoch 11/200\n",
      "276/276 [==============================] - 83s 299ms/step - loss: 1494.3747 - CHIPNexus.OCT4.logcount_loss: 41.6547 - CHIPNexus.OCT4.profile_loss: 1494.3743 - val_loss: 1945.7427 - val_CHIPNexus.OCT4.logcount_loss: 45.8440 - val_CHIPNexus.OCT4.profile_loss: 1347.0839\n",
      "Epoch 12/200\n",
      "276/276 [==============================] - 84s 306ms/step - loss: 1481.2988 - CHIPNexus.OCT4.logcount_loss: 48.3883 - CHIPNexus.OCT4.profile_loss: 1481.2977 - val_loss: 1996.2615 - val_CHIPNexus.OCT4.logcount_loss: 45.9819 - val_CHIPNexus.OCT4.profile_loss: 1347.3038\n",
      "Epoch 13/200\n",
      "276/276 [==============================] - 84s 304ms/step - loss: 1469.2005 - CHIPNexus.OCT4.logcount_loss: 38.9032 - CHIPNexus.OCT4.profile_loss: 1469.1996 - val_loss: 1985.5171 - val_CHIPNexus.OCT4.logcount_loss: 32.3714 - val_CHIPNexus.OCT4.profile_loss: 1343.9492\n",
      "val_loss 1877.34814453125\n",
      "val_CHIPNexus.OCT4.logcount_loss 40.141727447509766\n",
      "val_CHIPNexus.OCT4.profile_loss 1404.8956298828125\n",
      "loss 1628.5396460512566\n",
      "CHIPNexus.OCT4.logcount_loss 38.9427\n",
      "CHIPNexus.OCT4.profile_loss 1628.3777\n",
      "Seed: 1334\n",
      "LOG SPACE PREDS IS SET TO True\n",
      "Oct4only_ctaskweight125_seed1334.h5\n",
      "Model: \"model_14\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "sequence (InputLayer)           (None, 1346, 4)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_134 (Conv1D)             (None, 1326, 64)     5440        sequence[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_91 (Lambda)              (None, 1322, 64)     0           conv1d_134[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_135 (Conv1D)             (None, 1322, 64)     12352       conv1d_134[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_91 (Add)                    (None, 1322, 64)     0           lambda_91[0][0]                  \n",
      "                                                                 conv1d_135[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_92 (Lambda)              (None, 1314, 64)     0           add_91[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_136 (Conv1D)             (None, 1314, 64)     12352       add_91[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add_92 (Add)                    (None, 1314, 64)     0           lambda_92[0][0]                  \n",
      "                                                                 conv1d_136[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_93 (Lambda)              (None, 1298, 64)     0           add_92[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_137 (Conv1D)             (None, 1298, 64)     12352       add_92[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add_93 (Add)                    (None, 1298, 64)     0           lambda_93[0][0]                  \n",
      "                                                                 conv1d_137[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_94 (Lambda)              (None, 1266, 64)     0           add_93[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_138 (Conv1D)             (None, 1266, 64)     12352       add_93[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add_94 (Add)                    (None, 1266, 64)     0           lambda_94[0][0]                  \n",
      "                                                                 conv1d_138[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_95 (Lambda)              (None, 1202, 64)     0           add_94[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_139 (Conv1D)             (None, 1202, 64)     12352       add_94[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add_95 (Add)                    (None, 1202, 64)     0           lambda_95[0][0]                  \n",
      "                                                                 conv1d_139[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_96 (Lambda)              (None, 1074, 64)     0           add_95[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_140 (Conv1D)             (None, 1074, 64)     12352       add_95[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add_96 (Add)                    (None, 1074, 64)     0           lambda_96[0][0]                  \n",
      "                                                                 conv1d_140[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_16 (Gl (None, 64)           0           add_96[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "patchcap.logcount (InputLayer)  (None, 2)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_141 (Conv1D)             (None, 1000, 2)      9602        add_96[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "patchcap.profile (InputLayer)   (None, 1000, 2)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_59 (Concatenate)    (None, 66)           0           global_average_pooling1d_16[0][0]\n",
      "                                                                 patchcap.logcount[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_60 (Concatenate)    (None, 1000, 4)      0           conv1d_141[0][0]                 \n",
      "                                                                 patchcap.profile[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "CHIPNexus.OCT4.logcount (Dense) (None, 2)            134         concatenate_59[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "CHIPNexus.OCT4.profile (Conv1D) (None, 1000, 2)      10          concatenate_60[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 89,298\n",
      "Trainable params: 89,298\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "276/276 [==============================] - 83s 300ms/step - loss: 1815.5651 - CHIPNexus.OCT4.logcount_loss: 0.6058 - CHIPNexus.OCT4.profile_loss: 1739.8450 - val_loss: 2103.8203 - val_CHIPNexus.OCT4.logcount_loss: 0.3140 - val_CHIPNexus.OCT4.profile_loss: 1470.6494\n",
      "Epoch 2/200\n",
      "  3/276 [..............................] - ETA: 37s - loss: 1076.3990 - CHIPNexus.OCT4.logcount_loss: 0.2775 - CHIPNexus.OCT4.profile_loss: 1041.7145"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/avanti/anaconda3/envs/py376/lib/python3.7/site-packages/keras/callbacks/callbacks.py:95: RuntimeWarning: Method (on_train_batch_end) is slow compared to the batch update (0.179203). Check your callbacks.\n",
      "  % (hook_name, delta_t_median), RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "276/276 [==============================] - 85s 308ms/step - loss: 1731.2052 - CHIPNexus.OCT4.logcount_loss: 0.3102 - CHIPNexus.OCT4.profile_loss: 1692.4308 - val_loss: 2137.8745 - val_CHIPNexus.OCT4.logcount_loss: 0.3151 - val_CHIPNexus.OCT4.profile_loss: 1444.9603ss: 2254.6404 - ETA: 12s - loss: 18\n",
      "Epoch 3/200\n",
      "276/276 [==============================] - 85s 307ms/step - loss: 1693.7868 - CHIPNexus.OCT4.logcount_loss: 0.3076 - CHIPNexus.OCT4.profile_loss: 1655.3376 - val_loss: 2138.9971 - val_CHIPNexus.OCT4.logcount_loss: 0.2873 - val_CHIPNexus.OCT4.profile_loss: 1416.9099.OCT4.logcount - ETA: 40s - loss: 2509.1834 - CHIPNexus.OCT4.logcount_loss: 0.3191 - CHIPN - ETA: - ETA: 8s - loss: 1770.8957 - CHIPNexus.OCT4.\n",
      "Epoch 4/200\n",
      "276/276 [==============================] - 82s 298ms/step - loss: 1661.7674 - CHIPNexus.OCT4.logcount_loss: 0.3023 - CHIPNexus.OCT4.profile_loss: 1623.9773 - val_loss: 2079.4973 - val_CHIPNexus.OCT4.logcount_loss: 0.3212 - val_CHIPNexus.OCT4.profile_loss: 1403.39612.3799 - CHIPNexus.OCT4.logcount_loss: 0.3032 - CHIPNexus.OCT4.profile_loss: 10 - ETA: 29s - loss: 1081.8290 - ETA: 9s - loss: 1750.6584 - C\n",
      "Epoch 5/200\n",
      "276/276 [==============================] - 84s 304ms/step - loss: 1639.6056 - CHIPNexus.OCT4.logcount_loss: 0.2977 - CHIPNexus.OCT4.profile_loss: 1602.3861 - val_loss: 2059.1489 - val_CHIPNexus.OCT4.logcount_loss: 0.2935 - val_CHIPNexus.OCT4.profile_loss: 1388.5465\n",
      "Epoch 6/200\n",
      "276/276 [==============================] - 83s 302ms/step - loss: 1622.2922 - CHIPNexus.OCT4.logcount_loss: 0.3018 - CHIPNexus.OCT4.profile_loss: 1584.5620 - val_loss: 2069.8069 - val_CHIPNexus.OCT4.logcount_loss: 0.3089 - val_CHIPNexus.OCT4.profile_loss: 1376.5342.OCT4.logcount_loss: 0.3053 - CHIPNexus.OCT4.profile_loss: 1644.76 - ETA: 6s - loss: 1680.4891 - CHIPNexus.OCT4.logcount\n",
      "Epoch 7/200\n",
      "276/276 [==============================] - 84s 303ms/step - loss: 1602.6197 - CHIPNexus.OCT4.logcount_loss: 0.2872 - CHIPNexus.OCT4.profile_loss: 1566.7136 - val_loss: 2028.1533 - val_CHIPNexus.OCT4.logcount_loss: 0.2879 - val_CHIPNexus.OCT4.profile_loss: 1371.6273\n",
      "Epoch 8/200\n",
      "276/276 [==============================] - 85s 307ms/step - loss: 1584.8853 - CHIPNexus.OCT4.logcount_loss: 0.2889 - CHIPNexus.OCT4.profile_loss: 1548.7789 - val_loss: 2028.8668 - val_CHIPNexus.OCT4.logcount_loss: 0.2900 - val_CHIPNexus.OCT4.profile_loss: 1364.5842\n",
      "Epoch 9/200\n",
      "276/276 [==============================] - 83s 303ms/step - loss: 1571.4556 - CHIPNexus.OCT4.logcount_loss: 0.2908 - CHIPNexus.OCT4.profile_loss: 1535.1012 - val_loss: 2016.6108 - val_CHIPNexus.OCT4.logcount_loss: 0.2833 - val_CHIPNexus.OCT4.profile_loss: 1350.7062\n",
      "Epoch 10/200\n",
      "276/276 [==============================] - 87s 314ms/step - loss: 1555.7188 - CHIPNexus.OCT4.logcount_loss: 0.2888 - CHIPNexus.OCT4.profile_loss: 1519.6169 - val_loss: 2028.2671 - val_CHIPNexus.OCT4.logcount_loss: 0.2667 - val_CHIPNexus.OCT4.profile_loss: 1344.2920\n",
      "Epoch 11/200\n",
      "276/276 [==============================] - 90s 325ms/step - loss: 1538.9623 - CHIPNexus.OCT4.logcount_loss: 0.2915 - CHIPNexus.OCT4.profile_loss: 1502.5203 - val_loss: 1998.7058 - val_CHIPNexus.OCT4.logcount_loss: 0.2623 - val_CHIPNexus.OCT4.profile_loss: 1335.2972\n",
      "Epoch 12/200\n",
      "276/276 [==============================] - 87s 316ms/step - loss: 1523.0277 - CHIPNexus.OCT4.logcount_loss: 0.2716 - CHIPNexus.OCT4.profile_loss: 1489.0804 - val_loss: 2047.2909 - val_CHIPNexus.OCT4.logcount_loss: 0.2623 - val_CHIPNexus.OCT4.profile_loss: 1336.6198\n",
      "Epoch 13/200\n",
      "276/276 [==============================] - 83s 301ms/step - loss: 1506.4504 - CHIPNexus.OCT4.logcount_loss: 0.2896 - CHIPNexus.OCT4.profile_loss: 1470.2485 - val_loss: 2026.2218 - val_CHIPNexus.OCT4.logcount_loss: 0.2701 - val_CHIPNexus.OCT4.profile_loss: 1323.3542\n",
      "Epoch 14/200\n",
      "276/276 [==============================] - 82s 299ms/step - loss: 1490.2480 - CHIPNexus.OCT4.logcount_loss: 0.3027 - CHIPNexus.OCT4.profile_loss: 1452.4042 - val_loss: 1972.5496 - val_CHIPNexus.OCT4.logcount_loss: 0.2578 - val_CHIPNexus.OCT4.profile_loss: 1313.9270s.OCT4.logcount_los\n",
      "Epoch 15/200\n",
      "276/276 [==============================] - 86s 312ms/step - loss: 1478.1394 - CHIPNexus.OCT4.logcount_loss: 0.2722 - CHIPNexus.OCT4.profile_loss: 1444.1136 - val_loss: 2104.2534 - val_CHIPNexus.OCT4.logcount_loss: 0.2759 - val_CHIPNexus.OCT4.profile_loss: 1334.3884\n",
      "Epoch 16/200\n",
      "276/276 [==============================] - 85s 309ms/step - loss: 1471.1177 - CHIPNexus.OCT4.logcount_loss: 0.3161 - CHIPNexus.OCT4.profile_loss: 1431.6106 - val_loss: 2074.0771 - val_CHIPNexus.OCT4.logcount_loss: 0.2911 - val_CHIPNexus.OCT4.profile_loss: 1317.2184\n",
      "Epoch 17/200\n",
      "276/276 [==============================] - 88s 320ms/step - loss: 1413.7925 - CHIPNexus.OCT4.logcount_loss: 0.3006 - CHIPNexus.OCT4.profile_loss: 1376.2186 - val_loss: 2742.5886 - val_CHIPNexus.OCT4.logcount_loss: 0.8031 - val_CHIPNexus.OCT4.profile_loss: 1670.9131\n",
      "Epoch 18/200\n",
      "276/276 [==============================] - 83s 300ms/step - loss: 1399.2077 - CHIPNexus.OCT4.logcount_loss: 0.3698 - CHIPNexus.OCT4.profile_loss: 1352.9847 - val_loss: 2131.3757 - val_CHIPNexus.OCT4.logcount_loss: 0.2755 - val_CHIPNexus.OCT4.profile_loss: 1327.5692\n",
      "Epoch 19/200\n",
      "276/276 [==============================] - 83s 301ms/step - loss: 1306.6015 - CHIPNexus.OCT4.logcount_loss: 0.2965 - CHIPNexus.OCT4.profile_loss: 1269.5393 - val_loss: 2018.0753 - val_CHIPNexus.OCT4.logcount_loss: 0.2771 - val_CHIPNexus.OCT4.profile_loss: 1302.9586\n",
      "Epoch 20/200\n",
      "276/276 [==============================] - 83s 300ms/step - loss: 1316.9265 - CHIPNexus.OCT4.logcount_loss: 0.3154 - CHIPNexus.OCT4.profile_loss: 1277.5037 - val_loss: 2028.1093 - val_CHIPNexus.OCT4.logcount_loss: 0.2771 - val_CHIPNexus.OCT4.profile_loss: 1302.7881\n",
      "Epoch 21/200\n",
      "276/276 [==============================] - 83s 300ms/step - loss: 1348.0471 - CHIPNexus.OCT4.logcount_loss: 0.2900 - CHIPNexus.OCT4.profile_loss: 1311.8638 - val_loss: 2012.4590 - val_CHIPNexus.OCT4.logcount_loss: 0.2695 - val_CHIPNexus.OCT4.profile_loss: 1284.4641\n",
      "Epoch 22/200\n",
      "276/276 [==============================] - 83s 300ms/step - loss: 1233.5313 - CHIPNexus.OCT4.logcount_loss: 0.2907 - CHIPNexus.OCT4.profile_loss: 1197.1888 - val_loss: 2032.8898 - val_CHIPNexus.OCT4.logcount_loss: 0.2785 - val_CHIPNexus.OCT4.profile_loss: 1284.5868\n",
      "Epoch 23/200\n",
      "276/276 [==============================] - 88s 318ms/step - loss: 1362.8993 - CHIPNexus.OCT4.logcount_loss: 0.3039 - CHIPNexus.OCT4.profile_loss: 1324.9172 - val_loss: 2011.9668 - val_CHIPNexus.OCT4.logcount_loss: 0.2740 - val_CHIPNexus.OCT4.profile_loss: 1270.7864\n",
      "Epoch 24/200\n",
      "  3/276 [..............................] - ETA: 30s - loss: 1057.1174 - CHIPNexus.OCT4.logcount_loss: 0.2614 - CHIPNexus.OCT4.profile_loss: 1024.4391"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/avanti/anaconda3/envs/py376/lib/python3.7/site-packages/keras/callbacks/callbacks.py:95: RuntimeWarning: Method (on_train_batch_end) is slow compared to the batch update (0.108607). Check your callbacks.\n",
      "  % (hook_name, delta_t_median), RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "276/276 [==============================] - 86s 310ms/step - loss: 1199.1154 - CHIPNexus.OCT4.logcount_loss: 0.2874 - CHIPNexus.OCT4.profile_loss: 1163.1906 - val_loss: 2004.6970 - val_CHIPNexus.OCT4.logcount_loss: 0.2905 - val_CHIPNexus.OCT4.profile_loss: 1273.1741\n",
      "val_loss 1972.549560546875\n",
      "val_CHIPNexus.OCT4.logcount_loss 0.25784578919410706\n",
      "val_CHIPNexus.OCT4.profile_loss 1313.927001953125\n",
      "loss 1490.388813387113\n",
      "CHIPNexus.OCT4.logcount_loss 0.30274594\n",
      "CHIPNexus.OCT4.profile_loss 1452.4042\n",
      "Seed: 1334\n",
      "LOG SPACE PREDS IS SET TO True\n",
      "Oct4only_ctaskweight250_seed1334.h5\n",
      "Model: \"model_15\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "sequence (InputLayer)           (None, 1346, 4)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_142 (Conv1D)             (None, 1326, 64)     5440        sequence[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_97 (Lambda)              (None, 1322, 64)     0           conv1d_142[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_143 (Conv1D)             (None, 1322, 64)     12352       conv1d_142[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_97 (Add)                    (None, 1322, 64)     0           lambda_97[0][0]                  \n",
      "                                                                 conv1d_143[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_98 (Lambda)              (None, 1314, 64)     0           add_97[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_144 (Conv1D)             (None, 1314, 64)     12352       add_97[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add_98 (Add)                    (None, 1314, 64)     0           lambda_98[0][0]                  \n",
      "                                                                 conv1d_144[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_99 (Lambda)              (None, 1298, 64)     0           add_98[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_145 (Conv1D)             (None, 1298, 64)     12352       add_98[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add_99 (Add)                    (None, 1298, 64)     0           lambda_99[0][0]                  \n",
      "                                                                 conv1d_145[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_100 (Lambda)             (None, 1266, 64)     0           add_99[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_146 (Conv1D)             (None, 1266, 64)     12352       add_99[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add_100 (Add)                   (None, 1266, 64)     0           lambda_100[0][0]                 \n",
      "                                                                 conv1d_146[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_101 (Lambda)             (None, 1202, 64)     0           add_100[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_147 (Conv1D)             (None, 1202, 64)     12352       add_100[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_101 (Add)                   (None, 1202, 64)     0           lambda_101[0][0]                 \n",
      "                                                                 conv1d_147[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_102 (Lambda)             (None, 1074, 64)     0           add_101[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_148 (Conv1D)             (None, 1074, 64)     12352       add_101[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_102 (Add)                   (None, 1074, 64)     0           lambda_102[0][0]                 \n",
      "                                                                 conv1d_148[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_17 (Gl (None, 64)           0           add_102[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "patchcap.logcount (InputLayer)  (None, 2)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_149 (Conv1D)             (None, 1000, 2)      9602        add_102[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "patchcap.profile (InputLayer)   (None, 1000, 2)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_61 (Concatenate)    (None, 66)           0           global_average_pooling1d_17[0][0]\n",
      "                                                                 patchcap.logcount[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_62 (Concatenate)    (None, 1000, 4)      0           conv1d_149[0][0]                 \n",
      "                                                                 patchcap.profile[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "CHIPNexus.OCT4.logcount (Dense) (None, 2)            134         concatenate_61[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "CHIPNexus.OCT4.profile (Conv1D) (None, 1000, 2)      10          concatenate_62[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 89,298\n",
      "Trainable params: 89,298\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/200\n",
      "276/276 [==============================] - 88s 319ms/step - loss: 1839.6863 - CHIPNexus.OCT4.logcount_loss: 0.5707 - CHIPNexus.OCT4.profile_loss: 1697.0212 - val_loss: 2284.7769 - val_CHIPNexus.OCT4.logcount_loss: 0.2943 - val_CHIPNexus.OCT4.profile_loss: 1458.6589\n",
      "Epoch 2/200\n",
      "276/276 [==============================] - 82s 296ms/step - loss: 1727.7615 - CHIPNexus.OCT4.logcount_loss: 0.3047 - CHIPNexus.OCT4.profile_loss: 1651.5905 - val_loss: 2259.0234 - val_CHIPNexus.OCT4.logcount_loss: 0.2936 - val_CHIPNexus.OCT4.profile_loss: 1420.9205\n",
      "Epoch 3/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "276/276 [==============================] - 84s 304ms/step - loss: 1695.4783 - CHIPNexus.OCT4.logcount_loss: 0.3102 - CHIPNexus.OCT4.profile_loss: 1617.9236 - val_loss: 2287.8909 - val_CHIPNexus.OCT4.logcount_loss: 0.3040 - val_CHIPNexus.OCT4.profile_loss: 1395.5115\n",
      "Epoch 4/200\n",
      "276/276 [==============================] - 86s 313ms/step - loss: 1662.5917 - CHIPNexus.OCT4.logcount_loss: 0.3048 - CHIPNexus.OCT4.profile_loss: 1586.3912 - val_loss: 2170.1243 - val_CHIPNexus.OCT4.logcount_loss: 0.2843 - val_CHIPNexus.OCT4.profile_loss: 1379.2120\n",
      "Epoch 5/200\n",
      "276/276 [==============================] - 87s 316ms/step - loss: 1640.1225 - CHIPNexus.OCT4.logcount_loss: 0.3006 - CHIPNexus.OCT4.profile_loss: 1564.9642 - val_loss: 2180.2063 - val_CHIPNexus.OCT4.logcount_loss: 0.2765 - val_CHIPNexus.OCT4.profile_loss: 1366.9990\n",
      "Epoch 6/200\n",
      "276/276 [==============================] - 87s 314ms/step - loss: 1617.6190 - CHIPNexus.OCT4.logcount_loss: 0.3005 - CHIPNexus.OCT4.profile_loss: 1542.5015 - val_loss: 2103.0430 - val_CHIPNexus.OCT4.logcount_loss: 0.2737 - val_CHIPNexus.OCT4.profile_loss: 1354.5375\n",
      "Epoch 7/200\n",
      "276/276 [==============================] - 88s 319ms/step - loss: 1598.1936 - CHIPNexus.OCT4.logcount_loss: 0.2939 - CHIPNexus.OCT4.profile_loss: 1524.7111 - val_loss: 2126.7878 - val_CHIPNexus.OCT4.logcount_loss: 0.2751 - val_CHIPNexus.OCT4.profile_loss: 1344.5752\n",
      "Epoch 8/200\n",
      "276/276 [==============================] - 88s 317ms/step - loss: 1577.1524 - CHIPNexus.OCT4.logcount_loss: 0.2853 - CHIPNexus.OCT4.profile_loss: 1505.8185 - val_loss: 2102.3237 - val_CHIPNexus.OCT4.logcount_loss: 0.2823 - val_CHIPNexus.OCT4.profile_loss: 1344.3895\n",
      "Epoch 9/200\n",
      "276/276 [==============================] - 81s 295ms/step - loss: 1563.6640 - CHIPNexus.OCT4.logcount_loss: 0.2873 - CHIPNexus.OCT4.profile_loss: 1491.8312 - val_loss: 2155.3906 - val_CHIPNexus.OCT4.logcount_loss: 0.2852 - val_CHIPNexus.OCT4.profile_loss: 1327.2661\n",
      "Epoch 10/200\n",
      "276/276 [==============================] - 85s 307ms/step - loss: 1547.4360 - CHIPNexus.OCT4.logcount_loss: 0.2831 - CHIPNexus.OCT4.profile_loss: 1476.6704 - val_loss: 2093.4109 - val_CHIPNexus.OCT4.logcount_loss: 0.2635 - val_CHIPNexus.OCT4.profile_loss: 1317.2867\n",
      "Epoch 11/200\n",
      "276/276 [==============================] - 83s 301ms/step - loss: 1529.4572 - CHIPNexus.OCT4.logcount_loss: 0.2781 - CHIPNexus.OCT4.profile_loss: 1459.9381 - val_loss: 2046.2397 - val_CHIPNexus.OCT4.logcount_loss: 0.2596 - val_CHIPNexus.OCT4.profile_loss: 1313.4569\n",
      "Epoch 12/200\n",
      "276/276 [==============================] - 86s 312ms/step - loss: 1511.5038 - CHIPNexus.OCT4.logcount_loss: 0.2770 - CHIPNexus.OCT4.profile_loss: 1442.2534 - val_loss: 2057.6902 - val_CHIPNexus.OCT4.logcount_loss: 0.2579 - val_CHIPNexus.OCT4.profile_loss: 1306.2423\n",
      "Epoch 13/200\n",
      "276/276 [==============================] - 85s 308ms/step - loss: 1499.7305 - CHIPNexus.OCT4.logcount_loss: 0.2800 - CHIPNexus.OCT4.profile_loss: 1429.7291 - val_loss: 2038.5551 - val_CHIPNexus.OCT4.logcount_loss: 0.3117 - val_CHIPNexus.OCT4.profile_loss: 1297.4818\n",
      "Epoch 14/200\n",
      "  3/276 [..............................] - ETA: 29s - loss: 1059.5680 - CHIPNexus.OCT4.logcount_loss: 0.2424 - CHIPNexus.OCT4.profile_loss: 998.9603 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/avanti/anaconda3/envs/py376/lib/python3.7/site-packages/keras/callbacks/callbacks.py:95: RuntimeWarning: Method (on_train_batch_end) is slow compared to the batch update (0.103749). Check your callbacks.\n",
      "  % (hook_name, delta_t_median), RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "276/276 [==============================] - 82s 297ms/step - loss: 1489.7677 - CHIPNexus.OCT4.logcount_loss: 0.2831 - CHIPNexus.OCT4.profile_loss: 1418.9860 - val_loss: 2091.2778 - val_CHIPNexus.OCT4.logcount_loss: 0.2619 - val_CHIPNexus.OCT4.profile_loss: 1295.4563\n",
      "Epoch 15/200\n",
      "276/276 [==============================] - 83s 301ms/step - loss: 1449.0582 - CHIPNexus.OCT4.logcount_loss: 0.2729 - CHIPNexus.OCT4.profile_loss: 1380.8282 - val_loss: 2565.3008 - val_CHIPNexus.OCT4.logcount_loss: 0.7585 - val_CHIPNexus.OCT4.profile_loss: 1332.1434\n",
      "Epoch 16/200\n",
      "276/276 [==============================] - 83s 302ms/step - loss: 1490.1815 - CHIPNexus.OCT4.logcount_loss: 0.3346 - CHIPNexus.OCT4.profile_loss: 1406.5292 - val_loss: 2118.4009 - val_CHIPNexus.OCT4.logcount_loss: 0.2802 - val_CHIPNexus.OCT4.profile_loss: 1305.3383Nexus.OCT4.logcount_loss: 0.3673 - CHIPNexus.OCT4.pr - ETA: 41s - loss: 1182.5980 - CHIPNexus.OCT4.logcount_ - ETA: 28s - loss: 1163.4023 - CHIPNexus.OCT4.logcount_loss: - ETA\n",
      "Epoch 17/200\n",
      "276/276 [==============================] - 86s 312ms/step - loss: 1419.0198 - CHIPNexus.OCT4.logcount_loss: 0.3068 - CHIPNexus.OCT4.profile_loss: 1342.3212 - val_loss: 2224.0989 - val_CHIPNexus.OCT4.logcount_loss: 0.3106 - val_CHIPNexus.OCT4.profile_loss: 1341.0210\n",
      "Epoch 18/200\n",
      "276/276 [==============================] - 89s 324ms/step - loss: 1411.6255 - CHIPNexus.OCT4.logcount_loss: 0.3086 - CHIPNexus.OCT4.profile_loss: 1334.4742 - val_loss: 2165.0972 - val_CHIPNexus.OCT4.logcount_loss: 0.2663 - val_CHIPNexus.OCT4.profile_loss: 1301.2549\n",
      "Epoch 19/200\n",
      "276/276 [==============================] - 85s 309ms/step - loss: 1337.1884 - CHIPNexus.OCT4.logcount_loss: 0.2843 - CHIPNexus.OCT4.profile_loss: 1266.1176 - val_loss: 2142.1802 - val_CHIPNexus.OCT4.logcount_loss: 0.2830 - val_CHIPNexus.OCT4.profile_loss: 1297.5304\n",
      "Epoch 20/200\n",
      "276/276 [==============================] - 86s 312ms/step - loss: 1358.8319 - CHIPNexus.OCT4.logcount_loss: 0.2906 - CHIPNexus.OCT4.profile_loss: 1286.1807 - val_loss: 2089.2273 - val_CHIPNexus.OCT4.logcount_loss: 0.2715 - val_CHIPNexus.OCT4.profile_loss: 1277.9529\n",
      "Epoch 21/200\n",
      "276/276 [==============================] - 84s 306ms/step - loss: 1301.8907 - CHIPNexus.OCT4.logcount_loss: 0.2859 - CHIPNexus.OCT4.profile_loss: 1230.4254 - val_loss: 2087.8499 - val_CHIPNexus.OCT4.logcount_loss: 0.2723 - val_CHIPNexus.OCT4.profile_loss: 1258.4027\n",
      "Epoch 22/200\n",
      "276/276 [==============================] - 86s 312ms/step - loss: 1276.4345 - CHIPNexus.OCT4.logcount_loss: 0.2842 - CHIPNexus.OCT4.profile_loss: 1205.3860 - val_loss: 2037.3235 - val_CHIPNexus.OCT4.logcount_loss: 0.2825 - val_CHIPNexus.OCT4.profile_loss: 1247.9971PNexus.OCT4.profile_loss:\n",
      "Epoch 23/200\n",
      "276/276 [==============================] - 86s 310ms/step - loss: 1268.5574 - CHIPNexus.OCT4.logcount_loss: 0.2812 - CHIPNexus.OCT4.profile_loss: 1198.2484 - val_loss: 2042.5502 - val_CHIPNexus.OCT4.logcount_loss: 0.2631 - val_CHIPNexus.OCT4.profile_loss: 1243.2080\n",
      "Epoch 24/200\n",
      "276/276 [==============================] - 84s 303ms/step - loss: 1222.9876 - CHIPNexus.OCT4.logcount_loss: 0.2818 - CHIPNexus.OCT4.profile_loss: 1152.5273 - val_loss: 2053.6792 - val_CHIPNexus.OCT4.logcount_loss: 0.2644 - val_CHIPNexus.OCT4.profile_loss: 1239.3916\n",
      "Epoch 25/200\n",
      "276/276 [==============================] - 84s 306ms/step - loss: 1235.0113 - CHIPNexus.OCT4.logcount_loss: 0.2807 - CHIPNexus.OCT4.profile_loss: 1164.8394 - val_loss: 2032.6182 - val_CHIPNexus.OCT4.logcount_loss: 0.2638 - val_CHIPNexus.OCT4.profile_loss: 1230.2504\n",
      "Epoch 26/200\n",
      "276/276 [==============================] - 83s 301ms/step - loss: 1235.9197 - CHIPNexus.OCT4.logcount_loss: 0.2865 - CHIPNexus.OCT4.profile_loss: 1164.2828 - val_loss: 2097.5015 - val_CHIPNexus.OCT4.logcount_loss: 0.2956 - val_CHIPNexus.OCT4.profile_loss: 1227.4628\n",
      "Epoch 27/200\n",
      "276/276 [==============================] - 85s 309ms/step - loss: 1224.7209 - CHIPNexus.OCT4.logcount_loss: 0.2833 - CHIPNexus.OCT4.profile_loss: 1153.8861 - val_loss: 2037.8812 - val_CHIPNexus.OCT4.logcount_loss: 0.2658 - val_CHIPNexus.OCT4.profile_loss: 1217.0601\n",
      "Epoch 28/200\n",
      "276/276 [==============================] - 85s 309ms/step - loss: 1208.5755 - CHIPNexus.OCT4.logcount_loss: 0.2850 - CHIPNexus.OCT4.profile_loss: 1137.3352 - val_loss: 2053.1252 - val_CHIPNexus.OCT4.logcount_loss: 0.2655 - val_CHIPNexus.OCT4.profile_loss: 1214.0850\n",
      "Epoch 29/200\n",
      "276/276 [==============================] - 86s 311ms/step - loss: 1235.6821 - CHIPNexus.OCT4.logcount_loss: 0.2800 - CHIPNexus.OCT4.profile_loss: 1165.6880 - val_loss: 2123.5693 - val_CHIPNexus.OCT4.logcount_loss: 0.2687 - val_CHIPNexus.OCT4.profile_loss: 1229.656048.1378\n",
      "Epoch 30/200\n",
      "276/276 [==============================] - 85s 309ms/step - loss: 1373.6175 - CHIPNexus.OCT4.logcount_loss: 0.2851 - CHIPNexus.OCT4.profile_loss: 1302.3484 - val_loss: 2104.2524 - val_CHIPNexus.OCT4.logcount_loss: 0.3099 - val_CHIPNexus.OCT4.profile_loss: 1198.7542\n",
      "Epoch 31/200\n",
      "276/276 [==============================] - 87s 314ms/step - loss: 1332.1767 - CHIPNexus.OCT4.logcount_loss: 0.2764 - CHIPNexus.OCT4.profile_loss: 1263.0885 - val_loss: 2069.2625 - val_CHIPNexus.OCT4.logcount_loss: 0.2727 - val_CHIPNexus.OCT4.profile_loss: 1186.5615\n",
      "Epoch 32/200\n",
      "276/276 [==============================] - 88s 317ms/step - loss: 1285.3132 - CHIPNexus.OCT4.logcount_loss: 0.2756 - CHIPNexus.OCT4.profile_loss: 1216.4060 - val_loss: 2072.4165 - val_CHIPNexus.OCT4.logcount_loss: 0.2576 - val_CHIPNexus.OCT4.profile_loss: 1185.6929\n",
      "Epoch 33/200\n",
      "276/276 [==============================] - 85s 307ms/step - loss: 1223.3871 - CHIPNexus.OCT4.logcount_loss: 0.2676 - CHIPNexus.OCT4.profile_loss: 1156.4839 - val_loss: 2022.1493 - val_CHIPNexus.OCT4.logcount_loss: 0.2551 - val_CHIPNexus.OCT4.profile_loss: 1178.1176\n",
      "Epoch 34/200\n",
      "276/276 [==============================] - 81s 292ms/step - loss: 1163.0053 - CHIPNexus.OCT4.logcount_loss: 0.2701 - CHIPNexus.OCT4.profile_loss: 1095.4873 - val_loss: 2035.4119 - val_CHIPNexus.OCT4.logcount_loss: 0.2579 - val_CHIPNexus.OCT4.profile_loss: 1168.9574\n",
      "Epoch 35/200\n",
      "276/276 [==============================] - 82s 297ms/step - loss: 1205.9783 - CHIPNexus.OCT4.logcount_loss: 0.2709 - CHIPNexus.OCT4.profile_loss: 1138.2594 - val_loss: 2050.1895 - val_CHIPNexus.OCT4.logcount_loss: 0.2560 - val_CHIPNexus.OCT4.profile_loss: 1173.571591.0383 - CHIPNexus.OCT4.logc - ETA: 42s - loss: 1088.2926 - CHIPNexus.OCT4.logcount_loss: 0.2636 - CHIPNexus.OCT4.profil - ETA: 37s - loss: 1087.0233 - CHIPNexus.OCT4.logcount_loss: 0.2634 - CHIPNexus.OCT4.p - ETA: 31s -  - ETA: 8s - loss: 1216.9887 - CHIPN\n",
      "Epoch 36/200\n",
      "276/276 [==============================] - 85s 310ms/step - loss: 1202.0142 - CHIPNexus.OCT4.logcount_loss: 0.2814 - CHIPNexus.OCT4.profile_loss: 1131.6702 - val_loss: 2017.3110 - val_CHIPNexus.OCT4.logcount_loss: 0.2645 - val_CHIPNexus.OCT4.profile_loss: 1168.9471\n",
      "Epoch 37/200\n",
      "276/276 [==============================] - 81s 294ms/step - loss: 1251.9039 - CHIPNexus.OCT4.logcount_loss: 0.2672 - CHIPNexus.OCT4.profile_loss: 1185.0977 - val_loss: 2024.9454 - val_CHIPNexus.OCT4.logcount_loss: 0.2545 - val_CHIPNexus.OCT4.profile_loss: 1152.91053s - loss: 1288.3290 - CHIPNexus.OCT4.logcount_loss: 0.2670 - CHIPNexus.OCT4.profile_loss:  - ETA: 11s - loss: 1280.9501 - CHIPNexus.OCT4.logcount_loss: 0.2672 - CHIPNexus.OCT4. - ETA: 7s - loss: 1269.9945 - CHIPNexus.OCT4.\n",
      "Epoch 38/200\n",
      "276/276 [==============================] - 81s 293ms/step - loss: 1149.6834 - CHIPNexus.OCT4.logcount_loss: 0.2592 - CHIPNexus.OCT4.profile_loss: 1084.8961 - val_loss: 2034.2767 - val_CHIPNexus.OCT4.logcount_loss: 0.2531 - val_CHIPNexus.OCT4.profile_loss: 1147.4619\n",
      "Epoch 39/200\n",
      "276/276 [==============================] - 81s 294ms/step - loss: 1117.9212 - CHIPNexus.OCT4.logcount_loss: 0.2548 - CHIPNexus.OCT4.profile_loss: 1054.2247 - val_loss: 2002.3185 - val_CHIPNexus.OCT4.logcount_loss: 0.2500 - val_CHIPNexus.OCT4.profile_loss: 1143.8247\n",
      "Epoch 40/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "276/276 [==============================] - 82s 297ms/step - loss: 1162.0512 - CHIPNexus.OCT4.logcount_loss: 0.2556 - CHIPNexus.OCT4.profile_loss: 1098.1508 - val_loss: 2009.6486 - val_CHIPNexus.OCT4.logcount_loss: 0.2463 - val_CHIPNexus.OCT4.profile_loss: 1142.7151s - loss: 1086.8028 - CHIPNexus.OCT4.logcount_loss: 0.2644 - CHIPNexus.OCT4.profile_loss: 1020.695 - ETA: 56s - loss: 1087.6012 - C - ETA: 12s - loss: 1\n",
      "Epoch 41/200\n",
      "276/276 [==============================] - 86s 313ms/step - loss: 1177.0679 - CHIPNexus.OCT4.logcount_loss: 0.2677 - CHIPNexus.OCT4.profile_loss: 1110.1393 - val_loss: 2053.2654 - val_CHIPNexus.OCT4.logcount_loss: 0.2635 - val_CHIPNexus.OCT4.profile_loss: 1142.7075\n",
      "Epoch 42/200\n",
      "276/276 [==============================] - 95s 343ms/step - loss: 1197.8606 - CHIPNexus.OCT4.logcount_loss: 0.2535 - CHIPNexus.OCT4.profile_loss: 1134.4838 - val_loss: 2064.7581 - val_CHIPNexus.OCT4.logcount_loss: 0.2511 - val_CHIPNexus.OCT4.profile_loss: 1132.0292\n",
      "Epoch 43/200\n",
      "276/276 [==============================] - 98s 356ms/step - loss: 1125.3216 - CHIPNexus.OCT4.logcount_loss: 0.2443 - CHIPNexus.OCT4.profile_loss: 1064.2441 - val_loss: 1998.1332 - val_CHIPNexus.OCT4.logcount_loss: 0.2442 - val_CHIPNexus.OCT4.profile_loss: 1133.1152\n",
      "Epoch 44/200\n",
      "276/276 [==============================] - 99s 359ms/step - loss: 1115.3816 - CHIPNexus.OCT4.logcount_loss: 0.2426 - CHIPNexus.OCT4.profile_loss: 1054.7345 - val_loss: 2000.7146 - val_CHIPNexus.OCT4.logcount_loss: 0.2411 - val_CHIPNexus.OCT4.profile_loss: 1128.0459\n",
      "Epoch 45/200\n",
      "276/276 [==============================] - 104s 377ms/step - loss: 1106.5893 - CHIPNexus.OCT4.logcount_loss: 0.2377 - CHIPNexus.OCT4.profile_loss: 1047.1594 - val_loss: 1998.4646 - val_CHIPNexus.OCT4.logcount_loss: 0.2391 - val_CHIPNexus.OCT4.profile_loss: 1122.5764\n",
      "Epoch 46/200\n",
      "276/276 [==============================] - 106s 384ms/step - loss: 1109.4412 - CHIPNexus.OCT4.logcount_loss: 0.2337 - CHIPNexus.OCT4.profile_loss: 1051.0234 - val_loss: 2019.8347 - val_CHIPNexus.OCT4.logcount_loss: 0.2366 - val_CHIPNexus.OCT4.profile_loss: 1123.0403\n",
      "Epoch 47/200\n",
      "276/276 [==============================] - 106s 384ms/step - loss: 1193.4575 - CHIPNexus.OCT4.logcount_loss: 0.2459 - CHIPNexus.OCT4.profile_loss: 1131.9883 - val_loss: 1999.9880 - val_CHIPNexus.OCT4.logcount_loss: 0.2412 - val_CHIPNexus.OCT4.profile_loss: 1118.8739\n",
      "Epoch 48/200\n",
      "276/276 [==============================] - 101s 366ms/step - loss: 1126.9059 - CHIPNexus.OCT4.logcount_loss: 0.2348 - CHIPNexus.OCT4.profile_loss: 1068.2078 - val_loss: 2136.9641 - val_CHIPNexus.OCT4.logcount_loss: 0.2886 - val_CHIPNexus.OCT4.profile_loss: 1141.3414\n",
      "Epoch 49/200\n",
      "276/276 [==============================] - 102s 370ms/step - loss: 1146.2883 - CHIPNexus.OCT4.logcount_loss: 0.2369 - CHIPNexus.OCT4.profile_loss: 1087.0656 - val_loss: 1989.0411 - val_CHIPNexus.OCT4.logcount_loss: 0.2360 - val_CHIPNexus.OCT4.profile_loss: 1111.4443\n",
      "Epoch 50/200\n",
      "276/276 [==============================] - 94s 341ms/step - loss: 1105.3707 - CHIPNexus.OCT4.logcount_loss: 0.2272 - CHIPNexus.OCT4.profile_loss: 1048.5646 - val_loss: 1995.6139 - val_CHIPNexus.OCT4.logcount_loss: 0.2340 - val_CHIPNexus.OCT4.profile_loss: 1106.1136\n",
      "Epoch 51/200\n",
      "276/276 [==============================] - 104s 377ms/step - loss: 1089.1662 - CHIPNexus.OCT4.logcount_loss: 0.2272 - CHIPNexus.OCT4.profile_loss: 1032.3678 - val_loss: 1975.5127 - val_CHIPNexus.OCT4.logcount_loss: 0.2359 - val_CHIPNexus.OCT4.profile_loss: 1105.6774\n",
      "Epoch 52/200\n",
      "276/276 [==============================] - 106s 383ms/step - loss: 1096.2826 - CHIPNexus.OCT4.logcount_loss: 0.2355 - CHIPNexus.OCT4.profile_loss: 1037.4108 - val_loss: 1999.9365 - val_CHIPNexus.OCT4.logcount_loss: 0.2331 - val_CHIPNexus.OCT4.profile_loss: 1101.5693\n",
      "Epoch 53/200\n",
      "276/276 [==============================] - 102s 368ms/step - loss: 1126.5320 - CHIPNexus.OCT4.logcount_loss: 0.2272 - CHIPNexus.OCT4.profile_loss: 1069.7247 - val_loss: 1988.9482 - val_CHIPNexus.OCT4.logcount_loss: 0.2343 - val_CHIPNexus.OCT4.profile_loss: 1100.0869\n",
      "Epoch 54/200\n",
      "276/276 [==============================] - 102s 369ms/step - loss: 1094.4559 - CHIPNexus.OCT4.logcount_loss: 0.2271 - CHIPNexus.OCT4.profile_loss: 1037.6860 - val_loss: 2004.7114 - val_CHIPNexus.OCT4.logcount_loss: 0.2350 - val_CHIPNexus.OCT4.profile_loss: 1099.1637\n",
      "Epoch 55/200\n",
      "276/276 [==============================] - 104s 375ms/step - loss: 1100.8869 - CHIPNexus.OCT4.logcount_loss: 0.2270 - CHIPNexus.OCT4.profile_loss: 1044.1288 - val_loss: 1949.5321 - val_CHIPNexus.OCT4.logcount_loss: 0.2691 - val_CHIPNexus.OCT4.profile_loss: 1097.1930\n",
      "Epoch 56/200\n",
      "276/276 [==============================] - 107s 386ms/step - loss: 1094.6917 - CHIPNexus.OCT4.logcount_loss: 0.2223 - CHIPNexus.OCT4.profile_loss: 1039.1238 - val_loss: 2014.6189 - val_CHIPNexus.OCT4.logcount_loss: 0.2315 - val_CHIPNexus.OCT4.profile_loss: 1089.6228\n",
      "Epoch 57/200\n",
      "276/276 [==============================] - 106s 385ms/step - loss: 1083.5320 - CHIPNexus.OCT4.logcount_loss: 0.2209 - CHIPNexus.OCT4.profile_loss: 1028.3179 - val_loss: 2006.4222 - val_CHIPNexus.OCT4.logcount_loss: 0.2297 - val_CHIPNexus.OCT4.profile_loss: 1100.5004\n",
      "Epoch 58/200\n",
      "  3/276 [..............................] - ETA: 33s - loss: 1040.9898 - CHIPNexus.OCT4.logcount_loss: 0.2083 - CHIPNexus.OCT4.profile_loss: 988.9128 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/avanti/anaconda3/envs/py376/lib/python3.7/site-packages/keras/callbacks/callbacks.py:95: RuntimeWarning: Method (on_train_batch_end) is slow compared to the batch update (0.100222). Check your callbacks.\n",
      "  % (hook_name, delta_t_median), RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "276/276 [==============================] - 106s 383ms/step - loss: 1075.7572 - CHIPNexus.OCT4.logcount_loss: 0.2205 - CHIPNexus.OCT4.profile_loss: 1020.6386 - val_loss: 2007.0599 - val_CHIPNexus.OCT4.logcount_loss: 0.2303 - val_CHIPNexus.OCT4.profile_loss: 1086.4559\n",
      "Epoch 59/200\n",
      "276/276 [==============================] - 106s 383ms/step - loss: 1076.3613 - CHIPNexus.OCT4.logcount_loss: 0.2217 - CHIPNexus.OCT4.profile_loss: 1020.9382 - val_loss: 2000.3556 - val_CHIPNexus.OCT4.logcount_loss: 0.2336 - val_CHIPNexus.OCT4.profile_loss: 1086.6371\n",
      "Epoch 60/200\n",
      "276/276 [==============================] - 96s 348ms/step - loss: 1089.3259 - CHIPNexus.OCT4.logcount_loss: 0.2182 - CHIPNexus.OCT4.profile_loss: 1034.7645 - val_loss: 2055.4316 - val_CHIPNexus.OCT4.logcount_loss: 0.2363 - val_CHIPNexus.OCT4.profile_loss: 1092.9923\n",
      "Epoch 61/200\n",
      "276/276 [==============================] - 106s 384ms/step - loss: 1104.8517 - CHIPNexus.OCT4.logcount_loss: 0.2205 - CHIPNexus.OCT4.profile_loss: 1049.7163 - val_loss: 1950.2563 - val_CHIPNexus.OCT4.logcount_loss: 0.2516 - val_CHIPNexus.OCT4.profile_loss: 1083.3589\n",
      "Epoch 62/200\n",
      "  3/276 [..............................] - ETA: 32s - loss: 1105.2035 - CHIPNexus.OCT4.logcount_loss: 0.2495 - CHIPNexus.OCT4.profile_loss: 1042.8373"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/avanti/anaconda3/envs/py376/lib/python3.7/site-packages/keras/callbacks/callbacks.py:95: RuntimeWarning: Method (on_train_batch_end) is slow compared to the batch update (0.116878). Check your callbacks.\n",
      "  % (hook_name, delta_t_median), RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "276/276 [==============================] - 105s 381ms/step - loss: 1077.4982 - CHIPNexus.OCT4.logcount_loss: 0.2204 - CHIPNexus.OCT4.profile_loss: 1022.4024 - val_loss: 1996.2640 - val_CHIPNexus.OCT4.logcount_loss: 0.2279 - val_CHIPNexus.OCT4.profile_loss: 1079.8403\n",
      "Epoch 63/200\n",
      "276/276 [==============================] - 103s 375ms/step - loss: 1070.0337 - CHIPNexus.OCT4.logcount_loss: 0.2169 - CHIPNexus.OCT4.profile_loss: 1015.7965 - val_loss: 2003.7675 - val_CHIPNexus.OCT4.logcount_loss: 0.2276 - val_CHIPNexus.OCT4.profile_loss: 1074.2289\n",
      "Epoch 64/200\n",
      "276/276 [==============================] - 105s 380ms/step - loss: 1077.5577 - CHIPNexus.OCT4.logcount_loss: 0.2174 - CHIPNexus.OCT4.profile_loss: 1023.2178 - val_loss: 2013.2048 - val_CHIPNexus.OCT4.logcount_loss: 0.2381 - val_CHIPNexus.OCT4.profile_loss: 1073.6429\n",
      "Epoch 65/200\n",
      "276/276 [==============================] - 94s 339ms/step - loss: 1079.2823 - CHIPNexus.OCT4.logcount_loss: 0.2162 - CHIPNexus.OCT4.profile_loss: 1025.2330 - val_loss: 2008.5411 - val_CHIPNexus.OCT4.logcount_loss: 0.2279 - val_CHIPNexus.OCT4.profile_loss: 1071.0446\n",
      "val_loss 1949.5321044921875\n",
      "val_CHIPNexus.OCT4.logcount_loss 0.2690545618534088\n",
      "val_CHIPNexus.OCT4.profile_loss 1097.1929931640625\n",
      "loss 1100.9171620967386\n",
      "CHIPNexus.OCT4.logcount_loss 0.2270315\n",
      "CHIPNexus.OCT4.profile_loss 1044.1288\n",
      "Seed: 1334\n",
      "LOG SPACE PREDS IS SET TO True\n",
      "Oct4only_ctaskweight500_seed1334.h5\n",
      "Model: \"model_16\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "sequence (InputLayer)           (None, 1346, 4)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_150 (Conv1D)             (None, 1326, 64)     5440        sequence[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_103 (Lambda)             (None, 1322, 64)     0           conv1d_150[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_151 (Conv1D)             (None, 1322, 64)     12352       conv1d_150[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_103 (Add)                   (None, 1322, 64)     0           lambda_103[0][0]                 \n",
      "                                                                 conv1d_151[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_104 (Lambda)             (None, 1314, 64)     0           add_103[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_152 (Conv1D)             (None, 1314, 64)     12352       add_103[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_104 (Add)                   (None, 1314, 64)     0           lambda_104[0][0]                 \n",
      "                                                                 conv1d_152[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_105 (Lambda)             (None, 1298, 64)     0           add_104[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_153 (Conv1D)             (None, 1298, 64)     12352       add_104[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_105 (Add)                   (None, 1298, 64)     0           lambda_105[0][0]                 \n",
      "                                                                 conv1d_153[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_106 (Lambda)             (None, 1266, 64)     0           add_105[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_154 (Conv1D)             (None, 1266, 64)     12352       add_105[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_106 (Add)                   (None, 1266, 64)     0           lambda_106[0][0]                 \n",
      "                                                                 conv1d_154[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_107 (Lambda)             (None, 1202, 64)     0           add_106[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_155 (Conv1D)             (None, 1202, 64)     12352       add_106[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_107 (Add)                   (None, 1202, 64)     0           lambda_107[0][0]                 \n",
      "                                                                 conv1d_155[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_108 (Lambda)             (None, 1074, 64)     0           add_107[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_156 (Conv1D)             (None, 1074, 64)     12352       add_107[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_108 (Add)                   (None, 1074, 64)     0           lambda_108[0][0]                 \n",
      "                                                                 conv1d_156[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_18 (Gl (None, 64)           0           add_108[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "patchcap.logcount (InputLayer)  (None, 2)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_157 (Conv1D)             (None, 1000, 2)      9602        add_108[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "patchcap.profile (InputLayer)   (None, 1000, 2)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_63 (Concatenate)    (None, 66)           0           global_average_pooling1d_18[0][0]\n",
      "                                                                 patchcap.logcount[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_64 (Concatenate)    (None, 1000, 4)      0           conv1d_157[0][0]                 \n",
      "                                                                 patchcap.profile[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "CHIPNexus.OCT4.logcount (Dense) (None, 2)            134         concatenate_63[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "CHIPNexus.OCT4.profile (Conv1D) (None, 1000, 2)      10          concatenate_64[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 89,298\n",
      "Trainable params: 89,298\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "276/276 [==============================] - 79s 288ms/step - loss: 2054.2805 - CHIPNexus.OCT4.logcount_loss: 0.6006 - CHIPNexus.OCT4.profile_loss: 1753.9728 - val_loss: 2703.5645 - val_CHIPNexus.OCT4.logcount_loss: 0.3185 - val_CHIPNexus.OCT4.profile_loss: 1488.8280\n",
      "Epoch 2/200\n",
      "276/276 [==============================] - 81s 292ms/step - loss: 1876.7312 - CHIPNexus.OCT4.logcount_loss: 0.3133 - CHIPNexus.OCT4.profile_loss: 1720.0566 - val_loss: 2385.0620 - val_CHIPNexus.OCT4.logcount_loss: 0.3845 - val_CHIPNexus.OCT4.profile_loss: 1466.9987\n",
      "Epoch 3/200\n",
      "276/276 [==============================] - 84s 305ms/step - loss: 1839.7082 - CHIPNexus.OCT4.logcount_loss: 0.3042 - CHIPNexus.OCT4.profile_loss: 1687.6243 - val_loss: 2506.8677 - val_CHIPNexus.OCT4.logcount_loss: 0.2850 - val_CHIPNexus.OCT4.profile_loss: 1443.2197\n",
      "Epoch 4/200\n",
      "276/276 [==============================] - 82s 297ms/step - loss: 1801.3008 - CHIPNexus.OCT4.logcount_loss: 0.3005 - CHIPNexus.OCT4.profile_loss: 1651.0403 - val_loss: 2378.5144 - val_CHIPNexus.OCT4.logcount_loss: 0.3214 - val_CHIPNexus.OCT4.profile_loss: 1421.7344\n",
      "Epoch 5/200\n",
      "276/276 [==============================] - 84s 304ms/step - loss: 1775.9421 - CHIPNexus.OCT4.logcount_loss: 0.2993 - CHIPNexus.OCT4.profile_loss: 1626.2914 - val_loss: 2477.8496 - val_CHIPNexus.OCT4.logcount_loss: 0.2793 - val_CHIPNexus.OCT4.profile_loss: 1407.6094\n",
      "Epoch 6/200\n",
      "276/276 [==============================] - 83s 302ms/step - loss: 1755.0614 - CHIPNexus.OCT4.logcount_loss: 0.3021 - CHIPNexus.OCT4.profile_loss: 1604.0045 - val_loss: 2614.7937 - val_CHIPNexus.OCT4.logcount_loss: 0.3357 - val_CHIPNexus.OCT4.profile_loss: 1392.6862\n",
      "Epoch 7/200\n",
      "276/276 [==============================] - 85s 308ms/step - loss: 1729.3987 - CHIPNexus.OCT4.logcount_loss: 0.2929 - CHIPNexus.OCT4.profile_loss: 1582.9292 - val_loss: 2354.4624 - val_CHIPNexus.OCT4.logcount_loss: 0.2702 - val_CHIPNexus.OCT4.profile_loss: 1383.2771 1740.587 - ETA: 1\n",
      "Epoch 8/200\n",
      "276/276 [==============================] - 85s 308ms/step - loss: 1709.6281 - CHIPNexus.OCT4.logcount_loss: 0.2896 - CHIPNexus.OCT4.profile_loss: 1564.8304 - val_loss: 2463.4360 - val_CHIPNexus.OCT4.logcount_loss: 0.3012 - val_CHIPNexus.OCT4.profile_loss: 1373.1688: 0.2894 - CHIPNexus.OCT4.profile_los - ETA: 0s - loss: 1715.8722 - CHIPNexus.OCT4.logcount_loss: 0.2898 - CHIPNexus.OCT4.profile_loss: 1570.99 - ETA: 0s - loss: 1713.6897 - CHIPNexus.OCT4.logcount_loss: 0.2898 - CHIPNexus.OCT4.profile_loss: 1568.\n",
      "Epoch 9/200\n",
      "276/276 [==============================] - 83s 299ms/step - loss: 1687.0643 - CHIPNexus.OCT4.logcount_loss: 0.2840 - CHIPNexus.OCT4.profile_loss: 1545.0863 - val_loss: 2629.2693 - val_CHIPNexus.OCT4.logcount_loss: 0.3668 - val_CHIPNexus.OCT4.profile_loss: 1383.5946\n",
      "Epoch 10/200\n",
      "276/276 [==============================] - 84s 305ms/step - loss: 1676.7046 - CHIPNexus.OCT4.logcount_loss: 0.2891 - CHIPNexus.OCT4.profile_loss: 1532.1328 - val_loss: 2321.2744 - val_CHIPNexus.OCT4.logcount_loss: 0.2596 - val_CHIPNexus.OCT4.profile_loss: 1355.4415\n",
      "Epoch 11/200\n",
      "276/276 [==============================] - 85s 309ms/step - loss: 1659.5653 - CHIPNexus.OCT4.logcount_loss: 0.2815 - CHIPNexus.OCT4.profile_loss: 1518.7905 - val_loss: 2210.7607 - val_CHIPNexus.OCT4.logcount_loss: 0.2819 - val_CHIPNexus.OCT4.profile_loss: 1350.8441\n",
      "Epoch 12/200\n",
      "276/276 [==============================] - 81s 295ms/step - loss: 1635.7730 - CHIPNexus.OCT4.logcount_loss: 0.2717 - CHIPNexus.OCT4.profile_loss: 1499.9166 - val_loss: 2283.8198 - val_CHIPNexus.OCT4.logcount_loss: 0.2651 - val_CHIPNexus.OCT4.profile_loss: 1364.0194\n",
      "Epoch 13/200\n",
      "  3/276 [..............................] - ETA: 33s - loss: 1197.0354 - CHIPNexus.OCT4.logcount_loss: 0.2573 - CHIPNexus.OCT4.profile_loss: 1068.4093"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/avanti/anaconda3/envs/py376/lib/python3.7/site-packages/keras/callbacks/callbacks.py:95: RuntimeWarning: Method (on_train_batch_end) is slow compared to the batch update (0.124979). Check your callbacks.\n",
      "  % (hook_name, delta_t_median), RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "276/276 [==============================] - 84s 304ms/step - loss: 1607.6284 - CHIPNexus.OCT4.logcount_loss: 0.2638 - CHIPNexus.OCT4.profile_loss: 1475.7043 - val_loss: 2383.5088 - val_CHIPNexus.OCT4.logcount_loss: 0.2931 - val_CHIPNexus.OCT4.profile_loss: 1403.3580\n",
      "Epoch 14/200\n",
      "276/276 [==============================] - 86s 312ms/step - loss: 1632.0416 - CHIPNexus.OCT4.logcount_loss: 0.2975 - CHIPNexus.OCT4.profile_loss: 1483.2964 - val_loss: 2282.3447 - val_CHIPNexus.OCT4.logcount_loss: 0.2702 - val_CHIPNexus.OCT4.profile_loss: 1345.3550\n",
      "Epoch 15/200\n",
      "  3/276 [..............................] - ETA: 30s - loss: 1258.1176 - CHIPNexus.OCT4.logcount_loss: 0.3251 - CHIPNexus.OCT4.profile_loss: 1095.5771"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/avanti/anaconda3/envs/py376/lib/python3.7/site-packages/keras/callbacks/callbacks.py:95: RuntimeWarning: Method (on_train_batch_end) is slow compared to the batch update (0.103871). Check your callbacks.\n",
      "  % (hook_name, delta_t_median), RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "276/276 [==============================] - 85s 307ms/step - loss: 1590.6143 - CHIPNexus.OCT4.logcount_loss: 0.2820 - CHIPNexus.OCT4.profile_loss: 1449.6013 - val_loss: 3278.9292 - val_CHIPNexus.OCT4.logcount_loss: 0.4791 - val_CHIPNexus.OCT4.profile_loss: 1595.7908\n",
      "Epoch 16/200\n",
      "276/276 [==============================] - 87s 315ms/step - loss: 1607.2797 - CHIPNexus.OCT4.logcount_loss: 0.3043 - CHIPNexus.OCT4.profile_loss: 1455.1257 - val_loss: 2461.7798 - val_CHIPNexus.OCT4.logcount_loss: 0.2743 - val_CHIPNexus.OCT4.profile_loss: 1389.4800\n",
      "Epoch 17/200\n",
      "276/276 [==============================] - 84s 303ms/step - loss: 1474.9035 - CHIPNexus.OCT4.logcount_loss: 0.2880 - CHIPNexus.OCT4.profile_loss: 1330.9000 - val_loss: 2206.2202 - val_CHIPNexus.OCT4.logcount_loss: 0.3013 - val_CHIPNexus.OCT4.profile_loss: 1322.8344\n",
      "Epoch 18/200\n",
      "276/276 [==============================] - 87s 314ms/step - loss: 1530.5098 - CHIPNexus.OCT4.logcount_loss: 0.2867 - CHIPNexus.OCT4.profile_loss: 1387.1844 - val_loss: 3623.6150 - val_CHIPNexus.OCT4.logcount_loss: 0.5770 - val_CHIPNexus.OCT4.profile_loss: 1585.9580\n",
      "Epoch 19/200\n",
      "276/276 [==============================] - 86s 310ms/step - loss: 1505.9839 - CHIPNexus.OCT4.logcount_loss: 0.3134 - CHIPNexus.OCT4.profile_loss: 1349.2633 - val_loss: 2339.7578 - val_CHIPNexus.OCT4.logcount_loss: 0.2745 - val_CHIPNexus.OCT4.profile_loss: 1339.4999\n",
      "Epoch 20/200\n",
      "276/276 [==============================] - 80s 291ms/step - loss: 1386.3201 - CHIPNexus.OCT4.logcount_loss: 0.2882 - CHIPNexus.OCT4.profile_loss: 1242.2090 - val_loss: 2343.6367 - val_CHIPNexus.OCT4.logcount_loss: 0.2693 - val_CHIPNexus.OCT4.profile_loss: 1319.1354 - CHIPNexus.OCT4.logcount_loss: 0.2866 - CHIPNexus.OCT4.profi - ETA: 33s - loss: 1225.5160 - CHIPNexus.OCT4.logcount_loss - ETA: 22s - loss: 1221.2090 - CHIPNexus.OCT4.logcount_loss: 0. - ETA: 10s - loss: 1419.2549 - CHIPNexus.OCT4.logcount_loss: 0.2920 - CHIPNexus - ETA: 6s - loss: 1407.3852 - CHIPNexus.OCT4.logcou\n",
      "Epoch 21/200\n",
      "276/276 [==============================] - 80s 290ms/step - loss: 1456.3058 - CHIPNexus.OCT4.logcount_loss: 0.2926 - CHIPNexus.OCT4.profile_loss: 1310.0228 - val_loss: 2306.8127 - val_CHIPNexus.OCT4.logcount_loss: 0.2682 - val_CHIPNexus.OCT4.profile_loss: 1303.1102TA: 36s - loss: 1746.349\n",
      "Epoch 22/200\n",
      "276/276 [==============================] - 85s 309ms/step - loss: 1346.5746 - CHIPNexus.OCT4.logcount_loss: 0.2952 - CHIPNexus.OCT4.profile_loss: 1198.9546 - val_loss: 2262.8074 - val_CHIPNexus.OCT4.logcount_loss: 0.2682 - val_CHIPNexus.OCT4.profile_loss: 1288.7731\n",
      "Epoch 23/200\n",
      "276/276 [==============================] - 84s 304ms/step - loss: 1542.8544 - CHIPNexus.OCT4.logcount_loss: 0.2860 - CHIPNexus.OCT4.profile_loss: 1399.8646 - val_loss: 2214.8955 - val_CHIPNexus.OCT4.logcount_loss: 0.2669 - val_CHIPNexus.OCT4.profile_loss: 1275.9385\n",
      "Epoch 24/200\n",
      "276/276 [==============================] - 83s 301ms/step - loss: 1497.2290 - CHIPNexus.OCT4.logcount_loss: 0.2882 - CHIPNexus.OCT4.profile_loss: 1353.1073 - val_loss: 2289.9392 - val_CHIPNexus.OCT4.logcount_loss: 0.2693 - val_CHIPNexus.OCT4.profile_loss: 1272.306084 - CHIPNexus.OCT4.profile_los\n",
      "Epoch 25/200\n",
      "276/276 [==============================] - 83s 301ms/step - loss: 1322.1425 - CHIPNexus.OCT4.logcount_loss: 0.2799 - CHIPNexus.OCT4.profile_loss: 1182.2104 - val_loss: 2184.4312 - val_CHIPNexus.OCT4.logcount_loss: 0.2689 - val_CHIPNexus.OCT4.profile_loss: 1264.7765\n",
      "Epoch 26/200\n",
      "276/276 [==============================] - 81s 294ms/step - loss: 1362.9433 - CHIPNexus.OCT4.logcount_loss: 0.2794 - CHIPNexus.OCT4.profile_loss: 1223.2438 - val_loss: 2418.0413 - val_CHIPNexus.OCT4.logcount_loss: 0.2670 - val_CHIPNexus.OCT4.profile_loss: 1296.9042\n",
      "Epoch 27/200\n",
      "276/276 [==============================] - 84s 306ms/step - loss: 1382.9114 - CHIPNexus.OCT4.logcount_loss: 0.2887 - CHIPNexus.OCT4.profile_loss: 1238.5725 - val_loss: 2316.6831 - val_CHIPNexus.OCT4.logcount_loss: 0.2701 - val_CHIPNexus.OCT4.profile_loss: 1263.2238\n",
      "Epoch 28/200\n",
      "276/276 [==============================] - 84s 304ms/step - loss: 1491.0238 - CHIPNexus.OCT4.logcount_loss: 0.2849 - CHIPNexus.OCT4.profile_loss: 1348.5618 - val_loss: 2180.3813 - val_CHIPNexus.OCT4.logcount_loss: 0.2673 - val_CHIPNexus.OCT4.profile_loss: 1247.3654\n",
      "Epoch 29/200\n",
      "276/276 [==============================] - 89s 322ms/step - loss: 1424.4745 - CHIPNexus.OCT4.logcount_loss: 0.2804 - CHIPNexus.OCT4.profile_loss: 1284.2567 - val_loss: 2266.2124 - val_CHIPNexus.OCT4.logcount_loss: 0.2618 - val_CHIPNexus.OCT4.profile_loss: 1247.0769\n",
      "Epoch 30/200\n",
      "276/276 [==============================] - 86s 311ms/step - loss: 1319.5268 - CHIPNexus.OCT4.logcount_loss: 0.2771 - CHIPNexus.OCT4.profile_loss: 1180.9985 - val_loss: 2241.4736 - val_CHIPNexus.OCT4.logcount_loss: 0.2768 - val_CHIPNexus.OCT4.profile_loss: 1243.0863\n",
      "Epoch 31/200\n",
      "276/276 [==============================] - 83s 301ms/step - loss: 1299.7829 - CHIPNexus.OCT4.logcount_loss: 0.2739 - CHIPNexus.OCT4.profile_loss: 1162.8335 - val_loss: 2187.8647 - val_CHIPNexus.OCT4.logcount_loss: 0.2617 - val_CHIPNexus.OCT4.profile_loss: 1226.5765ETA: 9s - loss: 1319.6680 - C\n",
      "Epoch 32/200\n",
      "276/276 [==============================] - 84s 303ms/step - loss: 1374.5012 - CHIPNexus.OCT4.logcount_loss: 0.2724 - CHIPNexus.OCT4.profile_loss: 1238.3058 - val_loss: 2184.5161 - val_CHIPNexus.OCT4.logcount_loss: 0.2611 - val_CHIPNexus.OCT4.profile_loss: 1223.2438\n",
      "Epoch 33/200\n",
      "276/276 [==============================] - 88s 321ms/step - loss: 1269.0315 - CHIPNexus.OCT4.logcount_loss: 0.2695 - CHIPNexus.OCT4.profile_loss: 1134.2686 - val_loss: 2155.7712 - val_CHIPNexus.OCT4.logcount_loss: 0.2702 - val_CHIPNexus.OCT4.profile_loss: 1214.2788\n",
      "Epoch 34/200\n",
      "276/276 [==============================] - 87s 314ms/step - loss: 1263.6466 - CHIPNexus.OCT4.logcount_loss: 0.2657 - CHIPNexus.OCT4.profile_loss: 1130.7769 - val_loss: 2341.6758 - val_CHIPNexus.OCT4.logcount_loss: 0.2569 - val_CHIPNexus.OCT4.profile_loss: 1243.8329\n",
      "Epoch 35/200\n",
      "276/276 [==============================] - 83s 300ms/step - loss: 1295.3876 - CHIPNexus.OCT4.logcount_loss: 0.2689 - CHIPNexus.OCT4.profile_loss: 1160.9244 - val_loss: 2193.5042 - val_CHIPNexus.OCT4.logcount_loss: 0.2547 - val_CHIPNexus.OCT4.profile_loss: 1208.96417s - loss: 1339.3594 - CHIPNexus.OCT4.logcount_loss: 0.2703 - CHIPNexus.OCT4.profile_loss: 1204.211 -\n",
      "Epoch 36/200\n",
      "276/276 [==============================] - 84s 305ms/step - loss: 1348.3979 - CHIPNexus.OCT4.logcount_loss: 0.2636 - CHIPNexus.OCT4.profile_loss: 1216.6154 - val_loss: 2261.0261 - val_CHIPNexus.OCT4.logcount_loss: 0.2530 - val_CHIPNexus.OCT4.profile_loss: 1201.5402\n",
      "Epoch 37/200\n",
      "276/276 [==============================] - 84s 305ms/step - loss: 1247.9292 - CHIPNexus.OCT4.logcount_loss: 0.2539 - CHIPNexus.OCT4.profile_loss: 1120.9662 - val_loss: 2280.1748 - val_CHIPNexus.OCT4.logcount_loss: 0.2470 - val_CHIPNexus.OCT4.profile_loss: 1204.6655\n",
      "Epoch 38/200\n",
      "276/276 [==============================] - 85s 307ms/step - loss: 1235.7949 - CHIPNexus.OCT4.logcount_loss: 0.2534 - CHIPNexus.OCT4.profile_loss: 1109.0898 - val_loss: 2147.2698 - val_CHIPNexus.OCT4.logcount_loss: 0.2511 - val_CHIPNexus.OCT4.profile_loss: 1191.38676 - CHIPNexus.OCT4.profi\n",
      "Epoch 39/200\n",
      "276/276 [==============================] - 89s 322ms/step - loss: 1273.8313 - CHIPNexus.OCT4.logcount_loss: 0.2377 - CHIPNexus.OCT4.profile_loss: 1154.9594 - val_loss: 2136.5803 - val_CHIPNexus.OCT4.logcount_loss: 0.2642 - val_CHIPNexus.OCT4.profile_loss: 1185.8896\n",
      "Epoch 40/200\n",
      "276/276 [==============================] - 87s 316ms/step - loss: 1206.8297 - CHIPNexus.OCT4.logcount_loss: 0.2346 - CHIPNexus.OCT4.profile_loss: 1089.5071 - val_loss: 2163.0535 - val_CHIPNexus.OCT4.logcount_loss: 0.2577 - val_CHIPNexus.OCT4.profile_loss: 1186.2435\n",
      "Epoch 41/200\n",
      "276/276 [==============================] - 84s 303ms/step - loss: 1205.0207 - CHIPNexus.OCT4.logcount_loss: 0.2326 - CHIPNexus.OCT4.profile_loss: 1088.7273 - val_loss: 2285.7603 - val_CHIPNexus.OCT4.logcount_loss: 0.2496 - val_CHIPNexus.OCT4.profile_loss: 1180.7946loss: 1205.9560 - CHIPNexus.OCT4.logcount_loss: 0.2326 - CHIPNexus.OCT4.profile_loss: 108\n",
      "Epoch 42/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "276/276 [==============================] - 86s 311ms/step - loss: 1211.0838 - CHIPNexus.OCT4.logcount_loss: 0.2345 - CHIPNexus.OCT4.profile_loss: 1093.8523 - val_loss: 2207.3164 - val_CHIPNexus.OCT4.logcount_loss: 0.3086 - val_CHIPNexus.OCT4.profile_loss: 1199.7065\n",
      "Epoch 43/200\n",
      "276/276 [==============================] - 81s 294ms/step - loss: 1287.9483 - CHIPNexus.OCT4.logcount_loss: 0.2469 - CHIPNexus.OCT4.profile_loss: 1164.4762 - val_loss: 2186.5732 - val_CHIPNexus.OCT4.logcount_loss: 0.2449 - val_CHIPNexus.OCT4.profile_loss: 1166.35086819 - CHIPNexus.OCT4.logcount_l - ETA: 40s - loss: 1492.6115 - - ETA: 21s - loss: 1353.2400 - CHIPNexus.OCT4.logcount_loss: 0.2518 - CHIPNexus.OCT4.profile_lo - ETA: 18s - loss: 1341.1230 - CHIPNexus.OCT4.log - ETA: 7s - loss: 1302.5709 - CHIPNexus.OCT4.logc\n",
      "Epoch 44/200\n",
      "276/276 [==============================] - 86s 312ms/step - loss: 1195.2157 - CHIPNexus.OCT4.logcount_loss: 0.2289 - CHIPNexus.OCT4.profile_loss: 1080.7606 - val_loss: 2338.1567 - val_CHIPNexus.OCT4.logcount_loss: 0.2638 - val_CHIPNexus.OCT4.profile_loss: 1217.9261\n",
      "Epoch 45/200\n",
      "276/276 [==============================] - 81s 292ms/step - loss: 1234.1280 - CHIPNexus.OCT4.logcount_loss: 0.2718 - CHIPNexus.OCT4.profile_loss: 1098.2391 - val_loss: 2302.1443 - val_CHIPNexus.OCT4.logcount_loss: 0.2692 - val_CHIPNexus.OCT4.profile_loss: 1162.669792 - CHIPNex\n",
      "Epoch 46/200\n",
      "276/276 [==============================] - 85s 308ms/step - loss: 1202.8035 - CHIPNexus.OCT4.logcount_loss: 0.2415 - CHIPNexus.OCT4.profile_loss: 1082.0740 - val_loss: 2149.0366 - val_CHIPNexus.OCT4.logcount_loss: 0.2368 - val_CHIPNexus.OCT4.profile_loss: 1159.1517\n",
      "Epoch 47/200\n",
      "276/276 [==============================] - 86s 311ms/step - loss: 1186.4906 - CHIPNexus.OCT4.logcount_loss: 0.2302 - CHIPNexus.OCT4.profile_loss: 1071.3990 - val_loss: 2148.8821 - val_CHIPNexus.OCT4.logcount_loss: 0.2334 - val_CHIPNexus.OCT4.profile_loss: 1156.7256\n",
      "Epoch 48/200\n",
      "276/276 [==============================] - 83s 301ms/step - loss: 1230.1908 - CHIPNexus.OCT4.logcount_loss: 0.2251 - CHIPNexus.OCT4.profile_loss: 1117.6273 - val_loss: 2176.9683 - val_CHIPNexus.OCT4.logcount_loss: 0.2340 - val_CHIPNexus.OCT4.profile_loss: 1155.2371\n",
      "Epoch 49/200\n",
      "276/276 [==============================] - 83s 301ms/step - loss: 1182.1962 - CHIPNexus.OCT4.logcount_loss: 0.2265 - CHIPNexus.OCT4.profile_loss: 1068.9415 - val_loss: 2217.2891 - val_CHIPNexus.OCT4.logcount_loss: 0.2393 - val_CHIPNexus.OCT4.profile_loss: 1143.5201\n",
      "val_loss 2136.580322265625\n",
      "val_CHIPNexus.OCT4.logcount_loss 0.2642103135585785\n",
      "val_CHIPNexus.OCT4.profile_loss 1185.8896484375\n",
      "loss 1273.8575894512226\n",
      "CHIPNexus.OCT4.logcount_loss 0.23774315\n",
      "CHIPNexus.OCT4.profile_loss 1154.9594\n",
      "Seed: 1434\n",
      "LOG SPACE PREDS IS SET TO True\n",
      "Oct4only_ctaskweight0_seed1434.h5\n",
      "Model: \"model_17\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "sequence (InputLayer)           (None, 1346, 4)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_158 (Conv1D)             (None, 1326, 64)     5440        sequence[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_109 (Lambda)             (None, 1322, 64)     0           conv1d_158[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_159 (Conv1D)             (None, 1322, 64)     12352       conv1d_158[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_109 (Add)                   (None, 1322, 64)     0           lambda_109[0][0]                 \n",
      "                                                                 conv1d_159[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_110 (Lambda)             (None, 1314, 64)     0           add_109[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_160 (Conv1D)             (None, 1314, 64)     12352       add_109[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_110 (Add)                   (None, 1314, 64)     0           lambda_110[0][0]                 \n",
      "                                                                 conv1d_160[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_111 (Lambda)             (None, 1298, 64)     0           add_110[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_161 (Conv1D)             (None, 1298, 64)     12352       add_110[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_111 (Add)                   (None, 1298, 64)     0           lambda_111[0][0]                 \n",
      "                                                                 conv1d_161[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_112 (Lambda)             (None, 1266, 64)     0           add_111[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_162 (Conv1D)             (None, 1266, 64)     12352       add_111[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_112 (Add)                   (None, 1266, 64)     0           lambda_112[0][0]                 \n",
      "                                                                 conv1d_162[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_113 (Lambda)             (None, 1202, 64)     0           add_112[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_163 (Conv1D)             (None, 1202, 64)     12352       add_112[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_113 (Add)                   (None, 1202, 64)     0           lambda_113[0][0]                 \n",
      "                                                                 conv1d_163[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_114 (Lambda)             (None, 1074, 64)     0           add_113[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_164 (Conv1D)             (None, 1074, 64)     12352       add_113[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_114 (Add)                   (None, 1074, 64)     0           lambda_114[0][0]                 \n",
      "                                                                 conv1d_164[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_19 (Gl (None, 64)           0           add_114[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "patchcap.logcount (InputLayer)  (None, 2)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_165 (Conv1D)             (None, 1000, 2)      9602        add_114[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "patchcap.profile (InputLayer)   (None, 1000, 2)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_65 (Concatenate)    (None, 66)           0           global_average_pooling1d_19[0][0]\n",
      "                                                                 patchcap.logcount[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_66 (Concatenate)    (None, 1000, 4)      0           conv1d_165[0][0]                 \n",
      "                                                                 patchcap.profile[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "CHIPNexus.OCT4.logcount (Dense) (None, 2)            134         concatenate_65[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "CHIPNexus.OCT4.profile (Conv1D) (None, 1000, 2)      10          concatenate_66[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 89,298\n",
      "Trainable params: 89,298\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "276/276 [==============================] - 79s 286ms/step - loss: 1777.7146 - CHIPNexus.OCT4.logcount_loss: 68.1603 - CHIPNexus.OCT4.profile_loss: 1777.7137 - val_loss: 2039.9819 - val_CHIPNexus.OCT4.logcount_loss: 65.1704 - val_CHIPNexus.OCT4.profile_loss: 1369.8438\n",
      "Epoch 2/200\n",
      "276/276 [==============================] - 82s 296ms/step - loss: 1716.3510 - CHIPNexus.OCT4.logcount_loss: 52.7424 - CHIPNexus.OCT4.profile_loss: 1716.3497 - val_loss: 1980.6111 - val_CHIPNexus.OCT4.logcount_loss: 50.2029 - val_CHIPNexus.OCT4.profile_loss: 1343.5178\n",
      "Epoch 3/200\n",
      "276/276 [==============================] - 85s 307ms/step - loss: 1682.7725 - CHIPNexus.OCT4.logcount_loss: 50.2396 - CHIPNexus.OCT4.profile_loss: 1682.7715 - val_loss: 1940.9919 - val_CHIPNexus.OCT4.logcount_loss: 48.7111 - val_CHIPNexus.OCT4.profile_loss: 1328.7972\n",
      "Epoch 4/200\n",
      "276/276 [==============================] - 84s 305ms/step - loss: 1647.1109 - CHIPNexus.OCT4.logcount_loss: 47.9700 - CHIPNexus.OCT4.profile_loss: 1647.1108 - val_loss: 1938.5134 - val_CHIPNexus.OCT4.logcount_loss: 48.6124 - val_CHIPNexus.OCT4.profile_loss: 1311.0868\n",
      "Epoch 5/200\n",
      "276/276 [==============================] - 84s 304ms/step - loss: 1620.4803 - CHIPNexus.OCT4.logcount_loss: 52.6011 - CHIPNexus.OCT4.profile_loss: 1620.4796 - val_loss: 1904.2233 - val_CHIPNexus.OCT4.logcount_loss: 52.0931 - val_CHIPNexus.OCT4.profile_loss: 1297.6617\n",
      "Epoch 6/200\n",
      "276/276 [==============================] - 86s 313ms/step - loss: 1590.3219 - CHIPNexus.OCT4.logcount_loss: 55.5950 - CHIPNexus.OCT4.profile_loss: 1590.3228 - val_loss: 1917.7112 - val_CHIPNexus.OCT4.logcount_loss: 58.2978 - val_CHIPNexus.OCT4.profile_loss: 1288.1364\n",
      "Epoch 7/200\n",
      "276/276 [==============================] - 82s 296ms/step - loss: 1567.7816 - CHIPNexus.OCT4.logcount_loss: 49.3354 - CHIPNexus.OCT4.profile_loss: 1567.7810 - val_loss: 1915.8921 - val_CHIPNexus.OCT4.logcount_loss: 49.5473 - val_CHIPNexus.OCT4.profile_loss: 1279.9333ss: 1596.663 - ETA: 3s - loss: 1594.4478 - CHIPNexus.OCT4.logcount_loss: 49.3869 - CHIPNexus.OCT4.profile_lo\n",
      "Epoch 8/200\n",
      "276/276 [==============================] - 82s 298ms/step - loss: 1537.1132 - CHIPNexus.OCT4.logcount_loss: 48.6953 - CHIPNexus.OCT4.profile_loss: 1537.1132 - val_loss: 1933.7452 - val_CHIPNexus.OCT4.logcount_loss: 49.2375 - val_CHIPNexus.OCT4.profile_loss: 1272.4052\n",
      "Epoch 9/200\n",
      "276/276 [==============================] - 89s 322ms/step - loss: 1505.8711 - CHIPNexus.OCT4.logcount_loss: 53.6622 - CHIPNexus.OCT4.profile_loss: 1505.8702 - val_loss: 1930.2144 - val_CHIPNexus.OCT4.logcount_loss: 56.8849 - val_CHIPNexus.OCT4.profile_loss: 1262.1014\n",
      "Epoch 10/200\n",
      "276/276 [==============================] - 87s 315ms/step - loss: 1476.5583 - CHIPNexus.OCT4.logcount_loss: 56.2048 - CHIPNexus.OCT4.profile_loss: 1476.5581 - val_loss: 2059.8157 - val_CHIPNexus.OCT4.logcount_loss: 56.8896 - val_CHIPNexus.OCT4.profile_loss: 1328.8199\n",
      "Epoch 11/200\n",
      "276/276 [==============================] - 85s 307ms/step - loss: 1483.8456 - CHIPNexus.OCT4.logcount_loss: 72.6784 - CHIPNexus.OCT4.profile_loss: 1483.8456 - val_loss: 1925.8870 - val_CHIPNexus.OCT4.logcount_loss: 74.2813 - val_CHIPNexus.OCT4.profile_loss: 1260.1810\n",
      "Epoch 12/200\n",
      "276/276 [==============================] - 84s 304ms/step - loss: 1473.6170 - CHIPNexus.OCT4.logcount_loss: 60.6349 - CHIPNexus.OCT4.profile_loss: 1473.6167 - val_loss: 1962.3016 - val_CHIPNexus.OCT4.logcount_loss: 57.5019 - val_CHIPNexus.OCT4.profile_loss: 1254.5968\n",
      "Epoch 13/200\n",
      "276/276 [==============================] - 88s 317ms/step - loss: 1429.5751 - CHIPNexus.OCT4.logcount_loss: 56.8297 - CHIPNexus.OCT4.profile_loss: 1429.5742 - val_loss: 2035.6541 - val_CHIPNexus.OCT4.logcount_loss: 52.2926 - val_CHIPNexus.OCT4.profile_loss: 1313.7910\n",
      "Epoch 14/200\n",
      "276/276 [==============================] - 82s 297ms/step - loss: 1369.4576 - CHIPNexus.OCT4.logcount_loss: 51.5758 - CHIPNexus.OCT4.profile_loss: 1369.4575 - val_loss: 1936.9202 - val_CHIPNexus.OCT4.logcount_loss: 49.3781 - val_CHIPNexus.OCT4.profile_loss: 1239.1770\n",
      "Epoch 15/200\n",
      "276/276 [==============================] - 88s 317ms/step - loss: 1324.4067 - CHIPNexus.OCT4.logcount_loss: 57.6135 - CHIPNexus.OCT4.profile_loss: 1324.4072 - val_loss: 1938.9720 - val_CHIPNexus.OCT4.logcount_loss: 64.4307 - val_CHIPNexus.OCT4.profile_loss: 1237.1780 56.7599\n",
      "val_loss 1904.2232666015625\n",
      "val_CHIPNexus.OCT4.logcount_loss 52.09312438964844\n",
      "val_CHIPNexus.OCT4.profile_loss 1297.6617431640625\n",
      "loss 1620.6618748775081\n",
      "CHIPNexus.OCT4.logcount_loss 52.60106\n",
      "CHIPNexus.OCT4.profile_loss 1620.4796\n",
      "Seed: 1434\n",
      "LOG SPACE PREDS IS SET TO True\n",
      "Oct4only_ctaskweight125_seed1434.h5\n",
      "Model: \"model_18\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "sequence (InputLayer)           (None, 1346, 4)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_166 (Conv1D)             (None, 1326, 64)     5440        sequence[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_115 (Lambda)             (None, 1322, 64)     0           conv1d_166[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_167 (Conv1D)             (None, 1322, 64)     12352       conv1d_166[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_115 (Add)                   (None, 1322, 64)     0           lambda_115[0][0]                 \n",
      "                                                                 conv1d_167[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_116 (Lambda)             (None, 1314, 64)     0           add_115[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_168 (Conv1D)             (None, 1314, 64)     12352       add_115[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_116 (Add)                   (None, 1314, 64)     0           lambda_116[0][0]                 \n",
      "                                                                 conv1d_168[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_117 (Lambda)             (None, 1298, 64)     0           add_116[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_169 (Conv1D)             (None, 1298, 64)     12352       add_116[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_117 (Add)                   (None, 1298, 64)     0           lambda_117[0][0]                 \n",
      "                                                                 conv1d_169[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_118 (Lambda)             (None, 1266, 64)     0           add_117[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_170 (Conv1D)             (None, 1266, 64)     12352       add_117[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_118 (Add)                   (None, 1266, 64)     0           lambda_118[0][0]                 \n",
      "                                                                 conv1d_170[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_119 (Lambda)             (None, 1202, 64)     0           add_118[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_171 (Conv1D)             (None, 1202, 64)     12352       add_118[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_119 (Add)                   (None, 1202, 64)     0           lambda_119[0][0]                 \n",
      "                                                                 conv1d_171[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_120 (Lambda)             (None, 1074, 64)     0           add_119[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_172 (Conv1D)             (None, 1074, 64)     12352       add_119[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_120 (Add)                   (None, 1074, 64)     0           lambda_120[0][0]                 \n",
      "                                                                 conv1d_172[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_20 (Gl (None, 64)           0           add_120[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "patchcap.logcount (InputLayer)  (None, 2)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_173 (Conv1D)             (None, 1000, 2)      9602        add_120[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "patchcap.profile (InputLayer)   (None, 1000, 2)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_67 (Concatenate)    (None, 66)           0           global_average_pooling1d_20[0][0]\n",
      "                                                                 patchcap.logcount[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_68 (Concatenate)    (None, 1000, 4)      0           conv1d_173[0][0]                 \n",
      "                                                                 patchcap.profile[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "CHIPNexus.OCT4.logcount (Dense) (None, 2)            134         concatenate_67[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "CHIPNexus.OCT4.profile (Conv1D) (None, 1000, 2)      10          concatenate_68[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 89,298\n",
      "Trainable params: 89,298\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "276/276 [==============================] - 81s 295ms/step - loss: 1839.0781 - CHIPNexus.OCT4.logcount_loss: 0.8776 - CHIPNexus.OCT4.profile_loss: 1729.3768 - val_loss: 2379.1858 - val_CHIPNexus.OCT4.logcount_loss: 0.3694 - val_CHIPNexus.OCT4.profile_loss: 1371.4683\n",
      "Epoch 2/200\n",
      "276/276 [==============================] - 82s 299ms/step - loss: 1706.5761 - CHIPNexus.OCT4.logcount_loss: 0.3581 - CHIPNexus.OCT4.profile_loss: 1661.8116 - val_loss: 2304.4885 - val_CHIPNexus.OCT4.logcount_loss: 0.3571 - val_CHIPNexus.OCT4.profile_loss: 1325.8301\n",
      "Epoch 3/200\n",
      "276/276 [==============================] - 83s 302ms/step - loss: 1656.9685 - CHIPNexus.OCT4.logcount_loss: 0.3537 - CHIPNexus.OCT4.profile_loss: 1612.7488 - val_loss: 2273.8711 - val_CHIPNexus.OCT4.logcount_loss: 0.3494 - val_CHIPNexus.OCT4.profile_loss: 1302.0387\n",
      "Epoch 4/200\n",
      "276/276 [==============================] - 82s 298ms/step - loss: 1617.0425 - CHIPNexus.OCT4.logcount_loss: 0.3403 - CHIPNexus.OCT4.profile_loss: 1574.5040 - val_loss: 2294.9268 - val_CHIPNexus.OCT4.logcount_loss: 0.4676 - val_CHIPNexus.OCT4.profile_loss: 1289.7731\n",
      "Epoch 5/200\n",
      "276/276 [==============================] - 82s 299ms/step - loss: 1584.9031 - CHIPNexus.OCT4.logcount_loss: 0.3374 - CHIPNexus.OCT4.profile_loss: 1542.7255 - val_loss: 2199.1304 - val_CHIPNexus.OCT4.logcount_loss: 0.3317 - val_CHIPNexus.OCT4.profile_loss: 1278.9495\n",
      "Epoch 6/200\n",
      "276/276 [==============================] - 82s 298ms/step - loss: 1556.6115 - CHIPNexus.OCT4.logcount_loss: 0.3216 - CHIPNexus.OCT4.profile_loss: 1516.4137 - val_loss: 2139.0691 - val_CHIPNexus.OCT4.logcount_loss: 0.3068 - val_CHIPNexus.OCT4.profile_loss: 1271.7433\n",
      "Epoch 7/200\n",
      "276/276 [==============================] - 82s 299ms/step - loss: 1535.9941 - CHIPNexus.OCT4.logcount_loss: 0.3148 - CHIPNexus.OCT4.profile_loss: 1496.6469 - val_loss: 2101.1843 - val_CHIPNexus.OCT4.logcount_loss: 0.3062 - val_CHIPNexus.OCT4.profile_loss: 1261.0460T4.lo\n",
      "Epoch 8/200\n",
      "276/276 [==============================] - 86s 312ms/step - loss: 1513.0770 - CHIPNexus.OCT4.logcount_loss: 0.3050 - CHIPNexus.OCT4.profile_loss: 1474.9485 - val_loss: 2028.6799 - val_CHIPNexus.OCT4.logcount_loss: 0.3308 - val_CHIPNexus.OCT4.profile_loss: 1249.9119\n",
      "Epoch 9/200\n",
      "  3/276 [..............................] - ETA: 31s - loss: 1093.3681 - CHIPNexus.OCT4.logcount_loss: 0.2986 - CHIPNexus.OCT4.profile_loss: 1056.0409"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/avanti/anaconda3/envs/py376/lib/python3.7/site-packages/keras/callbacks/callbacks.py:95: RuntimeWarning: Method (on_train_batch_end) is slow compared to the batch update (0.126607). Check your callbacks.\n",
      "  % (hook_name, delta_t_median), RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "276/276 [==============================] - 83s 300ms/step - loss: 1491.7358 - CHIPNexus.OCT4.logcount_loss: 0.3034 - CHIPNexus.OCT4.profile_loss: 1453.8151 - val_loss: 2227.6101 - val_CHIPNexus.OCT4.logcount_loss: 0.3192 - val_CHIPNexus.OCT4.profile_loss: 1288.2317\n",
      "Epoch 10/200\n",
      "276/276 [==============================] - 83s 299ms/step - loss: 1475.8707 - CHIPNexus.OCT4.logcount_loss: 0.2999 - CHIPNexus.OCT4.profile_loss: 1438.3794 - val_loss: 2067.6736 - val_CHIPNexus.OCT4.logcount_loss: 0.2786 - val_CHIPNexus.OCT4.profile_loss: 1240.5382A: 29s - loss: 1078.8058 - CHIPNexus.OCT4.logcount_loss:\n",
      "Epoch 11/200\n",
      "276/276 [==============================] - 82s 299ms/step - loss: 1453.2344 - CHIPNexus.OCT4.logcount_loss: 0.2932 - CHIPNexus.OCT4.profile_loss: 1416.5815 - val_loss: 2152.9983 - val_CHIPNexus.OCT4.logcount_loss: 0.2822 - val_CHIPNexus.OCT4.profile_loss: 1246.8806\n",
      "Epoch 12/200\n",
      "276/276 [==============================] - 82s 296ms/step - loss: 1433.6572 - CHIPNexus.OCT4.logcount_loss: 0.2890 - CHIPNexus.OCT4.profile_loss: 1397.5269 - val_loss: 2159.9126 - val_CHIPNexus.OCT4.logcount_loss: 0.2721 - val_CHIPNexus.OCT4.profile_loss: 1244.1141xus.OCT4.logcou - ETA: 1\n",
      "Epoch 13/200\n",
      "276/276 [==============================] - 85s 309ms/step - loss: 1418.4020 - CHIPNexus.OCT4.logcount_loss: 0.2996 - CHIPNexus.OCT4.profile_loss: 1380.9542 - val_loss: 2005.4601 - val_CHIPNexus.OCT4.logcount_loss: 0.2767 - val_CHIPNexus.OCT4.profile_loss: 1214.2307\n",
      "Epoch 14/200\n",
      "276/276 [==============================] - 84s 304ms/step - loss: 1391.9232 - CHIPNexus.OCT4.logcount_loss: 0.2911 - CHIPNexus.OCT4.profile_loss: 1355.5370 - val_loss: 2228.5042 - val_CHIPNexus.OCT4.logcount_loss: 0.2847 - val_CHIPNexus.OCT4.profile_loss: 1285.3936\n",
      "Epoch 15/200\n",
      "276/276 [==============================] - 81s 294ms/step - loss: 1380.6227 - CHIPNexus.OCT4.logcount_loss: 0.3012 - CHIPNexus.OCT4.profile_loss: 1342.9723 - val_loss: 2040.0701 - val_CHIPNexus.OCT4.logcount_loss: 0.2639 - val_CHIPNexus.OCT4.profile_loss: 1206.5649\n",
      "Epoch 16/200\n",
      "276/276 [==============================] - 81s 294ms/step - loss: 1354.4535 - CHIPNexus.OCT4.logcount_loss: 0.2961 - CHIPNexus.OCT4.profile_loss: 1317.4355 - val_loss: 2035.8812 - val_CHIPNexus.OCT4.logcount_loss: 0.2628 - val_CHIPNexus.OCT4.profile_loss: 1203.0425\n",
      "Epoch 17/200\n",
      "276/276 [==============================] - 85s 308ms/step - loss: 1331.5364 - CHIPNexus.OCT4.logcount_loss: 0.2854 - CHIPNexus.OCT4.profile_loss: 1295.8615 - val_loss: 2127.1726 - val_CHIPNexus.OCT4.logcount_loss: 0.3065 - val_CHIPNexus.OCT4.profile_loss: 1203.06811341.6635 - CHIPNexus.OCT4.logcount_loss: 0.2851 - CHIPNexus.OCT4.p\n",
      "Epoch 18/200\n",
      "276/276 [==============================] - 85s 307ms/step - loss: 1298.1058 - CHIPNexus.OCT4.logcount_loss: 0.2898 - CHIPNexus.OCT4.profile_loss: 1261.8821 - val_loss: 2014.9825 - val_CHIPNexus.OCT4.logcount_loss: 0.2627 - val_CHIPNexus.OCT4.profile_loss: 1188.5472\n",
      "Epoch 19/200\n",
      "276/276 [==============================] - 83s 300ms/step - loss: 1269.2064 - CHIPNexus.OCT4.logcount_loss: 0.2916 - CHIPNexus.OCT4.profile_loss: 1232.7593 - val_loss: 2295.2109 - val_CHIPNexus.OCT4.logcount_loss: 0.2874 - val_CHIPNexus.OCT4.profile_loss: 1275.4873\n",
      "Epoch 20/200\n",
      "276/276 [==============================] - 84s 303ms/step - loss: 1219.4645 - CHIPNexus.OCT4.logcount_loss: 0.2840 - CHIPNexus.OCT4.profile_loss: 1183.9611 - val_loss: 2062.3188 - val_CHIPNexus.OCT4.logcount_loss: 0.2738 - val_CHIPNexus.OCT4.profile_loss: 1183.3236\n",
      "Epoch 21/200\n",
      "276/276 [==============================] - 83s 302ms/step - loss: 1214.7624 - CHIPNexus.OCT4.logcount_loss: 0.2799 - CHIPNexus.OCT4.profile_loss: 1179.7809 - val_loss: 2036.8368 - val_CHIPNexus.OCT4.logcount_loss: 0.2642 - val_CHIPNexus.OCT4.profile_loss: 1179.7263\n",
      "Epoch 22/200\n",
      "276/276 [==============================] - 81s 294ms/step - loss: 1155.1186 - CHIPNexus.OCT4.logcount_loss: 0.2753 - CHIPNexus.OCT4.profile_loss: 1120.7053 - val_loss: 2050.1682 - val_CHIPNexus.OCT4.logcount_loss: 0.2865 - val_CHIPNexus.OCT4.profile_loss: 1166.7883\n",
      "Epoch 23/200\n",
      "276/276 [==============================] - 85s 309ms/step - loss: 1204.8681 - CHIPNexus.OCT4.logcount_loss: 0.2850 - CHIPNexus.OCT4.profile_loss: 1169.2477 - val_loss: 2076.6716 - val_CHIPNexus.OCT4.logcount_loss: 0.2858 - val_CHIPNexus.OCT4.profile_loss: 1176.3041\n",
      "val_loss 2005.4600830078125\n",
      "val_CHIPNexus.OCT4.logcount_loss 0.2767089009284973\n",
      "val_CHIPNexus.OCT4.profile_loss 1214.230712890625\n",
      "loss 1418.504821186313\n",
      "CHIPNexus.OCT4.logcount_loss 0.2995802\n",
      "CHIPNexus.OCT4.profile_loss 1380.9542\n",
      "Seed: 1434\n",
      "LOG SPACE PREDS IS SET TO True\n",
      "Oct4only_ctaskweight250_seed1434.h5\n",
      "Model: \"model_19\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "sequence (InputLayer)           (None, 1346, 4)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_174 (Conv1D)             (None, 1326, 64)     5440        sequence[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_121 (Lambda)             (None, 1322, 64)     0           conv1d_174[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_175 (Conv1D)             (None, 1322, 64)     12352       conv1d_174[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_121 (Add)                   (None, 1322, 64)     0           lambda_121[0][0]                 \n",
      "                                                                 conv1d_175[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_122 (Lambda)             (None, 1314, 64)     0           add_121[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_176 (Conv1D)             (None, 1314, 64)     12352       add_121[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_122 (Add)                   (None, 1314, 64)     0           lambda_122[0][0]                 \n",
      "                                                                 conv1d_176[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_123 (Lambda)             (None, 1298, 64)     0           add_122[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_177 (Conv1D)             (None, 1298, 64)     12352       add_122[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_123 (Add)                   (None, 1298, 64)     0           lambda_123[0][0]                 \n",
      "                                                                 conv1d_177[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_124 (Lambda)             (None, 1266, 64)     0           add_123[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_178 (Conv1D)             (None, 1266, 64)     12352       add_123[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_124 (Add)                   (None, 1266, 64)     0           lambda_124[0][0]                 \n",
      "                                                                 conv1d_178[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_125 (Lambda)             (None, 1202, 64)     0           add_124[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_179 (Conv1D)             (None, 1202, 64)     12352       add_124[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_125 (Add)                   (None, 1202, 64)     0           lambda_125[0][0]                 \n",
      "                                                                 conv1d_179[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_126 (Lambda)             (None, 1074, 64)     0           add_125[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_180 (Conv1D)             (None, 1074, 64)     12352       add_125[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_126 (Add)                   (None, 1074, 64)     0           lambda_126[0][0]                 \n",
      "                                                                 conv1d_180[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_21 (Gl (None, 64)           0           add_126[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "patchcap.logcount (InputLayer)  (None, 2)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_181 (Conv1D)             (None, 1000, 2)      9602        add_126[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "patchcap.profile (InputLayer)   (None, 1000, 2)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_69 (Concatenate)    (None, 66)           0           global_average_pooling1d_21[0][0]\n",
      "                                                                 patchcap.logcount[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_70 (Concatenate)    (None, 1000, 4)      0           conv1d_181[0][0]                 \n",
      "                                                                 patchcap.profile[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "CHIPNexus.OCT4.logcount (Dense) (None, 2)            134         concatenate_69[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "CHIPNexus.OCT4.profile (Conv1D) (None, 1000, 2)      10          concatenate_70[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 89,298\n",
      "Trainable params: 89,298\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "276/276 [==============================] - 82s 296ms/step - loss: 1985.7113 - CHIPNexus.OCT4.logcount_loss: 0.8776 - CHIPNexus.OCT4.profile_loss: 1766.3083 - val_loss: 2701.1978 - val_CHIPNexus.OCT4.logcount_loss: 0.3919 - val_CHIPNexus.OCT4.profile_loss: 1382.1737\n",
      "Epoch 2/200\n",
      "276/276 [==============================] - 79s 287ms/step - loss: 1801.7604 - CHIPNexus.OCT4.logcount_loss: 0.3603 - CHIPNexus.OCT4.profile_loss: 1711.6765 - val_loss: 2706.5120 - val_CHIPNexus.OCT4.logcount_loss: 0.4045 - val_CHIPNexus.OCT4.profile_loss: 1353.6129646 - CHIPNexus.OCT4.profile_loss: 1881.40 - ETA: 13\n",
      "Epoch 3/200\n",
      "276/276 [==============================] - 81s 294ms/step - loss: 1746.3628 - CHIPNexus.OCT4.logcount_loss: 0.3499 - CHIPNexus.OCT4.profile_loss: 1658.8817 - val_loss: 2562.8745 - val_CHIPNexus.OCT4.logcount_loss: 0.3516 - val_CHIPNexus.OCT4.profile_loss: 1321.3903\n",
      "Epoch 4/200\n",
      "276/276 [==============================] - 82s 298ms/step - loss: 1699.6464 - CHIPNexus.OCT4.logcount_loss: 0.3450 - CHIPNexus.OCT4.profile_loss: 1613.3937 - val_loss: 2414.6279 - val_CHIPNexus.OCT4.logcount_loss: 0.3305 - val_CHIPNexus.OCT4.profile_loss: 1300.5142\n",
      "Epoch 5/200\n",
      "276/276 [==============================] - 85s 308ms/step - loss: 1663.0565 - CHIPNexus.OCT4.logcount_loss: 0.3273 - CHIPNexus.OCT4.profile_loss: 1581.2384 - val_loss: 2474.3855 - val_CHIPNexus.OCT4.logcount_loss: 0.3637 - val_CHIPNexus.OCT4.profile_loss: 1289.8096\n",
      "Epoch 6/200\n",
      "276/276 [==============================] - 83s 301ms/step - loss: 1629.9963 - CHIPNexus.OCT4.logcount_loss: 0.3248 - CHIPNexus.OCT4.profile_loss: 1548.7903 - val_loss: 2349.7649 - val_CHIPNexus.OCT4.logcount_loss: 0.3103 - val_CHIPNexus.OCT4.profile_loss: 1281.2811\n",
      "Epoch 7/200\n",
      "276/276 [==============================] - 82s 296ms/step - loss: 1597.5213 - CHIPNexus.OCT4.logcount_loss: 0.3188 - CHIPNexus.OCT4.profile_loss: 1517.8193 - val_loss: 2346.3794 - val_CHIPNexus.OCT4.logcount_loss: 0.3383 - val_CHIPNexus.OCT4.profile_loss: 1267.2637\n",
      "Epoch 8/200\n",
      "276/276 [==============================] - 84s 303ms/step - loss: 1574.3835 - CHIPNexus.OCT4.logcount_loss: 0.3070 - CHIPNexus.OCT4.profile_loss: 1497.6415 - val_loss: 2293.3215 - val_CHIPNexus.OCT4.logcount_loss: 0.3000 - val_CHIPNexus.OCT4.profile_loss: 1260.6427\n",
      "Epoch 9/200\n",
      "276/276 [==============================] - 80s 288ms/step - loss: 1589.3683 - CHIPNexus.OCT4.logcount_loss: 0.2980 - CHIPNexus.OCT4.profile_loss: 1514.9998 - val_loss: 2243.5940 - val_CHIPNexus.OCT4.logcount_loss: 0.2877 - val_CHIPNexus.OCT4.profile_loss: 1250.8259\n",
      "Epoch 10/200\n",
      "276/276 [==============================] - 79s 288ms/step - loss: 1531.9090 - CHIPNexus.OCT4.logcount_loss: 0.3000 - CHIPNexus.OCT4.profile_loss: 1456.9111 - val_loss: 2233.1665 - val_CHIPNexus.OCT4.logcount_loss: 0.2801 - val_CHIPNexus.OCT4.profile_loss: 1247.1733\n",
      "Epoch 11/200\n",
      "276/276 [==============================] - 83s 302ms/step - loss: 1509.9362 - CHIPNexus.OCT4.logcount_loss: 0.2891 - CHIPNexus.OCT4.profile_loss: 1437.6610 - val_loss: 2299.9722 - val_CHIPNexus.OCT4.logcount_loss: 0.3312 - val_CHIPNexus.OCT4.profile_loss: 1235.8290\n",
      "Epoch 12/200\n",
      "276/276 [==============================] - 84s 305ms/step - loss: 1493.3475 - CHIPNexus.OCT4.logcount_loss: 0.2897 - CHIPNexus.OCT4.profile_loss: 1420.9261 - val_loss: 2106.8650 - val_CHIPNexus.OCT4.logcount_loss: 0.2898 - val_CHIPNexus.OCT4.profile_loss: 1230.9496\n",
      "Epoch 13/200\n",
      "276/276 [==============================] - 84s 304ms/step - loss: 1472.9709 - CHIPNexus.OCT4.logcount_loss: 0.2968 - CHIPNexus.OCT4.profile_loss: 1398.7566 - val_loss: 2119.6592 - val_CHIPNexus.OCT4.logcount_loss: 0.2778 - val_CHIPNexus.OCT4.profile_loss: 1224.5442\n",
      "Epoch 14/200\n",
      "276/276 [==============================] - 83s 300ms/step - loss: 1458.1388 - CHIPNexus.OCT4.logcount_loss: 0.2833 - CHIPNexus.OCT4.profile_loss: 1387.3036 - val_loss: 2129.3098 - val_CHIPNexus.OCT4.logcount_loss: 0.2644 - val_CHIPNexus.OCT4.profile_loss: 1218.0051\n",
      "Epoch 15/200\n",
      "276/276 [==============================] - 85s 308ms/step - loss: 1418.2180 - CHIPNexus.OCT4.logcount_loss: 0.2807 - CHIPNexus.OCT4.profile_loss: 1348.0369 - val_loss: 2382.2705 - val_CHIPNexus.OCT4.logcount_loss: 0.2648 - val_CHIPNexus.OCT4.profile_loss: 1363.5381\n",
      "Epoch 16/200\n",
      "276/276 [==============================] - 82s 298ms/step - loss: 1409.8320 - CHIPNexus.OCT4.logcount_loss: 0.2883 - CHIPNexus.OCT4.profile_loss: 1337.7642 - val_loss: 2426.5129 - val_CHIPNexus.OCT4.logcount_loss: 0.3103 - val_CHIPNexus.OCT4.profile_loss: 1319.4554 - ETA: 0s - loss: 1411.1735 - CHIPNexus.OCT4.logcount_loss: 0.2881 - CHIPNexus.OCT4.profile_loss: 1339.\n",
      "Epoch 17/200\n",
      "276/276 [==============================] - 83s 301ms/step - loss: 1373.7466 - CHIPNexus.OCT4.logcount_loss: 0.2979 - CHIPNexus.OCT4.profile_loss: 1299.2708 - val_loss: 2196.6099 - val_CHIPNexus.OCT4.logcount_loss: 0.3047 - val_CHIPNexus.OCT4.profile_loss: 1201.3657\n",
      "Epoch 18/200\n",
      "276/276 [==============================] - 82s 298ms/step - loss: 1364.0084 - CHIPNexus.OCT4.logcount_loss: 0.2776 - CHIPNexus.OCT4.profile_loss: 1294.6111 - val_loss: 2202.4131 - val_CHIPNexus.OCT4.logcount_loss: 0.2797 - val_CHIPNexus.OCT4.profile_loss: 1213.5764\n",
      "Epoch 19/200\n",
      "276/276 [==============================] - 84s 304ms/step - loss: 1365.7966 - CHIPNexus.OCT4.logcount_loss: 0.2945 - CHIPNexus.OCT4.profile_loss: 1292.1593 - val_loss: 2128.4792 - val_CHIPNexus.OCT4.logcount_loss: 0.2692 - val_CHIPNexus.OCT4.profile_loss: 1197.4672\n",
      "Epoch 20/200\n",
      "276/276 [==============================] - 83s 299ms/step - loss: 1326.5246 - CHIPNexus.OCT4.logcount_loss: 0.2945 - CHIPNexus.OCT4.profile_loss: 1252.9102 - val_loss: 2223.8730 - val_CHIPNexus.OCT4.logcount_loss: 0.2761 - val_CHIPNexus.OCT4.profile_loss: 1201.1486\n",
      "Epoch 21/200\n",
      "276/276 [==============================] - 83s 301ms/step - loss: 1262.9582 - CHIPNexus.OCT4.logcount_loss: 0.2838 - CHIPNexus.OCT4.profile_loss: 1191.9956 - val_loss: 2208.7788 - val_CHIPNexus.OCT4.logcount_loss: 0.2703 - val_CHIPNexus.OCT4.profile_loss: 1207.4648\n",
      "Epoch 22/200\n",
      "276/276 [==============================] - 86s 311ms/step - loss: 1229.5007 - CHIPNexus.OCT4.logcount_loss: 0.2766 - CHIPNexus.OCT4.profile_loss: 1160.3383 - val_loss: 2187.9438 - val_CHIPNexus.OCT4.logcount_loss: 0.2664 - val_CHIPNexus.OCT4.profile_loss: 1243.1847\n",
      "val_loss 2106.864990234375\n",
      "val_CHIPNexus.OCT4.logcount_loss 0.28981122374534607\n",
      "val_CHIPNexus.OCT4.profile_loss 1230.9495849609375\n",
      "loss 1493.4446114545988\n",
      "CHIPNexus.OCT4.logcount_loss 0.28968373\n",
      "CHIPNexus.OCT4.profile_loss 1420.9261\n",
      "Seed: 1434\n",
      "LOG SPACE PREDS IS SET TO True\n",
      "Oct4only_ctaskweight500_seed1434.h5\n",
      "Model: \"model_20\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "sequence (InputLayer)           (None, 1346, 4)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_182 (Conv1D)             (None, 1326, 64)     5440        sequence[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_127 (Lambda)             (None, 1322, 64)     0           conv1d_182[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_183 (Conv1D)             (None, 1322, 64)     12352       conv1d_182[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_127 (Add)                   (None, 1322, 64)     0           lambda_127[0][0]                 \n",
      "                                                                 conv1d_183[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_128 (Lambda)             (None, 1314, 64)     0           add_127[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_184 (Conv1D)             (None, 1314, 64)     12352       add_127[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_128 (Add)                   (None, 1314, 64)     0           lambda_128[0][0]                 \n",
      "                                                                 conv1d_184[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_129 (Lambda)             (None, 1298, 64)     0           add_128[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_185 (Conv1D)             (None, 1298, 64)     12352       add_128[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_129 (Add)                   (None, 1298, 64)     0           lambda_129[0][0]                 \n",
      "                                                                 conv1d_185[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_130 (Lambda)             (None, 1266, 64)     0           add_129[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_186 (Conv1D)             (None, 1266, 64)     12352       add_129[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_130 (Add)                   (None, 1266, 64)     0           lambda_130[0][0]                 \n",
      "                                                                 conv1d_186[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_131 (Lambda)             (None, 1202, 64)     0           add_130[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_187 (Conv1D)             (None, 1202, 64)     12352       add_130[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_131 (Add)                   (None, 1202, 64)     0           lambda_131[0][0]                 \n",
      "                                                                 conv1d_187[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_132 (Lambda)             (None, 1074, 64)     0           add_131[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_188 (Conv1D)             (None, 1074, 64)     12352       add_131[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_132 (Add)                   (None, 1074, 64)     0           lambda_132[0][0]                 \n",
      "                                                                 conv1d_188[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_22 (Gl (None, 64)           0           add_132[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "patchcap.logcount (InputLayer)  (None, 2)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_189 (Conv1D)             (None, 1000, 2)      9602        add_132[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "patchcap.profile (InputLayer)   (None, 1000, 2)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_71 (Concatenate)    (None, 66)           0           global_average_pooling1d_22[0][0]\n",
      "                                                                 patchcap.logcount[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_72 (Concatenate)    (None, 1000, 4)      0           conv1d_189[0][0]                 \n",
      "                                                                 patchcap.profile[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "CHIPNexus.OCT4.logcount (Dense) (None, 2)            134         concatenate_71[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "CHIPNexus.OCT4.profile (Conv1D) (None, 1000, 2)      10          concatenate_72[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 89,298\n",
      "Trainable params: 89,298\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "276/276 [==============================] - 85s 308ms/step - loss: 2225.4152 - CHIPNexus.OCT4.logcount_loss: 0.9382 - CHIPNexus.OCT4.profile_loss: 1756.3346 - val_loss: 3257.4102 - val_CHIPNexus.OCT4.logcount_loss: 0.3673 - val_CHIPNexus.OCT4.profile_loss: 1384.6416\n",
      "Epoch 2/200\n",
      "276/276 [==============================] - 81s 293ms/step - loss: 1882.7571 - CHIPNexus.OCT4.logcount_loss: 0.3506 - CHIPNexus.OCT4.profile_loss: 1707.4685 - val_loss: 3283.2021 - val_CHIPNexus.OCT4.logcount_loss: 0.3722 - val_CHIPNexus.OCT4.profile_loss: 1358.0820\n",
      "Epoch 3/200\n",
      "276/276 [==============================] - 84s 304ms/step - loss: 1843.0729 - CHIPNexus.OCT4.logcount_loss: 0.3557 - CHIPNexus.OCT4.profile_loss: 1665.2394 - val_loss: 3091.2197 - val_CHIPNexus.OCT4.logcount_loss: 0.3440 - val_CHIPNexus.OCT4.profile_loss: 1335.3826\n",
      "Epoch 4/200\n",
      "276/276 [==============================] - 81s 294ms/step - loss: 1791.6874 - CHIPNexus.OCT4.logcount_loss: 0.3376 - CHIPNexus.OCT4.profile_loss: 1622.8903 - val_loss: 3005.3662 - val_CHIPNexus.OCT4.logcount_loss: 0.3387 - val_CHIPNexus.OCT4.profile_loss: 1309.8180\n",
      "Epoch 5/200\n",
      "276/276 [==============================] - 80s 289ms/step - loss: 1743.0113 - CHIPNexus.OCT4.logcount_loss: 0.3322 - CHIPNexus.OCT4.profile_loss: 1576.9240 - val_loss: 2913.8062 - val_CHIPNexus.OCT4.logcount_loss: 0.3271 - val_CHIPNexus.OCT4.profile_loss: 1289.4075.4408 - CHIPNexus.OCT4.logcount_loss: 0.3520 - CHIPNexus.OC - ETA: 51s - loss: 3815.7826 - CHIPNexus.OCT4.logcount_loss: 0.3374 - CHIPNexus.OCT4.profi - ETA: 47s - loss: 3182.4113 - CHIPNexus.OCT4.logcount_loss: 0.3374 - CHIPNexus.OC\n",
      "Epoch 6/200\n",
      "276/276 [==============================] - 87s 316ms/step - loss: 1712.4133 - CHIPNexus.OCT4.logcount_loss: 0.3361 - CHIPNexus.OCT4.profile_loss: 1544.3475 - val_loss: 2748.7131 - val_CHIPNexus.OCT4.logcount_loss: 0.3110 - val_CHIPNexus.OCT4.profile_loss: 1278.9092\n",
      "Epoch 7/200\n",
      "276/276 [==============================] - 81s 292ms/step - loss: 1674.7251 - CHIPNexus.OCT4.logcount_loss: 0.3191 - CHIPNexus.OCT4.profile_loss: 1515.1534 - val_loss: 2827.1660 - val_CHIPNexus.OCT4.logcount_loss: 0.3333 - val_CHIPNexus.OCT4.profile_loss: 1266.5516\n",
      "Epoch 8/200\n",
      "276/276 [==============================] - 80s 291ms/step - loss: 1648.5793 - CHIPNexus.OCT4.logcount_loss: 0.3125 - CHIPNexus.OCT4.profile_loss: 1492.3253 - val_loss: 2539.4136 - val_CHIPNexus.OCT4.logcount_loss: 0.3027 - val_CHIPNexus.OCT4.profile_loss: 1264.7671\n",
      "Epoch 9/200\n",
      "276/276 [==============================] - 85s 308ms/step - loss: 1623.8752 - CHIPNexus.OCT4.logcount_loss: 0.3055 - CHIPNexus.OCT4.profile_loss: 1471.1332 - val_loss: 2658.8799 - val_CHIPNexus.OCT4.logcount_loss: 0.2994 - val_CHIPNexus.OCT4.profile_loss: 1254.9795\n",
      "Epoch 10/200\n",
      "276/276 [==============================] - 81s 295ms/step - loss: 1595.6804 - CHIPNexus.OCT4.logcount_loss: 0.2922 - CHIPNexus.OCT4.profile_loss: 1449.5796 - val_loss: 2701.8916 - val_CHIPNexus.OCT4.logcount_loss: 0.3119 - val_CHIPNexus.OCT4.profile_loss: 1253.9407\n",
      "Epoch 11/200\n",
      "276/276 [==============================] - 80s 291ms/step - loss: 1579.8014 - CHIPNexus.OCT4.logcount_loss: 0.2960 - CHIPNexus.OCT4.profile_loss: 1431.8137 - val_loss: 2635.7500 - val_CHIPNexus.OCT4.logcount_loss: 0.3217 - val_CHIPNexus.OCT4.profile_loss: 1233.69432297.2986 - CHIPNexus.OCT4.logcount_loss: 0.2900 - CHIPNexus.OCT4.p - ETA: 38s - loss: 2\n",
      "Epoch 12/200\n",
      "276/276 [==============================] - 80s 289ms/step - loss: 1562.1672 - CHIPNexus.OCT4.logcount_loss: 0.2917 - CHIPNexus.OCT4.profile_loss: 1416.3254 - val_loss: 2565.1682 - val_CHIPNexus.OCT4.logcount_loss: 0.3099 - val_CHIPNexus.OCT4.profile_loss: 1226.7538\n",
      "Epoch 13/200\n",
      "276/276 [==============================] - 78s 284ms/step - loss: 1527.1393 - CHIPNexus.OCT4.logcount_loss: 0.2759 - CHIPNexus.OCT4.profile_loss: 1389.2102 - val_loss: 2714.2373 - val_CHIPNexus.OCT4.logcount_loss: 0.3457 - val_CHIPNexus.OCT4.profile_loss: 1257.3174\n",
      "Epoch 14/200\n",
      "276/276 [==============================] - 81s 293ms/step - loss: 1525.3290 - CHIPNexus.OCT4.logcount_loss: 0.2957 - CHIPNexus.OCT4.profile_loss: 1377.4594 - val_loss: 2291.0686 - val_CHIPNexus.OCT4.logcount_loss: 0.2890 - val_CHIPNexus.OCT4.profile_loss: 1213.9125\n",
      "Epoch 15/200\n",
      "276/276 [==============================] - 81s 292ms/step - loss: 1505.1308 - CHIPNexus.OCT4.logcount_loss: 0.2859 - CHIPNexus.OCT4.profile_loss: 1362.1962 - val_loss: 2445.4490 - val_CHIPNexus.OCT4.logcount_loss: 0.2839 - val_CHIPNexus.OCT4.profile_loss: 1213.2314\n",
      "Epoch 16/200\n",
      "276/276 [==============================] - 84s 304ms/step - loss: 1483.4084 - CHIPNexus.OCT4.logcount_loss: 0.2844 - CHIPNexus.OCT4.profile_loss: 1341.1997 - val_loss: 2238.0286 - val_CHIPNexus.OCT4.logcount_loss: 0.2931 - val_CHIPNexus.OCT4.profile_loss: 1200.3972\n",
      "Epoch 17/200\n",
      "276/276 [==============================] - 82s 299ms/step - loss: 1458.0397 - CHIPNexus.OCT4.logcount_loss: 0.2848 - CHIPNexus.OCT4.profile_loss: 1315.6205 - val_loss: 2379.4976 - val_CHIPNexus.OCT4.logcount_loss: 0.2676 - val_CHIPNexus.OCT4.profile_loss: 1199.5339\n",
      "Epoch 18/200\n",
      "276/276 [==============================] - 84s 304ms/step - loss: 1436.9513 - CHIPNexus.OCT4.logcount_loss: 0.2866 - CHIPNexus.OCT4.profile_loss: 1293.6528 - val_loss: 2466.4036 - val_CHIPNexus.OCT4.logcount_loss: 0.3097 - val_CHIPNexus.OCT4.profile_loss: 1190.2483\n",
      "Epoch 19/200\n",
      "276/276 [==============================] - 84s 303ms/step - loss: 1419.0952 - CHIPNexus.OCT4.logcount_loss: 0.2832 - CHIPNexus.OCT4.profile_loss: 1277.4745 - val_loss: 2424.4111 - val_CHIPNexus.OCT4.logcount_loss: 0.2670 - val_CHIPNexus.OCT4.profile_loss: 1199.1549\n",
      "Epoch 20/200\n",
      "276/276 [==============================] - 86s 313ms/step - loss: 1415.5646 - CHIPNexus.OCT4.logcount_loss: 0.3031 - CHIPNexus.OCT4.profile_loss: 1264.0364 - val_loss: 2265.1282 - val_CHIPNexus.OCT4.logcount_loss: 0.2669 - val_CHIPNexus.OCT4.profile_loss: 1186.3026\n",
      "Epoch 21/200\n",
      "276/276 [==============================] - 84s 303ms/step - loss: 1382.1760 - CHIPNexus.OCT4.logcount_loss: 0.2957 - CHIPNexus.OCT4.profile_loss: 1234.3390 - val_loss: 2313.4824 - val_CHIPNexus.OCT4.logcount_loss: 0.2648 - val_CHIPNexus.OCT4.profile_loss: 1189.3304\n",
      "Epoch 22/200\n",
      "276/276 [==============================] - 85s 308ms/step - loss: 1279.7690 - CHIPNexus.OCT4.logcount_loss: 0.2807 - CHIPNexus.OCT4.profile_loss: 1139.4253 - val_loss: 2267.0938 - val_CHIPNexus.OCT4.logcount_loss: 0.2758 - val_CHIPNexus.OCT4.profile_loss: 1196.3147\n",
      "Epoch 23/200\n",
      "276/276 [==============================] - 83s 302ms/step - loss: 1304.9284 - CHIPNexus.OCT4.logcount_loss: 0.2877 - CHIPNexus.OCT4.profile_loss: 1161.0570 - val_loss: 2300.9204 - val_CHIPNexus.OCT4.logcount_loss: 0.2735 - val_CHIPNexus.OCT4.profile_loss: 1177.3898\n",
      "Epoch 24/200\n",
      "276/276 [==============================] - 85s 309ms/step - loss: 1396.0447 - CHIPNexus.OCT4.logcount_loss: 0.2813 - CHIPNexus.OCT4.profile_loss: 1255.3763 - val_loss: 2258.8398 - val_CHIPNexus.OCT4.logcount_loss: 0.2643 - val_CHIPNexus.OCT4.profile_loss: 1162.6395\n",
      "Epoch 25/200\n",
      "276/276 [==============================] - 81s 294ms/step - loss: 1356.5836 - CHIPNexus.OCT4.logcount_loss: 0.2808 - CHIPNexus.OCT4.profile_loss: 1216.1881 - val_loss: 2276.0938 - val_CHIPNexus.OCT4.logcount_loss: 0.2635 - val_CHIPNexus.OCT4.profile_loss: 1158.8018s: 1464.1222 - CHIPNexus.OCT4.logcount_loss: 0.2798 -\n",
      "Epoch 26/200\n",
      "276/276 [==============================] - 85s 308ms/step - loss: 1284.1341 - CHIPNexus.OCT4.logcount_loss: 0.2837 - CHIPNexus.OCT4.profile_loss: 1142.2634 - val_loss: 2410.4380 - val_CHIPNexus.OCT4.logcount_loss: 0.3177 - val_CHIPNexus.OCT4.profile_loss: 1269.6664\n",
      "val_loss 2238.028564453125\n",
      "val_CHIPNexus.OCT4.logcount_loss 0.29313957691192627\n",
      "val_CHIPNexus.OCT4.profile_loss 1200.397216796875\n",
      "loss 1483.498551635984\n",
      "CHIPNexus.OCT4.logcount_loss 0.2844162\n",
      "CHIPNexus.OCT4.profile_loss 1341.1997\n",
      "Seed: 1534\n",
      "LOG SPACE PREDS IS SET TO True\n",
      "Oct4only_ctaskweight0_seed1534.h5\n",
      "Model: \"model_21\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "sequence (InputLayer)           (None, 1346, 4)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_190 (Conv1D)             (None, 1326, 64)     5440        sequence[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_133 (Lambda)             (None, 1322, 64)     0           conv1d_190[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_191 (Conv1D)             (None, 1322, 64)     12352       conv1d_190[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_133 (Add)                   (None, 1322, 64)     0           lambda_133[0][0]                 \n",
      "                                                                 conv1d_191[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_134 (Lambda)             (None, 1314, 64)     0           add_133[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_192 (Conv1D)             (None, 1314, 64)     12352       add_133[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_134 (Add)                   (None, 1314, 64)     0           lambda_134[0][0]                 \n",
      "                                                                 conv1d_192[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_135 (Lambda)             (None, 1298, 64)     0           add_134[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_193 (Conv1D)             (None, 1298, 64)     12352       add_134[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_135 (Add)                   (None, 1298, 64)     0           lambda_135[0][0]                 \n",
      "                                                                 conv1d_193[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_136 (Lambda)             (None, 1266, 64)     0           add_135[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_194 (Conv1D)             (None, 1266, 64)     12352       add_135[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_136 (Add)                   (None, 1266, 64)     0           lambda_136[0][0]                 \n",
      "                                                                 conv1d_194[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_137 (Lambda)             (None, 1202, 64)     0           add_136[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_195 (Conv1D)             (None, 1202, 64)     12352       add_136[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_137 (Add)                   (None, 1202, 64)     0           lambda_137[0][0]                 \n",
      "                                                                 conv1d_195[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_138 (Lambda)             (None, 1074, 64)     0           add_137[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_196 (Conv1D)             (None, 1074, 64)     12352       add_137[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_138 (Add)                   (None, 1074, 64)     0           lambda_138[0][0]                 \n",
      "                                                                 conv1d_196[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_23 (Gl (None, 64)           0           add_138[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "patchcap.logcount (InputLayer)  (None, 2)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_197 (Conv1D)             (None, 1000, 2)      9602        add_138[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "patchcap.profile (InputLayer)   (None, 1000, 2)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_73 (Concatenate)    (None, 66)           0           global_average_pooling1d_23[0][0]\n",
      "                                                                 patchcap.logcount[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_74 (Concatenate)    (None, 1000, 4)      0           conv1d_197[0][0]                 \n",
      "                                                                 patchcap.profile[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "CHIPNexus.OCT4.logcount (Dense) (None, 2)            134         concatenate_73[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "CHIPNexus.OCT4.profile (Conv1D) (None, 1000, 2)      10          concatenate_74[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 89,298\n",
      "Trainable params: 89,298\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "276/276 [==============================] - 79s 287ms/step - loss: 1873.5022 - CHIPNexus.OCT4.logcount_loss: 20.1098 - CHIPNexus.OCT4.profile_loss: 1873.5009 - val_loss: 2016.1158 - val_CHIPNexus.OCT4.logcount_loss: 21.5428 - val_CHIPNexus.OCT4.profile_loss: 1650.3755026.4106 - CHIPNexus.OCT4.logcount_loss: 19.7100 - CHIPNexus.OCT4.profile_loss: 2026. - ETA: 10s - loss: 2019.2622 - CHIPNexus.OCT4.logcount_loss: 19.\n",
      "Epoch 2/200\n",
      "276/276 [==============================] - 80s 290ms/step - loss: 1816.9996 - CHIPNexus.OCT4.logcount_loss: 21.7616 - CHIPNexus.OCT4.profile_loss: 1816.9990 - val_loss: 2018.2710 - val_CHIPNexus.OCT4.logcount_loss: 23.2339 - val_CHIPNexus.OCT4.profile_loss: 1609.0598\n",
      "Epoch 3/200\n",
      "276/276 [==============================] - 79s 284ms/step - loss: 1775.6407 - CHIPNexus.OCT4.logcount_loss: 23.7186 - CHIPNexus.OCT4.profile_loss: 1775.6398 - val_loss: 1977.0873 - val_CHIPNexus.OCT4.logcount_loss: 23.9880 - val_CHIPNexus.OCT4.profile_loss: 1587.1750\n",
      "Epoch 4/200\n",
      "  3/276 [..............................] - ETA: 33s - loss: 989.8323 - CHIPNexus.OCT4.logcount_loss: 23.2901 - CHIPNexus.OCT4.profile_loss: 989.8323  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/avanti/anaconda3/envs/py376/lib/python3.7/site-packages/keras/callbacks/callbacks.py:95: RuntimeWarning: Method (on_train_batch_end) is slow compared to the batch update (0.115396). Check your callbacks.\n",
      "  % (hook_name, delta_t_median), RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "276/276 [==============================] - 80s 288ms/step - loss: 1751.6155 - CHIPNexus.OCT4.logcount_loss: 23.0219 - CHIPNexus.OCT4.profile_loss: 1751.6146 - val_loss: 1924.1490 - val_CHIPNexus.OCT4.logcount_loss: 21.8823 - val_CHIPNexus.OCT4.profile_loss: 1568.5795\n",
      "Epoch 5/200\n",
      "276/276 [==============================] - 80s 289ms/step - loss: 1723.9731 - CHIPNexus.OCT4.logcount_loss: 21.2169 - CHIPNexus.OCT4.profile_loss: 1723.9723 - val_loss: 1952.7263 - val_CHIPNexus.OCT4.logcount_loss: 20.9941 - val_CHIPNexus.OCT4.profile_loss: 1560.7184\n",
      "Epoch 6/200\n",
      "276/276 [==============================] - 84s 306ms/step - loss: 1700.7881 - CHIPNexus.OCT4.logcount_loss: 22.5678 - CHIPNexus.OCT4.profile_loss: 1700.7877 - val_loss: 1931.9956 - val_CHIPNexus.OCT4.logcount_loss: 25.1392 - val_CHIPNexus.OCT4.profile_loss: 1544.5752\n",
      "Epoch 7/200\n",
      "276/276 [==============================] - 83s 302ms/step - loss: 1681.5920 - CHIPNexus.OCT4.logcount_loss: 23.0753 - CHIPNexus.OCT4.profile_loss: 1681.5919 - val_loss: 1894.7368 - val_CHIPNexus.OCT4.logcount_loss: 22.8830 - val_CHIPNexus.OCT4.profile_loss: 1525.4677\n",
      "Epoch 8/200\n",
      "276/276 [==============================] - 81s 293ms/step - loss: 1659.3467 - CHIPNexus.OCT4.logcount_loss: 24.5251 - CHIPNexus.OCT4.profile_loss: 1659.3464 - val_loss: 1938.7830 - val_CHIPNexus.OCT4.logcount_loss: 27.7100 - val_CHIPNexus.OCT4.profile_loss: 1516.8862\n",
      "Epoch 9/200\n",
      "276/276 [==============================] - 82s 297ms/step - loss: 1638.3801 - CHIPNexus.OCT4.logcount_loss: 25.8841 - CHIPNexus.OCT4.profile_loss: 1638.3796 - val_loss: 1935.0505 - val_CHIPNexus.OCT4.logcount_loss: 23.1367 - val_CHIPNexus.OCT4.profile_loss: 1504.1520oss: 25.9982 - CHIPNexus.OCT4.profile_los\n",
      "Epoch 10/200\n",
      "276/276 [==============================] - 80s 290ms/step - loss: 1618.6590 - CHIPNexus.OCT4.logcount_loss: 24.4977 - CHIPNexus.OCT4.profile_loss: 1618.6586 - val_loss: 1944.4445 - val_CHIPNexus.OCT4.logcount_loss: 25.4362 - val_CHIPNexus.OCT4.profile_loss: 1494.8077\n",
      "Epoch 11/200\n",
      "276/276 [==============================] - 81s 294ms/step - loss: 1593.5850 - CHIPNexus.OCT4.logcount_loss: 24.2369 - CHIPNexus.OCT4.profile_loss: 1593.5842 - val_loss: 1963.9292 - val_CHIPNexus.OCT4.logcount_loss: 22.8636 - val_CHIPNexus.OCT4.profile_loss: 1486.4762\n",
      "Epoch 12/200\n",
      "276/276 [==============================] - 78s 282ms/step - loss: 1557.1896 - CHIPNexus.OCT4.logcount_loss: 23.8161 - CHIPNexus.OCT4.profile_loss: 1557.1890 - val_loss: 1942.9956 - val_CHIPNexus.OCT4.logcount_loss: 23.3647 - val_CHIPNexus.OCT4.profile_loss: 1472.1796\n",
      "Epoch 13/200\n",
      "276/276 [==============================] - 84s 305ms/step - loss: 1485.7798 - CHIPNexus.OCT4.logcount_loss: 23.3025 - CHIPNexus.OCT4.profile_loss: 1485.7795 - val_loss: 2070.4875 - val_CHIPNexus.OCT4.logcount_loss: 24.2214 - val_CHIPNexus.OCT4.profile_loss: 1527.8417\n",
      "Epoch 14/200\n",
      "276/276 [==============================] - 81s 293ms/step - loss: 1404.9081 - CHIPNexus.OCT4.logcount_loss: 23.8972 - CHIPNexus.OCT4.profile_loss: 1404.9075 - val_loss: 2029.4216 - val_CHIPNexus.OCT4.logcount_loss: 24.6481 - val_CHIPNexus.OCT4.profile_loss: 1494.4500\n",
      "Epoch 15/200\n",
      "276/276 [==============================] - 78s 283ms/step - loss: 1328.4612 - CHIPNexus.OCT4.logcount_loss: 24.3460 - CHIPNexus.OCT4.profile_loss: 1328.4612 - val_loss: 1986.8898 - val_CHIPNexus.OCT4.logcount_loss: 24.2541 - val_CHIPNexus.OCT4.profile_loss: 1454.7797\n",
      "Epoch 16/200\n",
      "276/276 [==============================] - 84s 303ms/step - loss: 1446.6357 - CHIPNexus.OCT4.logcount_loss: 24.3204 - CHIPNexus.OCT4.profile_loss: 1446.6353 - val_loss: 2111.4019 - val_CHIPNexus.OCT4.logcount_loss: 23.7538 - val_CHIPNexus.OCT4.profile_loss: 1530.7765\n",
      "Epoch 17/200\n",
      "276/276 [==============================] - 84s 305ms/step - loss: 1315.6422 - CHIPNexus.OCT4.logcount_loss: 28.7194 - CHIPNexus.OCT4.profile_loss: 1315.6414 - val_loss: 1999.6532 - val_CHIPNexus.OCT4.logcount_loss: 37.3535 - val_CHIPNexus.OCT4.profile_loss: 1444.7156\n",
      "val_loss 1894.73681640625\n",
      "val_CHIPNexus.OCT4.logcount_loss 22.88301658630371\n",
      "val_CHIPNexus.OCT4.profile_loss 1525.4676513671875\n",
      "loss 1681.7728907378144\n",
      "CHIPNexus.OCT4.logcount_loss 23.075293\n",
      "CHIPNexus.OCT4.profile_loss 1681.5919\n",
      "Seed: 1534\n",
      "LOG SPACE PREDS IS SET TO True\n",
      "Oct4only_ctaskweight125_seed1534.h5\n",
      "Model: \"model_22\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "sequence (InputLayer)           (None, 1346, 4)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_198 (Conv1D)             (None, 1326, 64)     5440        sequence[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_139 (Lambda)             (None, 1322, 64)     0           conv1d_198[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_199 (Conv1D)             (None, 1322, 64)     12352       conv1d_198[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_139 (Add)                   (None, 1322, 64)     0           lambda_139[0][0]                 \n",
      "                                                                 conv1d_199[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_140 (Lambda)             (None, 1314, 64)     0           add_139[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_200 (Conv1D)             (None, 1314, 64)     12352       add_139[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_140 (Add)                   (None, 1314, 64)     0           lambda_140[0][0]                 \n",
      "                                                                 conv1d_200[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_141 (Lambda)             (None, 1298, 64)     0           add_140[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_201 (Conv1D)             (None, 1298, 64)     12352       add_140[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_141 (Add)                   (None, 1298, 64)     0           lambda_141[0][0]                 \n",
      "                                                                 conv1d_201[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_142 (Lambda)             (None, 1266, 64)     0           add_141[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_202 (Conv1D)             (None, 1266, 64)     12352       add_141[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_142 (Add)                   (None, 1266, 64)     0           lambda_142[0][0]                 \n",
      "                                                                 conv1d_202[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_143 (Lambda)             (None, 1202, 64)     0           add_142[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_203 (Conv1D)             (None, 1202, 64)     12352       add_142[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_143 (Add)                   (None, 1202, 64)     0           lambda_143[0][0]                 \n",
      "                                                                 conv1d_203[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_144 (Lambda)             (None, 1074, 64)     0           add_143[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_204 (Conv1D)             (None, 1074, 64)     12352       add_143[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_144 (Add)                   (None, 1074, 64)     0           lambda_144[0][0]                 \n",
      "                                                                 conv1d_204[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_24 (Gl (None, 64)           0           add_144[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "patchcap.logcount (InputLayer)  (None, 2)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_205 (Conv1D)             (None, 1000, 2)      9602        add_144[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "patchcap.profile (InputLayer)   (None, 1000, 2)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_75 (Concatenate)    (None, 66)           0           global_average_pooling1d_24[0][0]\n",
      "                                                                 patchcap.logcount[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_76 (Concatenate)    (None, 1000, 4)      0           conv1d_205[0][0]                 \n",
      "                                                                 patchcap.profile[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "CHIPNexus.OCT4.logcount (Dense) (None, 2)            134         concatenate_75[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "CHIPNexus.OCT4.profile (Conv1D) (None, 1000, 2)      10          concatenate_76[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 89,298\n",
      "Trainable params: 89,298\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "276/276 [==============================] - 83s 299ms/step - loss: 1918.7502 - CHIPNexus.OCT4.logcount_loss: 0.4148 - CHIPNexus.OCT4.profile_loss: 1866.8945 - val_loss: 2174.0386 - val_CHIPNexus.OCT4.logcount_loss: 0.3390 - val_CHIPNexus.OCT4.profile_loss: 1638.9862\n",
      "Epoch 2/200\n",
      "276/276 [==============================] - 87s 316ms/step - loss: 1910.4633 - CHIPNexus.OCT4.logcount_loss: 0.3021 - CHIPNexus.OCT4.profile_loss: 1872.9215 - val_loss: 2098.6130 - val_CHIPNexus.OCT4.logcount_loss: 0.2807 - val_CHIPNexus.OCT4.profile_loss: 1604.9352s: 2861.8598 -\n",
      "Epoch 3/200\n",
      "276/276 [==============================] - 89s 323ms/step - loss: 1799.8657 - CHIPNexus.OCT4.logcount_loss: 0.2947 - CHIPNexus.OCT4.profile_loss: 1763.0220 - val_loss: 2164.5730 - val_CHIPNexus.OCT4.logcount_loss: 0.3261 - val_CHIPNexus.OCT4.profile_loss: 1595.4684: 1095.6055 - CHIPNexus\n",
      "Epoch 4/200\n",
      "276/276 [==============================] - 87s 316ms/step - loss: 1766.1983 - CHIPNexus.OCT4.logcount_loss: 0.2985 - CHIPNexus.OCT4.profile_loss: 1728.8833 - val_loss: 2017.9310 - val_CHIPNexus.OCT4.logcount_loss: 0.2778 - val_CHIPNexus.OCT4.profile_loss: 1560.4155\n",
      "Epoch 5/200\n",
      "  3/276 [..............................] - ETA: 35s - loss: 1022.6531 - CHIPNexus.OCT4.logcount_loss: 0.2673 - CHIPNexus.OCT4.profile_loss: 989.2449"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/avanti/anaconda3/envs/py376/lib/python3.7/site-packages/keras/callbacks/callbacks.py:95: RuntimeWarning: Method (on_train_batch_end) is slow compared to the batch update (0.131033). Check your callbacks.\n",
      "  % (hook_name, delta_t_median), RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "276/276 [==============================] - 80s 291ms/step - loss: 1741.0957 - CHIPNexus.OCT4.logcount_loss: 0.3005 - CHIPNexus.OCT4.profile_loss: 1703.5314 - val_loss: 2039.1553 - val_CHIPNexus.OCT4.logcount_loss: 0.2769 - val_CHIPNexus.OCT4.profile_loss: 1537.3818\n",
      "Epoch 6/200\n",
      "276/276 [==============================] - 82s 299ms/step - loss: 1717.0592 - CHIPNexus.OCT4.logcount_loss: 0.2999 - CHIPNexus.OCT4.profile_loss: 1679.5674 - val_loss: 1999.6953 - val_CHIPNexus.OCT4.logcount_loss: 0.2660 - val_CHIPNexus.OCT4.profile_loss: 1521.6556 30s - loss: 2236.8410 - CHIPNexus.OCT4.logcou -\n",
      "Epoch 7/200\n",
      "276/276 [==============================] - 84s 304ms/step - loss: 1696.0584 - CHIPNexus.OCT4.logcount_loss: 0.3009 - CHIPNexus.OCT4.profile_loss: 1658.4503 - val_loss: 1992.0507 - val_CHIPNexus.OCT4.logcount_loss: 0.2896 - val_CHIPNexus.OCT4.profile_loss: 1511.2706\n",
      "Epoch 8/200\n",
      "276/276 [==============================] - 81s 295ms/step - loss: 1673.5957 - CHIPNexus.OCT4.logcount_loss: 0.2889 - CHIPNexus.OCT4.profile_loss: 1637.4829 - val_loss: 1979.3792 - val_CHIPNexus.OCT4.logcount_loss: 0.2641 - val_CHIPNexus.OCT4.profile_loss: 1495.2820\n",
      "Epoch 9/200\n",
      "276/276 [==============================] - 86s 312ms/step - loss: 1652.9126 - CHIPNexus.OCT4.logcount_loss: 0.2878 - CHIPNexus.OCT4.profile_loss: 1616.9406 - val_loss: 1982.4729 - val_CHIPNexus.OCT4.logcount_loss: 0.2887 - val_CHIPNexus.OCT4.profile_loss: 1482.9562\n",
      "Epoch 10/200\n",
      "276/276 [==============================] - 82s 297ms/step - loss: 1631.0331 - CHIPNexus.OCT4.logcount_loss: 0.2888 - CHIPNexus.OCT4.profile_loss: 1594.9274 - val_loss: 1973.4398 - val_CHIPNexus.OCT4.logcount_loss: 0.2640 - val_CHIPNexus.OCT4.profile_loss: 1469.7977\n",
      "Epoch 11/200\n",
      "276/276 [==============================] - 84s 306ms/step - loss: 1605.9584 - CHIPNexus.OCT4.logcount_loss: 0.2902 - CHIPNexus.OCT4.profile_loss: 1569.6851 - val_loss: 2018.1750 - val_CHIPNexus.OCT4.logcount_loss: 0.2664 - val_CHIPNexus.OCT4.profile_loss: 1462.8696\n",
      "Epoch 12/200\n",
      "276/276 [==============================] - 86s 313ms/step - loss: 1583.3301 - CHIPNexus.OCT4.logcount_loss: 0.2878 - CHIPNexus.OCT4.profile_loss: 1547.3583 - val_loss: 2011.2631 - val_CHIPNexus.OCT4.logcount_loss: 0.2659 - val_CHIPNexus.OCT4.profile_loss: 1446.7125\n",
      "Epoch 13/200\n",
      "276/276 [==============================] - 84s 304ms/step - loss: 1561.5129 - CHIPNexus.OCT4.logcount_loss: 0.2863 - CHIPNexus.OCT4.profile_loss: 1525.7212 - val_loss: 1999.3014 - val_CHIPNexus.OCT4.logcount_loss: 0.2690 - val_CHIPNexus.OCT4.profile_loss: 1442.2218\n",
      "Epoch 14/200\n",
      "276/276 [==============================] - 83s 300ms/step - loss: 1538.3263 - CHIPNexus.OCT4.logcount_loss: 0.2887 - CHIPNexus.OCT4.profile_loss: 1502.2411 - val_loss: 2037.8623 - val_CHIPNexus.OCT4.logcount_loss: 0.2664 - val_CHIPNexus.OCT4.profile_loss: 1428.8116\n",
      "Epoch 15/200\n",
      "276/276 [==============================] - 82s 296ms/step - loss: 1509.3965 - CHIPNexus.OCT4.logcount_loss: 0.2962 - CHIPNexus.OCT4.profile_loss: 1472.3680 - val_loss: 2045.4003 - val_CHIPNexus.OCT4.logcount_loss: 0.2688 - val_CHIPNexus.OCT4.profile_loss: 1411.8892\n",
      "Epoch 16/200\n",
      "276/276 [==============================] - 78s 284ms/step - loss: 1497.5814 - CHIPNexus.OCT4.logcount_loss: 0.3099 - CHIPNexus.OCT4.profile_loss: 1458.8418 - val_loss: 1992.9719 - val_CHIPNexus.OCT4.logcount_loss: 0.2665 - val_CHIPNexus.OCT4.profile_loss: 1404.2816\n",
      "Epoch 17/200\n",
      "276/276 [==============================] - 83s 302ms/step - loss: 1444.3502 - CHIPNexus.OCT4.logcount_loss: 0.3109 - CHIPNexus.OCT4.profile_loss: 1405.4930 - val_loss: 2037.9227 - val_CHIPNexus.OCT4.logcount_loss: 0.2785 - val_CHIPNexus.OCT4.profile_loss: 1399.8575\n",
      "Epoch 18/200\n",
      "276/276 [==============================] - 84s 303ms/step - loss: 1371.2191 - CHIPNexus.OCT4.logcount_loss: 0.2874 - CHIPNexus.OCT4.profile_loss: 1335.2938 - val_loss: 2012.7659 - val_CHIPNexus.OCT4.logcount_loss: 0.2719 - val_CHIPNexus.OCT4.profile_loss: 1389.7954\n",
      "Epoch 19/200\n",
      "276/276 [==============================] - 81s 295ms/step - loss: 1328.9967 - CHIPNexus.OCT4.logcount_loss: 0.2921 - CHIPNexus.OCT4.profile_loss: 1292.4789 - val_loss: 1978.0259 - val_CHIPNexus.OCT4.logcount_loss: 0.2819 - val_CHIPNexus.OCT4.profile_loss: 1384.2870\n",
      "Epoch 20/200\n",
      "276/276 [==============================] - 80s 289ms/step - loss: 1287.2127 - CHIPNexus.OCT4.logcount_loss: 0.2880 - CHIPNexus.OCT4.profile_loss: 1251.2181 - val_loss: 2017.6571 - val_CHIPNexus.OCT4.logcount_loss: 0.2708 - val_CHIPNexus.OCT4.profile_loss: 1384.6062oss: 0.289\n",
      "val_loss 1973.4398193359375\n",
      "val_CHIPNexus.OCT4.logcount_loss 0.2639563977718353\n",
      "val_CHIPNexus.OCT4.profile_loss 1469.7977294921875\n",
      "loss 1631.1932888453325\n",
      "CHIPNexus.OCT4.logcount_loss 0.2888419\n",
      "CHIPNexus.OCT4.profile_loss 1594.9274\n",
      "Seed: 1534\n",
      "LOG SPACE PREDS IS SET TO True\n",
      "Oct4only_ctaskweight250_seed1534.h5\n",
      "Model: \"model_23\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "sequence (InputLayer)           (None, 1346, 4)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_206 (Conv1D)             (None, 1326, 64)     5440        sequence[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_145 (Lambda)             (None, 1322, 64)     0           conv1d_206[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_207 (Conv1D)             (None, 1322, 64)     12352       conv1d_206[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_145 (Add)                   (None, 1322, 64)     0           lambda_145[0][0]                 \n",
      "                                                                 conv1d_207[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_146 (Lambda)             (None, 1314, 64)     0           add_145[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_208 (Conv1D)             (None, 1314, 64)     12352       add_145[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_146 (Add)                   (None, 1314, 64)     0           lambda_146[0][0]                 \n",
      "                                                                 conv1d_208[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_147 (Lambda)             (None, 1298, 64)     0           add_146[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_209 (Conv1D)             (None, 1298, 64)     12352       add_146[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_147 (Add)                   (None, 1298, 64)     0           lambda_147[0][0]                 \n",
      "                                                                 conv1d_209[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_148 (Lambda)             (None, 1266, 64)     0           add_147[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_210 (Conv1D)             (None, 1266, 64)     12352       add_147[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_148 (Add)                   (None, 1266, 64)     0           lambda_148[0][0]                 \n",
      "                                                                 conv1d_210[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_149 (Lambda)             (None, 1202, 64)     0           add_148[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_211 (Conv1D)             (None, 1202, 64)     12352       add_148[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_149 (Add)                   (None, 1202, 64)     0           lambda_149[0][0]                 \n",
      "                                                                 conv1d_211[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_150 (Lambda)             (None, 1074, 64)     0           add_149[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_212 (Conv1D)             (None, 1074, 64)     12352       add_149[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_150 (Add)                   (None, 1074, 64)     0           lambda_150[0][0]                 \n",
      "                                                                 conv1d_212[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_25 (Gl (None, 64)           0           add_150[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "patchcap.logcount (InputLayer)  (None, 2)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_213 (Conv1D)             (None, 1000, 2)      9602        add_150[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "patchcap.profile (InputLayer)   (None, 1000, 2)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_77 (Concatenate)    (None, 66)           0           global_average_pooling1d_25[0][0]\n",
      "                                                                 patchcap.logcount[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_78 (Concatenate)    (None, 1000, 4)      0           conv1d_213[0][0]                 \n",
      "                                                                 patchcap.profile[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "CHIPNexus.OCT4.logcount (Dense) (None, 2)            134         concatenate_77[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "CHIPNexus.OCT4.profile (Conv1D) (None, 1000, 2)      10          concatenate_78[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 89,298\n",
      "Trainable params: 89,298\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "276/276 [==============================] - 77s 280ms/step - loss: 2083.9030 - CHIPNexus.OCT4.logcount_loss: 0.4370 - CHIPNexus.OCT4.profile_loss: 1974.6608 - val_loss: 2313.0430 - val_CHIPNexus.OCT4.logcount_loss: 0.2902 - val_CHIPNexus.OCT4.profile_loss: 1729.9860\n",
      "Epoch 2/200\n",
      "276/276 [==============================] - 85s 308ms/step - loss: 2007.1566 - CHIPNexus.OCT4.logcount_loss: 0.3004 - CHIPNexus.OCT4.profile_loss: 1932.0609 - val_loss: 2249.7327 - val_CHIPNexus.OCT4.logcount_loss: 0.2778 - val_CHIPNexus.OCT4.profile_loss: 1693.2487\n",
      "Epoch 3/200\n",
      "276/276 [==============================] - 82s 296ms/step - loss: 1958.8464 - CHIPNexus.OCT4.logcount_loss: 0.3033 - CHIPNexus.OCT4.profile_loss: 1883.0267 - val_loss: 2242.0244 - val_CHIPNexus.OCT4.logcount_loss: 0.2775 - val_CHIPNexus.OCT4.profile_loss: 1664.5242\n",
      "Epoch 4/200\n",
      "276/276 [==============================] - 79s 285ms/step - loss: 1914.8159 - CHIPNexus.OCT4.logcount_loss: 0.3013 - CHIPNexus.OCT4.profile_loss: 1839.4867 - val_loss: 2168.7915 - val_CHIPNexus.OCT4.logcount_loss: 0.2725 - val_CHIPNexus.OCT4.profile_loss: 1630.636032.8797 - CHIPNexus.OCT4.logcount_loss: 0.3013 - CHIPNexus.OCT4.profile_los\n",
      "Epoch 5/200\n",
      "  3/276 [..............................] - ETA: 28s - loss: 1082.1381 - CHIPNexus.OCT4.logcount_loss: 0.2691 - CHIPNexus.OCT4.profile_loss: 1014.8576"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/avanti/anaconda3/envs/py376/lib/python3.7/site-packages/keras/callbacks/callbacks.py:95: RuntimeWarning: Method (on_train_batch_end) is slow compared to the batch update (0.114502). Check your callbacks.\n",
      "  % (hook_name, delta_t_median), RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "276/276 [==============================] - 79s 285ms/step - loss: 1887.7237 - CHIPNexus.OCT4.logcount_loss: 0.2972 - CHIPNexus.OCT4.profile_loss: 1813.4319 - val_loss: 2100.0605 - val_CHIPNexus.OCT4.logcount_loss: 0.3230 - val_CHIPNexus.OCT4.profile_loss: 1616.7738nt_loss: 0.2856 - CH - ETA: 27s - loss: 2451.0184 - CHIPNexus.OCT4.logcount_loss: 0.2878 - CHIPNexus.OCT4.profile_loss - ETA: 25s - loss: 2364.6469 - CHIPNexus.OCT4.logcount_loss: 0.2876 - CHIPNexus.OCT4. - ETA: 19s - loss: 2220.1474 - CHIPNexus.OCT4.logcount_loss: 0.2935 - CHIPNex - ETA: 12s - l\n",
      "Epoch 6/200\n",
      "276/276 [==============================] - 85s 307ms/step - loss: 1860.9282 - CHIPNexus.OCT4.logcount_loss: 0.2935 - CHIPNexus.OCT4.profile_loss: 1787.5460 - val_loss: 2097.3076 - val_CHIPNexus.OCT4.logcount_loss: 0.2711 - val_CHIPNexus.OCT4.profile_loss: 1599.3951\n",
      "Epoch 7/200\n",
      "276/276 [==============================] - 85s 309ms/step - loss: 1837.2775 - CHIPNexus.OCT4.logcount_loss: 0.2917 - CHIPNexus.OCT4.profile_loss: 1764.3615 - val_loss: 2076.7632 - val_CHIPNexus.OCT4.logcount_loss: 0.2716 - val_CHIPNexus.OCT4.profile_loss: 1586.5687\n",
      "Epoch 8/200\n",
      "276/276 [==============================] - 89s 323ms/step - loss: 1812.9635 - CHIPNexus.OCT4.logcount_loss: 0.2874 - CHIPNexus.OCT4.profile_loss: 1741.1123 - val_loss: 2079.7007 - val_CHIPNexus.OCT4.logcount_loss: 0.2638 - val_CHIPNexus.OCT4.profile_loss: 1571.4054\n",
      "Epoch 9/200\n",
      "276/276 [==============================] - 79s 285ms/step - loss: 1791.8201 - CHIPNexus.OCT4.logcount_loss: 0.2892 - CHIPNexus.OCT4.profile_loss: 1719.5316 - val_loss: 2052.4626 - val_CHIPNexus.OCT4.logcount_loss: 0.2855 - val_CHIPNexus.OCT4.profile_loss: 1560.8588PNexus.OCT4.logcou - ETA: 1s - loss: 1804.5542 - CHIPNexus.OCT4.logcount_loss: 0.2894 - CHIPNexus.OCT4.profile_loss:\n",
      "Epoch 10/200\n",
      "276/276 [==============================] - 80s 291ms/step - loss: 1764.0805 - CHIPNexus.OCT4.logcount_loss: 0.2860 - CHIPNexus.OCT4.profile_loss: 1692.5868 - val_loss: 2374.5505 - val_CHIPNexus.OCT4.logcount_loss: 0.4182 - val_CHIPNexus.OCT4.profile_loss: 1594.7672\n",
      "Epoch 11/200\n",
      "276/276 [==============================] - 92s 333ms/step - loss: 1751.5802 - CHIPNexus.OCT4.logcount_loss: 0.2939 - CHIPNexus.OCT4.profile_loss: 1678.1117 - val_loss: 2114.4612 - val_CHIPNexus.OCT4.logcount_loss: 0.2632 - val_CHIPNexus.OCT4.profile_loss: 1532.7642\n",
      "Epoch 12/200\n",
      "276/276 [==============================] - 82s 298ms/step - loss: 1727.6374 - CHIPNexus.OCT4.logcount_loss: 0.2956 - CHIPNexus.OCT4.profile_loss: 1653.7277 - val_loss: 2070.8442 - val_CHIPNexus.OCT4.logcount_loss: 0.2633 - val_CHIPNexus.OCT4.profile_loss: 1526.0034\n",
      "Epoch 13/200\n",
      "276/276 [==============================] - 81s 294ms/step - loss: 1704.5225 - CHIPNexus.OCT4.logcount_loss: 0.2891 - CHIPNexus.OCT4.profile_loss: 1632.2363 - val_loss: 2087.4250 - val_CHIPNexus.OCT4.logcount_loss: 0.2787 - val_CHIPNexus.OCT4.profile_loss: 1507.9744\n",
      "Epoch 14/200\n",
      "276/276 [==============================] - 80s 291ms/step - loss: 1686.1320 - CHIPNexus.OCT4.logcount_loss: 0.2850 - CHIPNexus.OCT4.profile_loss: 1614.8893 - val_loss: 2109.7856 - val_CHIPNexus.OCT4.logcount_loss: 0.2949 - val_CHIPNexus.OCT4.profile_loss: 1495.08424.logcount_loss: 0.2857 - CHIPNexus.OCT4.pro\n",
      "Epoch 15/200\n",
      "276/276 [==============================] - 82s 298ms/step - loss: 1657.7892 - CHIPNexus.OCT4.logcount_loss: 0.2878 - CHIPNexus.OCT4.profile_loss: 1585.8508 - val_loss: 2089.3762 - val_CHIPNexus.OCT4.logcount_loss: 0.2683 - val_CHIPNexus.OCT4.profile_loss: 1488.8375\n",
      "Epoch 16/200\n",
      "276/276 [==============================] - 80s 289ms/step - loss: 1633.7207 - CHIPNexus.OCT4.logcount_loss: 0.2985 - CHIPNexus.OCT4.profile_loss: 1559.0863 - val_loss: 2155.5010 - val_CHIPNexus.OCT4.logcount_loss: 0.3196 - val_CHIPNexus.OCT4.profile_loss: 1473.4760\n",
      "Epoch 17/200\n",
      "276/276 [==============================] - 81s 292ms/step - loss: 1609.1486 - CHIPNexus.OCT4.logcount_loss: 0.2979 - CHIPNexus.OCT4.profile_loss: 1534.6678 - val_loss: 2024.1409 - val_CHIPNexus.OCT4.logcount_loss: 0.3054 - val_CHIPNexus.OCT4.profile_loss: 1466.5199\n",
      "Epoch 18/200\n",
      "276/276 [==============================] - 81s 292ms/step - loss: 1570.7506 - CHIPNexus.OCT4.logcount_loss: 0.3056 - CHIPNexus.OCT4.profile_loss: 1494.3453 - val_loss: 2068.7019 - val_CHIPNexus.OCT4.logcount_loss: 0.2711 - val_CHIPNexus.OCT4.profile_loss: 1455.9724\n",
      "Epoch 19/200\n",
      "276/276 [==============================] - 79s 286ms/step - loss: 1500.5398 - CHIPNexus.OCT4.logcount_loss: 0.3042 - CHIPNexus.OCT4.profile_loss: 1424.4847 - val_loss: 2046.2585 - val_CHIPNexus.OCT4.logcount_loss: 0.2811 - val_CHIPNexus.OCT4.profile_loss: 1445.3309ss: 10\n",
      "Epoch 20/200\n",
      "276/276 [==============================] - 80s 290ms/step - loss: 1469.9290 - CHIPNexus.OCT4.logcount_loss: 0.3251 - CHIPNexus.OCT4.profile_loss: 1388.6543 - val_loss: 2218.7048 - val_CHIPNexus.OCT4.logcount_loss: 0.2695 - val_CHIPNexus.OCT4.profile_loss: 1486.0961\n",
      "Epoch 21/200\n",
      "276/276 [==============================] - 81s 295ms/step - loss: 1367.8844 - CHIPNexus.OCT4.logcount_loss: 0.2893 - CHIPNexus.OCT4.profile_loss: 1295.5671 - val_loss: 2137.9995 - val_CHIPNexus.OCT4.logcount_loss: 0.3332 - val_CHIPNexus.OCT4.profile_loss: 1435.5070\n",
      "Epoch 22/200\n",
      "276/276 [==============================] - 81s 295ms/step - loss: 1359.6630 - CHIPNexus.OCT4.logcount_loss: 0.2902 - CHIPNexus.OCT4.profile_loss: 1287.1021 - val_loss: 2119.5752 - val_CHIPNexus.OCT4.logcount_loss: 0.2716 - val_CHIPNexus.OCT4.profile_loss: 1432.6838\n",
      "Epoch 23/200\n",
      "276/276 [==============================] - 82s 296ms/step - loss: 1519.6790 - CHIPNexus.OCT4.logcount_loss: 0.3022 - CHIPNexus.OCT4.profile_loss: 1444.1198 - val_loss: 2107.0164 - val_CHIPNexus.OCT4.logcount_loss: 0.2693 - val_CHIPNexus.OCT4.profile_loss: 1417.4575\n",
      "Epoch 24/200\n",
      "276/276 [==============================] - 79s 285ms/step - loss: 1332.1733 - CHIPNexus.OCT4.logcount_loss: 0.2851 - CHIPNexus.OCT4.profile_loss: 1260.8939 - val_loss: 2118.5364 - val_CHIPNexus.OCT4.logcount_loss: 0.2671 - val_CHIPNexus.OCT4.profile_loss: 1417.8411\n",
      "Epoch 25/200\n",
      "276/276 [==============================] - 82s 296ms/step - loss: 1315.9491 - CHIPNexus.OCT4.logcount_loss: 0.2812 - CHIPNexus.OCT4.profile_loss: 1245.6520 - val_loss: 2092.5417 - val_CHIPNexus.OCT4.logcount_loss: 0.2688 - val_CHIPNexus.OCT4.profile_loss: 1403.0835\n",
      "Epoch 26/200\n",
      "276/276 [==============================] - 79s 287ms/step - loss: 1379.2276 - CHIPNexus.OCT4.logcount_loss: 0.2930 - CHIPNexus.OCT4.profile_loss: 1305.9791 - val_loss: 2099.5271 - val_CHIPNexus.OCT4.logcount_loss: 0.2772 - val_CHIPNexus.OCT4.profile_loss: 1389.9305\n",
      "Epoch 27/200\n",
      "276/276 [==============================] - 81s 294ms/step - loss: 1255.1474 - CHIPNexus.OCT4.logcount_loss: 0.2815 - CHIPNexus.OCT4.profile_loss: 1184.7682 - val_loss: 2086.4092 - val_CHIPNexus.OCT4.logcount_loss: 0.2660 - val_CHIPNexus.OCT4.profile_loss: 1390.9117\n",
      "val_loss 2024.140869140625\n",
      "val_CHIPNexus.OCT4.logcount_loss 0.30540117621421814\n",
      "val_CHIPNexus.OCT4.profile_loss 1466.5198974609375\n",
      "loss 1609.2756435585034\n",
      "CHIPNexus.OCT4.logcount_loss 0.29792282\n",
      "CHIPNexus.OCT4.profile_loss 1534.6678\n",
      "Seed: 1534\n",
      "LOG SPACE PREDS IS SET TO True\n",
      "Oct4only_ctaskweight500_seed1534.h5\n",
      "Model: \"model_24\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "sequence (InputLayer)           (None, 1346, 4)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_214 (Conv1D)             (None, 1326, 64)     5440        sequence[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_151 (Lambda)             (None, 1322, 64)     0           conv1d_214[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_215 (Conv1D)             (None, 1322, 64)     12352       conv1d_214[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_151 (Add)                   (None, 1322, 64)     0           lambda_151[0][0]                 \n",
      "                                                                 conv1d_215[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_152 (Lambda)             (None, 1314, 64)     0           add_151[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_216 (Conv1D)             (None, 1314, 64)     12352       add_151[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_152 (Add)                   (None, 1314, 64)     0           lambda_152[0][0]                 \n",
      "                                                                 conv1d_216[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_153 (Lambda)             (None, 1298, 64)     0           add_152[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_217 (Conv1D)             (None, 1298, 64)     12352       add_152[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_153 (Add)                   (None, 1298, 64)     0           lambda_153[0][0]                 \n",
      "                                                                 conv1d_217[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_154 (Lambda)             (None, 1266, 64)     0           add_153[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_218 (Conv1D)             (None, 1266, 64)     12352       add_153[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_154 (Add)                   (None, 1266, 64)     0           lambda_154[0][0]                 \n",
      "                                                                 conv1d_218[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_155 (Lambda)             (None, 1202, 64)     0           add_154[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_219 (Conv1D)             (None, 1202, 64)     12352       add_154[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_155 (Add)                   (None, 1202, 64)     0           lambda_155[0][0]                 \n",
      "                                                                 conv1d_219[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_156 (Lambda)             (None, 1074, 64)     0           add_155[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_220 (Conv1D)             (None, 1074, 64)     12352       add_155[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_156 (Add)                   (None, 1074, 64)     0           lambda_156[0][0]                 \n",
      "                                                                 conv1d_220[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_26 (Gl (None, 64)           0           add_156[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "patchcap.logcount (InputLayer)  (None, 2)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_221 (Conv1D)             (None, 1000, 2)      9602        add_156[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "patchcap.profile (InputLayer)   (None, 1000, 2)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_79 (Concatenate)    (None, 66)           0           global_average_pooling1d_26[0][0]\n",
      "                                                                 patchcap.logcount[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_80 (Concatenate)    (None, 1000, 4)      0           conv1d_221[0][0]                 \n",
      "                                                                 patchcap.profile[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "CHIPNexus.OCT4.logcount (Dense) (None, 2)            134         concatenate_79[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "CHIPNexus.OCT4.profile (Conv1D) (None, 1000, 2)      10          concatenate_80[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 89,298\n",
      "Trainable params: 89,298\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "276/276 [==============================] - 77s 279ms/step - loss: 2164.3789 - CHIPNexus.OCT4.logcount_loss: 0.4095 - CHIPNexus.OCT4.profile_loss: 1959.6447 - val_loss: 2422.6377 - val_CHIPNexus.OCT4.logcount_loss: 0.3044 - val_CHIPNexus.OCT4.profile_loss: 1718.3329\n",
      "Epoch 2/200\n",
      "276/276 [==============================] - 77s 279ms/step - loss: 2068.9594 - CHIPNexus.OCT4.logcount_loss: 0.2976 - CHIPNexus.OCT4.profile_loss: 1920.1599 - val_loss: 2543.3521 - val_CHIPNexus.OCT4.logcount_loss: 0.2827 - val_CHIPNexus.OCT4.profile_loss: 1685.912779 - CHIPNexus.OCT4.logcount_loss: 0.2985 - CHIPNexus.OCT4 - ETA: 12s - loss: 2259.9082 - CHIPNexus.OCT4.logcount_loss: 0.2974 - CHIPNexus.OCT4.profile_loss: 2111.198 - ETA: 11s - l\n",
      "Epoch 3/200\n",
      "276/276 [==============================] - 80s 288ms/step - loss: 2030.3846 - CHIPNexus.OCT4.logcount_loss: 0.3035 - CHIPNexus.OCT4.profile_loss: 1878.6383 - val_loss: 2510.9937 - val_CHIPNexus.OCT4.logcount_loss: 0.2812 - val_CHIPNexus.OCT4.profile_loss: 1654.5521\n",
      "Epoch 4/200\n",
      "276/276 [==============================] - 81s 292ms/step - loss: 1985.7390 - CHIPNexus.OCT4.logcount_loss: 0.2974 - CHIPNexus.OCT4.profile_loss: 1837.0397 - val_loss: 2343.4736 - val_CHIPNexus.OCT4.logcount_loss: 0.2798 - val_CHIPNexus.OCT4.profile_loss: 1629.0421\n",
      "Epoch 5/200\n",
      "276/276 [==============================] - 81s 293ms/step - loss: 1949.8369 - CHIPNexus.OCT4.logcount_loss: 0.2979 - CHIPNexus.OCT4.profile_loss: 1800.8695 - val_loss: 2305.9084 - val_CHIPNexus.OCT4.logcount_loss: 0.2732 - val_CHIPNexus.OCT4.profile_loss: 1606.4060\n",
      "Epoch 6/200\n",
      "276/276 [==============================] - 82s 296ms/step - loss: 1918.8797 - CHIPNexus.OCT4.logcount_loss: 0.2906 - CHIPNexus.OCT4.profile_loss: 1773.5739 - val_loss: 2360.4780 - val_CHIPNexus.OCT4.logcount_loss: 0.2679 - val_CHIPNexus.OCT4.profile_loss: 1595.9614\n",
      "Epoch 7/200\n",
      "276/276 [==============================] - 81s 293ms/step - loss: 1896.3064 - CHIPNexus.OCT4.logcount_loss: 0.2931 - CHIPNexus.OCT4.profile_loss: 1749.7501 - val_loss: 2347.1804 - val_CHIPNexus.OCT4.logcount_loss: 0.2656 - val_CHIPNexus.OCT4.profile_loss: 1579.8540\n",
      "Epoch 8/200\n",
      "276/276 [==============================] - 89s 322ms/step - loss: 1870.5727 - CHIPNexus.OCT4.logcount_loss: 0.2898 - CHIPNexus.OCT4.profile_loss: 1725.6637 - val_loss: 2238.8313 - val_CHIPNexus.OCT4.logcount_loss: 0.2740 - val_CHIPNexus.OCT4.profile_loss: 1563.8732\n",
      "Epoch 9/200\n",
      "276/276 [==============================] - 82s 295ms/step - loss: 1849.6015 - CHIPNexus.OCT4.logcount_loss: 0.2912 - CHIPNexus.OCT4.profile_loss: 1704.0112 - val_loss: 2302.0691 - val_CHIPNexus.OCT4.logcount_loss: 0.2606 - val_CHIPNexus.OCT4.profile_loss: 1551.4736\n",
      "Epoch 10/200\n",
      "276/276 [==============================] - 81s 292ms/step - loss: 1828.6842 - CHIPNexus.OCT4.logcount_loss: 0.2863 - CHIPNexus.OCT4.profile_loss: 1685.5414 - val_loss: 2218.6030 - val_CHIPNexus.OCT4.logcount_loss: 0.2664 - val_CHIPNexus.OCT4.profile_loss: 1536.2133\n",
      "Epoch 11/200\n",
      "276/276 [==============================] - 81s 292ms/step - loss: 1800.9397 - CHIPNexus.OCT4.logcount_loss: 0.2746 - CHIPNexus.OCT4.profile_loss: 1663.6613 - val_loss: 2296.2786 - val_CHIPNexus.OCT4.logcount_loss: 0.2594 - val_CHIPNexus.OCT4.profile_loss: 1526.1898\n",
      "Epoch 12/200\n",
      "276/276 [==============================] - 81s 295ms/step - loss: 1779.2568 - CHIPNexus.OCT4.logcount_loss: 0.2794 - CHIPNexus.OCT4.profile_loss: 1639.5399 - val_loss: 2343.9219 - val_CHIPNexus.OCT4.logcount_loss: 0.2762 - val_CHIPNexus.OCT4.profile_loss: 1513.0400\n",
      "Epoch 13/200\n",
      "276/276 [==============================] - 81s 292ms/step - loss: 1745.3225 - CHIPNexus.OCT4.logcount_loss: 0.2628 - CHIPNexus.OCT4.profile_loss: 1613.9181 - val_loss: 2391.5000 - val_CHIPNexus.OCT4.logcount_loss: 0.2695 - val_CHIPNexus.OCT4.profile_loss: 1541.8069\n",
      "Epoch 14/200\n",
      "  3/276 [..............................] - ETA: 32s - loss: 1155.0071 - CHIPNexus.OCT4.logcount_loss: 0.2518 - CHIPNexus.OCT4.profile_loss: 1029.1255"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/avanti/anaconda3/envs/py376/lib/python3.7/site-packages/keras/callbacks/callbacks.py:95: RuntimeWarning: Method (on_train_batch_end) is slow compared to the batch update (0.115256). Check your callbacks.\n",
      "  % (hook_name, delta_t_median), RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "276/276 [==============================] - 80s 291ms/step - loss: 1728.7186 - CHIPNexus.OCT4.logcount_loss: 0.2754 - CHIPNexus.OCT4.profile_loss: 1591.0205 - val_loss: 2341.0159 - val_CHIPNexus.OCT4.logcount_loss: 0.2766 - val_CHIPNexus.OCT4.profile_loss: 1550.4631\n",
      "Epoch 15/200\n",
      "276/276 [==============================] - 80s 288ms/step - loss: 1708.2505 - CHIPNexus.OCT4.logcount_loss: 0.2873 - CHIPNexus.OCT4.profile_loss: 1564.6172 - val_loss: 2232.8293 - val_CHIPNexus.OCT4.logcount_loss: 0.2503 - val_CHIPNexus.OCT4.profile_loss: 1477.2659\n",
      "Epoch 16/200\n",
      "276/276 [==============================] - 80s 290ms/step - loss: 1694.4412 - CHIPNexus.OCT4.logcount_loss: 0.2698 - CHIPNexus.OCT4.profile_loss: 1559.5457 - val_loss: 2290.1580 - val_CHIPNexus.OCT4.logcount_loss: 0.2613 - val_CHIPNexus.OCT4.profile_loss: 1472.3518\n",
      "Epoch 17/200\n",
      "276/276 [==============================] - 78s 282ms/step - loss: 1669.0752 - CHIPNexus.OCT4.logcount_loss: 0.3063 - CHIPNexus.OCT4.profile_loss: 1515.9039 - val_loss: 2244.9937 - val_CHIPNexus.OCT4.logcount_loss: 0.2554 - val_CHIPNexus.OCT4.profile_loss: 1469.9131\n",
      "Epoch 18/200\n",
      "276/276 [==============================] - 80s 291ms/step - loss: 1616.7149 - CHIPNexus.OCT4.logcount_loss: 0.2987 - CHIPNexus.OCT4.profile_loss: 1467.3661 - val_loss: 2317.1431 - val_CHIPNexus.OCT4.logcount_loss: 0.2660 - val_CHIPNexus.OCT4.profile_loss: 1455.2867\n",
      "Epoch 19/200\n",
      "276/276 [==============================] - 81s 294ms/step - loss: 1539.8360 - CHIPNexus.OCT4.logcount_loss: 0.2992 - CHIPNexus.OCT4.profile_loss: 1390.2440 - val_loss: 2599.6050 - val_CHIPNexus.OCT4.logcount_loss: 0.3951 - val_CHIPNexus.OCT4.profile_loss: 1596.1578\n",
      "Epoch 20/200\n",
      "276/276 [==============================] - 81s 292ms/step - loss: 1509.9233 - CHIPNexus.OCT4.logcount_loss: 0.2872 - CHIPNexus.OCT4.profile_loss: 1366.3467 - val_loss: 2248.4556 - val_CHIPNexus.OCT4.logcount_loss: 0.2654 - val_CHIPNexus.OCT4.profile_loss: 1442.6351\n",
      "val_loss 2218.60302734375\n",
      "val_CHIPNexus.OCT4.logcount_loss 0.2664232850074768\n",
      "val_CHIPNexus.OCT4.profile_loss 1536.2132568359375\n",
      "loss 1828.86906084622\n",
      "CHIPNexus.OCT4.logcount_loss 0.286284\n",
      "CHIPNexus.OCT4.profile_loss 1685.5414\n",
      "Seed: 1634\n",
      "LOG SPACE PREDS IS SET TO True\n",
      "Oct4only_ctaskweight0_seed1634.h5\n",
      "Model: \"model_25\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "sequence (InputLayer)           (None, 1346, 4)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_222 (Conv1D)             (None, 1326, 64)     5440        sequence[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_157 (Lambda)             (None, 1322, 64)     0           conv1d_222[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_223 (Conv1D)             (None, 1322, 64)     12352       conv1d_222[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_157 (Add)                   (None, 1322, 64)     0           lambda_157[0][0]                 \n",
      "                                                                 conv1d_223[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_158 (Lambda)             (None, 1314, 64)     0           add_157[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_224 (Conv1D)             (None, 1314, 64)     12352       add_157[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_158 (Add)                   (None, 1314, 64)     0           lambda_158[0][0]                 \n",
      "                                                                 conv1d_224[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_159 (Lambda)             (None, 1298, 64)     0           add_158[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_225 (Conv1D)             (None, 1298, 64)     12352       add_158[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_159 (Add)                   (None, 1298, 64)     0           lambda_159[0][0]                 \n",
      "                                                                 conv1d_225[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_160 (Lambda)             (None, 1266, 64)     0           add_159[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_226 (Conv1D)             (None, 1266, 64)     12352       add_159[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_160 (Add)                   (None, 1266, 64)     0           lambda_160[0][0]                 \n",
      "                                                                 conv1d_226[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_161 (Lambda)             (None, 1202, 64)     0           add_160[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_227 (Conv1D)             (None, 1202, 64)     12352       add_160[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_161 (Add)                   (None, 1202, 64)     0           lambda_161[0][0]                 \n",
      "                                                                 conv1d_227[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_162 (Lambda)             (None, 1074, 64)     0           add_161[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_228 (Conv1D)             (None, 1074, 64)     12352       add_161[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_162 (Add)                   (None, 1074, 64)     0           lambda_162[0][0]                 \n",
      "                                                                 conv1d_228[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_27 (Gl (None, 64)           0           add_162[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "patchcap.logcount (InputLayer)  (None, 2)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_229 (Conv1D)             (None, 1000, 2)      9602        add_162[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "patchcap.profile (InputLayer)   (None, 1000, 2)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_81 (Concatenate)    (None, 66)           0           global_average_pooling1d_27[0][0]\n",
      "                                                                 patchcap.logcount[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_82 (Concatenate)    (None, 1000, 4)      0           conv1d_229[0][0]                 \n",
      "                                                                 patchcap.profile[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "CHIPNexus.OCT4.logcount (Dense) (None, 2)            134         concatenate_81[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "CHIPNexus.OCT4.profile (Conv1D) (None, 1000, 2)      10          concatenate_82[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 89,298\n",
      "Trainable params: 89,298\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "276/276 [==============================] - 85s 309ms/step - loss: 1420.1063 - CHIPNexus.OCT4.logcount_loss: 44.9352 - CHIPNexus.OCT4.profile_loss: 1420.1061 - val_loss: 1979.3431 - val_CHIPNexus.OCT4.logcount_loss: 49.5715 - val_CHIPNexus.OCT4.profile_loss: 1308.1155\n",
      "Epoch 2/200\n",
      "276/276 [==============================] - 79s 286ms/step - loss: 1367.6005 - CHIPNexus.OCT4.logcount_loss: 49.7772 - CHIPNexus.OCT4.profile_loss: 1367.6007 - val_loss: 1938.9216 - val_CHIPNexus.OCT4.logcount_loss: 42.2155 - val_CHIPNexus.OCT4.profile_loss: 1264.2054\n",
      "Epoch 3/200\n",
      "276/276 [==============================] - 80s 288ms/step - loss: 1337.9386 - CHIPNexus.OCT4.logcount_loss: 42.8168 - CHIPNexus.OCT4.profile_loss: 1337.9384 - val_loss: 1923.2684 - val_CHIPNexus.OCT4.logcount_loss: 44.7133 - val_CHIPNexus.OCT4.profile_loss: 1244.1754\n",
      "Epoch 4/200\n",
      "276/276 [==============================] - 81s 294ms/step - loss: 1314.6330 - CHIPNexus.OCT4.logcount_loss: 46.0723 - CHIPNexus.OCT4.profile_loss: 1314.6321 - val_loss: 1883.7927 - val_CHIPNexus.OCT4.logcount_loss: 43.0191 - val_CHIPNexus.OCT4.profile_loss: 1227.4983\n",
      "Epoch 5/200\n",
      "276/276 [==============================] - 82s 297ms/step - loss: 1297.9807 - CHIPNexus.OCT4.logcount_loss: 38.0412 - CHIPNexus.OCT4.profile_loss: 1297.9811 - val_loss: 1870.4576 - val_CHIPNexus.OCT4.logcount_loss: 36.0362 - val_CHIPNexus.OCT4.profile_loss: 1218.3402\n",
      "Epoch 6/200\n",
      "276/276 [==============================] - 81s 294ms/step - loss: 1281.3338 - CHIPNexus.OCT4.logcount_loss: 50.3589 - CHIPNexus.OCT4.profile_loss: 1281.3335 - val_loss: 1877.6456 - val_CHIPNexus.OCT4.logcount_loss: 55.6378 - val_CHIPNexus.OCT4.profile_loss: 1200.3055\n",
      "Epoch 7/200\n",
      "276/276 [==============================] - 79s 286ms/step - loss: 1266.7469 - CHIPNexus.OCT4.logcount_loss: 45.0444 - CHIPNexus.OCT4.profile_loss: 1266.7468 - val_loss: 1849.3850 - val_CHIPNexus.OCT4.logcount_loss: 39.3690 - val_CHIPNexus.OCT4.profile_loss: 1191.909378 - CHIPNexus.OCT4.logcount_loss: 46.7028 \n",
      "Epoch 8/200\n",
      "276/276 [==============================] - 80s 291ms/step - loss: 1252.1950 - CHIPNexus.OCT4.logcount_loss: 109.0531 - CHIPNexus.OCT4.profile_loss: 1252.1946 - val_loss: 1897.0690 - val_CHIPNexus.OCT4.logcount_loss: 108.0022 - val_CHIPNexus.OCT4.profile_loss: 1178.8384\n",
      "Epoch 9/200\n",
      "276/276 [==============================] - 82s 297ms/step - loss: 1239.0016 - CHIPNexus.OCT4.logcount_loss: 79.2684 - CHIPNexus.OCT4.profile_loss: 1239.0015 - val_loss: 1885.4388 - val_CHIPNexus.OCT4.logcount_loss: 55.9794 - val_CHIPNexus.OCT4.profile_loss: 1168.2764\n",
      "Epoch 10/200\n",
      "276/276 [==============================] - 89s 322ms/step - loss: 1223.2778 - CHIPNexus.OCT4.logcount_loss: 51.9539 - CHIPNexus.OCT4.profile_loss: 1223.2777 - val_loss: 1865.9525 - val_CHIPNexus.OCT4.logcount_loss: 45.1420 - val_CHIPNexus.OCT4.profile_loss: 1161.0582\n",
      "Epoch 11/200\n",
      "276/276 [==============================] - 80s 291ms/step - loss: 1207.8457 - CHIPNexus.OCT4.logcount_loss: 41.5935 - CHIPNexus.OCT4.profile_loss: 1207.8455 - val_loss: 1893.8647 - val_CHIPNexus.OCT4.logcount_loss: 34.6112 - val_CHIPNexus.OCT4.profile_loss: 1150.5618\n",
      "Epoch 12/200\n",
      "276/276 [==============================] - 80s 289ms/step - loss: 1192.0823 - CHIPNexus.OCT4.logcount_loss: 43.4757 - CHIPNexus.OCT4.profile_loss: 1192.0822 - val_loss: 1913.7615 - val_CHIPNexus.OCT4.logcount_loss: 80.2600 - val_CHIPNexus.OCT4.profile_loss: 1154.6475\n",
      "Epoch 13/200\n",
      "276/276 [==============================] - 80s 289ms/step - loss: 1176.9280 - CHIPNexus.OCT4.logcount_loss: 73.5487 - CHIPNexus.OCT4.profile_loss: 1176.9268 - val_loss: 1923.0505 - val_CHIPNexus.OCT4.logcount_loss: 65.5369 - val_CHIPNexus.OCT4.profile_loss: 1139.7124\n",
      "Epoch 14/200\n",
      "276/276 [==============================] - 80s 290ms/step - loss: 1162.3775 - CHIPNexus.OCT4.logcount_loss: 61.0475 - CHIPNexus.OCT4.profile_loss: 1162.3776 - val_loss: 1949.9303 - val_CHIPNexus.OCT4.logcount_loss: 50.1841 - val_CHIPNexus.OCT4.profile_loss: 1167.4814\n",
      "Epoch 15/200\n",
      "276/276 [==============================] - 81s 292ms/step - loss: 1157.2944 - CHIPNexus.OCT4.logcount_loss: 51.8959 - CHIPNexus.OCT4.profile_loss: 1157.2939 - val_loss: 1903.0679 - val_CHIPNexus.OCT4.logcount_loss: 51.8408 - val_CHIPNexus.OCT4.profile_loss: 1141.2863\n",
      "Epoch 16/200\n",
      "276/276 [==============================] - 80s 289ms/step - loss: 1143.0166 - CHIPNexus.OCT4.logcount_loss: 55.4682 - CHIPNexus.OCT4.profile_loss: 1143.0164 - val_loss: 1882.0502 - val_CHIPNexus.OCT4.logcount_loss: 60.7694 - val_CHIPNexus.OCT4.profile_loss: 1130.6766\n",
      "Epoch 17/200\n",
      "276/276 [==============================] - 82s 299ms/step - loss: 1134.5548 - CHIPNexus.OCT4.logcount_loss: 53.9708 - CHIPNexus.OCT4.profile_loss: 1134.5552 - val_loss: 1888.6616 - val_CHIPNexus.OCT4.logcount_loss: 41.8704 - val_CHIPNexus.OCT4.profile_loss: 1129.4232\n",
      "val_loss 1849.385009765625\n",
      "val_CHIPNexus.OCT4.logcount_loss 39.368953704833984\n",
      "val_CHIPNexus.OCT4.profile_loss 1191.9093017578125\n",
      "loss 1266.8109590757324\n",
      "CHIPNexus.OCT4.logcount_loss 45.044403\n",
      "CHIPNexus.OCT4.profile_loss 1266.7468\n",
      "Seed: 1634\n",
      "LOG SPACE PREDS IS SET TO True\n",
      "Oct4only_ctaskweight125_seed1634.h5\n",
      "Model: \"model_26\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "sequence (InputLayer)           (None, 1346, 4)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_230 (Conv1D)             (None, 1326, 64)     5440        sequence[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_163 (Lambda)             (None, 1322, 64)     0           conv1d_230[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_231 (Conv1D)             (None, 1322, 64)     12352       conv1d_230[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_163 (Add)                   (None, 1322, 64)     0           lambda_163[0][0]                 \n",
      "                                                                 conv1d_231[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_164 (Lambda)             (None, 1314, 64)     0           add_163[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_232 (Conv1D)             (None, 1314, 64)     12352       add_163[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_164 (Add)                   (None, 1314, 64)     0           lambda_164[0][0]                 \n",
      "                                                                 conv1d_232[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_165 (Lambda)             (None, 1298, 64)     0           add_164[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_233 (Conv1D)             (None, 1298, 64)     12352       add_164[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_165 (Add)                   (None, 1298, 64)     0           lambda_165[0][0]                 \n",
      "                                                                 conv1d_233[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_166 (Lambda)             (None, 1266, 64)     0           add_165[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_234 (Conv1D)             (None, 1266, 64)     12352       add_165[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_166 (Add)                   (None, 1266, 64)     0           lambda_166[0][0]                 \n",
      "                                                                 conv1d_234[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_167 (Lambda)             (None, 1202, 64)     0           add_166[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_235 (Conv1D)             (None, 1202, 64)     12352       add_166[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_167 (Add)                   (None, 1202, 64)     0           lambda_167[0][0]                 \n",
      "                                                                 conv1d_235[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_168 (Lambda)             (None, 1074, 64)     0           add_167[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_236 (Conv1D)             (None, 1074, 64)     12352       add_167[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_168 (Add)                   (None, 1074, 64)     0           lambda_168[0][0]                 \n",
      "                                                                 conv1d_236[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_28 (Gl (None, 64)           0           add_168[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "patchcap.logcount (InputLayer)  (None, 2)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_237 (Conv1D)             (None, 1000, 2)      9602        add_168[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "patchcap.profile (InputLayer)   (None, 1000, 2)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_83 (Concatenate)    (None, 66)           0           global_average_pooling1d_28[0][0]\n",
      "                                                                 patchcap.logcount[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_84 (Concatenate)    (None, 1000, 4)      0           conv1d_237[0][0]                 \n",
      "                                                                 patchcap.profile[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "CHIPNexus.OCT4.logcount (Dense) (None, 2)            134         concatenate_83[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "CHIPNexus.OCT4.profile (Conv1D) (None, 1000, 2)      10          concatenate_84[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 89,298\n",
      "Trainable params: 89,298\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "276/276 [==============================] - 79s 286ms/step - loss: 1512.1327 - CHIPNexus.OCT4.logcount_loss: 0.5542 - CHIPNexus.OCT4.profile_loss: 1442.8606 - val_loss: 2193.5867 - val_CHIPNexus.OCT4.logcount_loss: 0.3551 - val_CHIPNexus.OCT4.profile_loss: 1318.6167\n",
      "Epoch 2/200\n",
      "276/276 [==============================] - 81s 293ms/step - loss: 1422.2976 - CHIPNexus.OCT4.logcount_loss: 0.3126 - CHIPNexus.OCT4.profile_loss: 1383.2205 - val_loss: 2173.4814 - val_CHIPNexus.OCT4.logcount_loss: 0.2979 - val_CHIPNexus.OCT4.profile_loss: 1286.2012\n",
      "Epoch 3/200\n",
      "276/276 [==============================] - 85s 309ms/step - loss: 1384.1115 - CHIPNexus.OCT4.logcount_loss: 0.3051 - CHIPNexus.OCT4.profile_loss: 1345.9780 - val_loss: 2114.6353 - val_CHIPNexus.OCT4.logcount_loss: 0.2910 - val_CHIPNexus.OCT4.profile_loss: 1263.9952\n",
      "Epoch 4/200\n",
      "276/276 [==============================] - 85s 308ms/step - loss: 1359.2401 - CHIPNexus.OCT4.logcount_loss: 0.3123 - CHIPNexus.OCT4.profile_loss: 1320.2001 - val_loss: 2030.7175 - val_CHIPNexus.OCT4.logcount_loss: 0.3154 - val_CHIPNexus.OCT4.profile_loss: 1238.8058\n",
      "Epoch 5/200\n",
      "276/276 [==============================] - 87s 315ms/step - loss: 1336.5466 - CHIPNexus.OCT4.logcount_loss: 0.3036 - CHIPNexus.OCT4.profile_loss: 1298.5913 - val_loss: 2034.9858 - val_CHIPNexus.OCT4.logcount_loss: 0.2804 - val_CHIPNexus.OCT4.profile_loss: 1226.2555\n",
      "Epoch 6/200\n",
      "276/276 [==============================] - 86s 313ms/step - loss: 1319.2965 - CHIPNexus.OCT4.logcount_loss: 0.3026 - CHIPNexus.OCT4.profile_loss: 1281.4695 - val_loss: 2044.0043 - val_CHIPNexus.OCT4.logcount_loss: 0.3116 - val_CHIPNexus.OCT4.profile_loss: 1211.6465\n",
      "Epoch 7/200\n",
      "  3/276 [..............................] - ETA: 32s - loss: 1001.8128 - CHIPNexus.OCT4.logcount_loss: 0.2616 - CHIPNexus.OCT4.profile_loss: 969.1069"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/avanti/anaconda3/envs/py376/lib/python3.7/site-packages/keras/callbacks/callbacks.py:95: RuntimeWarning: Method (on_train_batch_end) is slow compared to the batch update (0.111291). Check your callbacks.\n",
      "  % (hook_name, delta_t_median), RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "276/276 [==============================] - 84s 306ms/step - loss: 1299.1876 - CHIPNexus.OCT4.logcount_loss: 0.2885 - CHIPNexus.OCT4.profile_loss: 1263.1263 - val_loss: 2068.0183 - val_CHIPNexus.OCT4.logcount_loss: 0.2839 - val_CHIPNexus.OCT4.profile_loss: 1220.6265\n",
      "Epoch 8/200\n",
      "276/276 [==============================] - 82s 296ms/step - loss: 1287.7746 - CHIPNexus.OCT4.logcount_loss: 0.2974 - CHIPNexus.OCT4.profile_loss: 1250.5962 - val_loss: 2034.9635 - val_CHIPNexus.OCT4.logcount_loss: 0.2735 - val_CHIPNexus.OCT4.profile_loss: 1189.6903\n",
      "Epoch 9/200\n",
      "276/276 [==============================] - 84s 304ms/step - loss: 1269.4435 - CHIPNexus.OCT4.logcount_loss: 0.2927 - CHIPNexus.OCT4.profile_loss: 1232.8607 - val_loss: 2025.4307 - val_CHIPNexus.OCT4.logcount_loss: 0.2899 - val_CHIPNexus.OCT4.profile_loss: 1178.9814ss: 1298.3440 - CHIPNexus.O\n",
      "Epoch 10/200\n",
      "276/276 [==============================] - 83s 300ms/step - loss: 1255.2186 - CHIPNexus.OCT4.logcount_loss: 0.2770 - CHIPNexus.OCT4.profile_loss: 1220.5897 - val_loss: 2025.7665 - val_CHIPNexus.OCT4.logcount_loss: 0.2648 - val_CHIPNexus.OCT4.profile_loss: 1177.2462\n",
      "Epoch 11/200\n",
      "276/276 [==============================] - 81s 293ms/step - loss: 1242.5128 - CHIPNexus.OCT4.logcount_loss: 0.2972 - CHIPNexus.OCT4.profile_loss: 1205.3594 - val_loss: 1967.2710 - val_CHIPNexus.OCT4.logcount_loss: 0.2735 - val_CHIPNexus.OCT4.profile_loss: 1155.9111\n",
      "Epoch 12/200\n",
      "276/276 [==============================] - 83s 299ms/step - loss: 1222.4550 - CHIPNexus.OCT4.logcount_loss: 0.2786 - CHIPNexus.OCT4.profile_loss: 1187.6273 - val_loss: 2010.7446 - val_CHIPNexus.OCT4.logcount_loss: 0.2672 - val_CHIPNexus.OCT4.profile_loss: 1177.1124\n",
      "Epoch 13/200\n",
      "276/276 [==============================] - 82s 299ms/step - loss: 1208.6732 - CHIPNexus.OCT4.logcount_loss: 0.2787 - CHIPNexus.OCT4.profile_loss: 1173.8314 - val_loss: 1999.9410 - val_CHIPNexus.OCT4.logcount_loss: 0.2552 - val_CHIPNexus.OCT4.profile_loss: 1153.1904\n",
      "Epoch 14/200\n",
      "276/276 [==============================] - 84s 305ms/step - loss: 1186.2303 - CHIPNexus.OCT4.logcount_loss: 0.2779 - CHIPNexus.OCT4.profile_loss: 1151.4948 - val_loss: 2029.8641 - val_CHIPNexus.OCT4.logcount_loss: 0.2960 - val_CHIPNexus.OCT4.profile_loss: 1153.8160\n",
      "Epoch 15/200\n",
      "276/276 [==============================] - 82s 297ms/step - loss: 1169.6170 - CHIPNexus.OCT4.logcount_loss: 0.2923 - CHIPNexus.OCT4.profile_loss: 1133.0836 - val_loss: 1970.0160 - val_CHIPNexus.OCT4.logcount_loss: 0.2591 - val_CHIPNexus.OCT4.profile_loss: 1118.8878\n",
      "Epoch 16/200\n",
      "276/276 [==============================] - 80s 291ms/step - loss: 1156.9587 - CHIPNexus.OCT4.logcount_loss: 0.2963 - CHIPNexus.OCT4.profile_loss: 1119.9247 - val_loss: 1961.0820 - val_CHIPNexus.OCT4.logcount_loss: 0.2575 - val_CHIPNexus.OCT4.profile_loss: 1117.0316\n",
      "Epoch 17/200\n",
      "276/276 [==============================] - 81s 293ms/step - loss: 1131.1338 - CHIPNexus.OCT4.logcount_loss: 0.2681 - CHIPNexus.OCT4.profile_loss: 1097.6202 - val_loss: 1963.6895 - val_CHIPNexus.OCT4.logcount_loss: 0.2639 - val_CHIPNexus.OCT4.profile_loss: 1107.8816\n",
      "Epoch 18/200\n",
      "276/276 [==============================] - 87s 314ms/step - loss: 1168.6964 - CHIPNexus.OCT4.logcount_loss: 0.3005 - CHIPNexus.OCT4.profile_loss: 1131.1312 - val_loss: 2037.1101 - val_CHIPNexus.OCT4.logcount_loss: 0.2628 - val_CHIPNexus.OCT4.profile_loss: 1118.4138\n",
      "Epoch 19/200\n",
      "276/276 [==============================] - 82s 298ms/step - loss: 1134.2330 - CHIPNexus.OCT4.logcount_loss: 0.2736 - CHIPNexus.OCT4.profile_loss: 1100.0317 - val_loss: 1951.3521 - val_CHIPNexus.OCT4.logcount_loss: 0.2612 - val_CHIPNexus.OCT4.profile_loss: 1106.2935\n",
      "Epoch 20/200\n",
      "276/276 [==============================] - 84s 304ms/step - loss: 1132.6983 - CHIPNexus.OCT4.logcount_loss: 0.2694 - CHIPNexus.OCT4.profile_loss: 1099.0186 - val_loss: 1968.1448 - val_CHIPNexus.OCT4.logcount_loss: 0.2564 - val_CHIPNexus.OCT4.profile_loss: 1105.0317\n",
      "Epoch 21/200\n",
      "276/276 [==============================] - 89s 322ms/step - loss: 1122.0328 - CHIPNexus.OCT4.logcount_loss: 0.2702 - CHIPNexus.OCT4.profile_loss: 1088.2607 - val_loss: 1927.8611 - val_CHIPNexus.OCT4.logcount_loss: 0.2536 - val_CHIPNexus.OCT4.profile_loss: 1100.7679\n",
      "Epoch 22/200\n",
      "276/276 [==============================] - 82s 298ms/step - loss: 1111.9660 - CHIPNexus.OCT4.logcount_loss: 0.2665 - CHIPNexus.OCT4.profile_loss: 1078.6576 - val_loss: 1942.1802 - val_CHIPNexus.OCT4.logcount_loss: 0.2520 - val_CHIPNexus.OCT4.profile_loss: 1093.0568\n",
      "Epoch 23/200\n",
      "276/276 [==============================] - 80s 289ms/step - loss: 1092.0859 - CHIPNexus.OCT4.logcount_loss: 0.2588 - CHIPNexus.OCT4.profile_loss: 1059.7415 - val_loss: 1928.5483 - val_CHIPNexus.OCT4.logcount_loss: 0.2520 - val_CHIPNexus.OCT4.profile_loss: 1092.4806\n",
      "Epoch 24/200\n",
      "276/276 [==============================] - 81s 294ms/step - loss: 1083.8854 - CHIPNexus.OCT4.logcount_loss: 0.2579 - CHIPNexus.OCT4.profile_loss: 1051.6443 - val_loss: 1953.6678 - val_CHIPNexus.OCT4.logcount_loss: 0.2471 - val_CHIPNexus.OCT4.profile_loss: 1091.3278Nexus.OCT4.\n",
      "Epoch 25/200\n",
      "276/276 [==============================] - 82s 295ms/step - loss: 1116.0958 - CHIPNexus.OCT4.logcount_loss: 0.2609 - CHIPNexus.OCT4.profile_loss: 1083.4799 - val_loss: 1927.8546 - val_CHIPNexus.OCT4.logcount_loss: 0.2822 - val_CHIPNexus.OCT4.profile_loss: 1100.6218\n",
      "Epoch 26/200\n",
      "276/276 [==============================] - 80s 290ms/step - loss: 1090.6174 - CHIPNexus.OCT4.logcount_loss: 0.2608 - CHIPNexus.OCT4.profile_loss: 1058.0114 - val_loss: 1927.2035 - val_CHIPNexus.OCT4.logcount_loss: 0.2613 - val_CHIPNexus.OCT4.profile_loss: 1084.9749\n",
      "Epoch 27/200\n",
      "276/276 [==============================] - 80s 288ms/step - loss: 1081.7349 - CHIPNexus.OCT4.logcount_loss: 0.2729 - CHIPNexus.OCT4.profile_loss: 1047.6229 - val_loss: 2064.6348 - val_CHIPNexus.OCT4.logcount_loss: 0.2674 - val_CHIPNexus.OCT4.profile_loss: 1135.3969\n",
      "Epoch 28/200\n",
      "276/276 [==============================] - 88s 318ms/step - loss: 1106.3505 - CHIPNexus.OCT4.logcount_loss: 0.2698 - CHIPNexus.OCT4.profile_loss: 1072.6210 - val_loss: 2069.8037 - val_CHIPNexus.OCT4.logcount_loss: 0.2679 - val_CHIPNexus.OCT4.profile_loss: 1109.9150\n",
      "Epoch 29/200\n",
      "276/276 [==============================] - 82s 297ms/step - loss: 1152.4654 - CHIPNexus.OCT4.logcount_loss: 0.2757 - CHIPNexus.OCT4.profile_loss: 1118.0049 - val_loss: 1939.6261 - val_CHIPNexus.OCT4.logcount_loss: 0.2585 - val_CHIPNexus.OCT4.profile_loss: 1091.3961\n",
      "Epoch 30/200\n",
      "276/276 [==============================] - 81s 295ms/step - loss: 1103.1731 - CHIPNexus.OCT4.logcount_loss: 0.2682 - CHIPNexus.OCT4.profile_loss: 1069.6473 - val_loss: 1958.5762 - val_CHIPNexus.OCT4.logcount_loss: 0.2506 - val_CHIPNexus.OCT4.profile_loss: 1088.8635\n",
      "Epoch 31/200\n",
      "276/276 [==============================] - 83s 300ms/step - loss: 1074.3142 - CHIPNexus.OCT4.logcount_loss: 0.2580 - CHIPNexus.OCT4.profile_loss: 1042.0664 - val_loss: 1930.8173 - val_CHIPNexus.OCT4.logcount_loss: 0.2477 - val_CHIPNexus.OCT4.profile_loss: 1085.2482\n",
      "Epoch 32/200\n",
      "276/276 [==============================] - 82s 296ms/step - loss: 1067.4155 - CHIPNexus.OCT4.logcount_loss: 0.2536 - CHIPNexus.OCT4.profile_loss: 1035.7189 - val_loss: 1927.5829 - val_CHIPNexus.OCT4.logcount_loss: 0.2456 - val_CHIPNexus.OCT4.profile_loss: 1078.5104\n",
      "Epoch 33/200\n",
      "276/276 [==============================] - 81s 295ms/step - loss: 1063.8188 - CHIPNexus.OCT4.logcount_loss: 0.2504 - CHIPNexus.OCT4.profile_loss: 1032.5225 - val_loss: 1925.3635 - val_CHIPNexus.OCT4.logcount_loss: 0.2534 - val_CHIPNexus.OCT4.profile_loss: 1076.4645\n",
      "Epoch 34/200\n",
      "276/276 [==============================] - 84s 304ms/step - loss: 1061.2485 - CHIPNexus.OCT4.logcount_loss: 0.2487 - CHIPNexus.OCT4.profile_loss: 1030.1633 - val_loss: 1932.3910 - val_CHIPNexus.OCT4.logcount_loss: 0.2366 - val_CHIPNexus.OCT4.profile_loss: 1075.0785\n",
      "Epoch 35/200\n",
      "276/276 [==============================] - 82s 298ms/step - loss: 1066.8282 - CHIPNexus.OCT4.logcount_loss: 0.2463 - CHIPNexus.OCT4.profile_loss: 1036.0360 - val_loss: 1905.8169 - val_CHIPNexus.OCT4.logcount_loss: 0.2507 - val_CHIPNexus.OCT4.profile_loss: 1069.4319\n",
      "Epoch 36/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "276/276 [==============================] - 81s 292ms/step - loss: 1058.7881 - CHIPNexus.OCT4.logcount_loss: 0.2478 - CHIPNexus.OCT4.profile_loss: 1027.8098 - val_loss: 1939.5148 - val_CHIPNexus.OCT4.logcount_loss: 0.2496 - val_CHIPNexus.OCT4.profile_loss: 1070.3367\n",
      "Epoch 37/200\n",
      "276/276 [==============================] - 82s 297ms/step - loss: 1056.9933 - CHIPNexus.OCT4.logcount_loss: 0.2423 - CHIPNexus.OCT4.profile_loss: 1026.7018 - val_loss: 1950.1565 - val_CHIPNexus.OCT4.logcount_loss: 0.2549 - val_CHIPNexus.OCT4.profile_loss: 1071.4789\n",
      "Epoch 38/200\n",
      "276/276 [==============================] - 85s 308ms/step - loss: 1068.5817 - CHIPNexus.OCT4.logcount_loss: 0.2387 - CHIPNexus.OCT4.profile_loss: 1038.7504 - val_loss: 1897.5316 - val_CHIPNexus.OCT4.logcount_loss: 0.2780 - val_CHIPNexus.OCT4.profile_loss: 1071.7305\n",
      "Epoch 39/200\n",
      "276/276 [==============================] - 80s 291ms/step - loss: 1055.9207 - CHIPNexus.OCT4.logcount_loss: 0.2375 - CHIPNexus.OCT4.profile_loss: 1026.2302 - val_loss: 1928.8118 - val_CHIPNexus.OCT4.logcount_loss: 0.2348 - val_CHIPNexus.OCT4.profile_loss: 1067.2494\n",
      "Epoch 40/200\n",
      "276/276 [==============================] - 81s 294ms/step - loss: 1055.1895 - CHIPNexus.OCT4.logcount_loss: 0.2377 - CHIPNexus.OCT4.profile_loss: 1025.4764 - val_loss: 1913.4277 - val_CHIPNexus.OCT4.logcount_loss: 0.2348 - val_CHIPNexus.OCT4.profile_loss: 1059.2125\n",
      "Epoch 41/200\n",
      "276/276 [==============================] - 82s 296ms/step - loss: 1043.9226 - CHIPNexus.OCT4.logcount_loss: 0.2280 - CHIPNexus.OCT4.profile_loss: 1015.4228 - val_loss: 1929.4161 - val_CHIPNexus.OCT4.logcount_loss: 0.2272 - val_CHIPNexus.OCT4.profile_loss: 1060.4279\n",
      "Epoch 42/200\n",
      "276/276 [==============================] - 82s 297ms/step - loss: 1044.3054 - CHIPNexus.OCT4.logcount_loss: 0.2293 - CHIPNexus.OCT4.profile_loss: 1015.6450 - val_loss: 1926.1677 - val_CHIPNexus.OCT4.logcount_loss: 0.2274 - val_CHIPNexus.OCT4.profile_loss: 1059.3530\n",
      "Epoch 43/200\n",
      "276/276 [==============================] - 82s 296ms/step - loss: 1048.2206 - CHIPNexus.OCT4.logcount_loss: 0.2266 - CHIPNexus.OCT4.profile_loss: 1019.8988 - val_loss: 1920.2642 - val_CHIPNexus.OCT4.logcount_loss: 0.2274 - val_CHIPNexus.OCT4.profile_loss: 1058.0461\n",
      "Epoch 44/200\n",
      "276/276 [==============================] - 81s 292ms/step - loss: 1042.5936 - CHIPNexus.OCT4.logcount_loss: 0.2235 - CHIPNexus.OCT4.profile_loss: 1014.6552 - val_loss: 1931.4514 - val_CHIPNexus.OCT4.logcount_loss: 0.2257 - val_CHIPNexus.OCT4.profile_loss: 1056.6260\n",
      "Epoch 45/200\n",
      "276/276 [==============================] - 81s 295ms/step - loss: 1048.5503 - CHIPNexus.OCT4.logcount_loss: 0.2206 - CHIPNexus.OCT4.profile_loss: 1020.9756 - val_loss: 1918.2736 - val_CHIPNexus.OCT4.logcount_loss: 0.2253 - val_CHIPNexus.OCT4.profile_loss: 1058.1160\n",
      "Epoch 46/200\n",
      "276/276 [==============================] - 83s 302ms/step - loss: 1040.6736 - CHIPNexus.OCT4.logcount_loss: 0.2218 - CHIPNexus.OCT4.profile_loss: 1012.9514 - val_loss: 1911.0771 - val_CHIPNexus.OCT4.logcount_loss: 0.2226 - val_CHIPNexus.OCT4.profile_loss: 1055.1913\n",
      "Epoch 47/200\n",
      "276/276 [==============================] - 85s 307ms/step - loss: 1044.9836 - CHIPNexus.OCT4.logcount_loss: 0.2220 - CHIPNexus.OCT4.profile_loss: 1017.2348 - val_loss: 1915.0854 - val_CHIPNexus.OCT4.logcount_loss: 0.2216 - val_CHIPNexus.OCT4.profile_loss: 1061.8258\n",
      "Epoch 48/200\n",
      "276/276 [==============================] - 82s 295ms/step - loss: 1047.9442 - CHIPNexus.OCT4.logcount_loss: 0.2197 - CHIPNexus.OCT4.profile_loss: 1020.4878 - val_loss: 1911.4043 - val_CHIPNexus.OCT4.logcount_loss: 0.2240 - val_CHIPNexus.OCT4.profile_loss: 1058.5018\n",
      "val_loss 1897.5316162109375\n",
      "val_CHIPNexus.OCT4.logcount_loss 0.27803611755371094\n",
      "val_CHIPNexus.OCT4.profile_loss 1071.73046875\n",
      "loss 1068.5619165040446\n",
      "CHIPNexus.OCT4.logcount_loss 0.23865438\n",
      "CHIPNexus.OCT4.profile_loss 1038.7504\n",
      "Seed: 1634\n",
      "LOG SPACE PREDS IS SET TO True\n",
      "Oct4only_ctaskweight250_seed1634.h5\n",
      "Model: \"model_27\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "sequence (InputLayer)           (None, 1346, 4)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_238 (Conv1D)             (None, 1326, 64)     5440        sequence[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_169 (Lambda)             (None, 1322, 64)     0           conv1d_238[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_239 (Conv1D)             (None, 1322, 64)     12352       conv1d_238[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_169 (Add)                   (None, 1322, 64)     0           lambda_169[0][0]                 \n",
      "                                                                 conv1d_239[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_170 (Lambda)             (None, 1314, 64)     0           add_169[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_240 (Conv1D)             (None, 1314, 64)     12352       add_169[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_170 (Add)                   (None, 1314, 64)     0           lambda_170[0][0]                 \n",
      "                                                                 conv1d_240[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_171 (Lambda)             (None, 1298, 64)     0           add_170[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_241 (Conv1D)             (None, 1298, 64)     12352       add_170[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_171 (Add)                   (None, 1298, 64)     0           lambda_171[0][0]                 \n",
      "                                                                 conv1d_241[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_172 (Lambda)             (None, 1266, 64)     0           add_171[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_242 (Conv1D)             (None, 1266, 64)     12352       add_171[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_172 (Add)                   (None, 1266, 64)     0           lambda_172[0][0]                 \n",
      "                                                                 conv1d_242[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_173 (Lambda)             (None, 1202, 64)     0           add_172[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_243 (Conv1D)             (None, 1202, 64)     12352       add_172[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_173 (Add)                   (None, 1202, 64)     0           lambda_173[0][0]                 \n",
      "                                                                 conv1d_243[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_174 (Lambda)             (None, 1074, 64)     0           add_173[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_244 (Conv1D)             (None, 1074, 64)     12352       add_173[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_174 (Add)                   (None, 1074, 64)     0           lambda_174[0][0]                 \n",
      "                                                                 conv1d_244[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_29 (Gl (None, 64)           0           add_174[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "patchcap.logcount (InputLayer)  (None, 2)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_245 (Conv1D)             (None, 1000, 2)      9602        add_174[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "patchcap.profile (InputLayer)   (None, 1000, 2)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_85 (Concatenate)    (None, 66)           0           global_average_pooling1d_29[0][0]\n",
      "                                                                 patchcap.logcount[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_86 (Concatenate)    (None, 1000, 4)      0           conv1d_245[0][0]                 \n",
      "                                                                 patchcap.profile[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "CHIPNexus.OCT4.logcount (Dense) (None, 2)            134         concatenate_85[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "CHIPNexus.OCT4.profile (Conv1D) (None, 1000, 2)      10          concatenate_86[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 89,298\n",
      "Trainable params: 89,298\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "276/276 [==============================] - 82s 298ms/step - loss: 1619.4161 - CHIPNexus.OCT4.logcount_loss: 0.5995 - CHIPNexus.OCT4.profile_loss: 1469.5376 - val_loss: 2350.9490 - val_CHIPNexus.OCT4.logcount_loss: 0.3083 - val_CHIPNexus.OCT4.profile_loss: 1317.4852\n",
      "Epoch 2/200\n",
      "276/276 [==============================] - 80s 290ms/step - loss: 1497.3391 - CHIPNexus.OCT4.logcount_loss: 0.3056 - CHIPNexus.OCT4.profile_loss: 1420.9456 - val_loss: 2390.9666 - val_CHIPNexus.OCT4.logcount_loss: 0.3175 - val_CHIPNexus.OCT4.profile_loss: 1288.4974\n",
      "Epoch 3/200\n",
      "276/276 [==============================] - 81s 293ms/step - loss: 1455.6223 - CHIPNexus.OCT4.logcount_loss: 0.3094 - CHIPNexus.OCT4.profile_loss: 1378.2698 - val_loss: 2266.1763 - val_CHIPNexus.OCT4.logcount_loss: 0.2954 - val_CHIPNexus.OCT4.profile_loss: 1258.3837\n",
      "Epoch 4/200\n",
      "276/276 [==============================] - 82s 297ms/step - loss: 1423.6196 - CHIPNexus.OCT4.logcount_loss: 0.3038 - CHIPNexus.OCT4.profile_loss: 1347.6754 - val_loss: 2186.3032 - val_CHIPNexus.OCT4.logcount_loss: 0.2918 - val_CHIPNexus.OCT4.profile_loss: 1243.0989\n",
      "Epoch 5/200\n",
      "276/276 [==============================] - 82s 296ms/step - loss: 1402.2805 - CHIPNexus.OCT4.logcount_loss: 0.3018 - CHIPNexus.OCT4.profile_loss: 1326.8378 - val_loss: 2151.5752 - val_CHIPNexus.OCT4.logcount_loss: 0.2853 - val_CHIPNexus.OCT4.profile_loss: 1218.5312\n",
      "Epoch 6/200\n",
      "276/276 [==============================] - 81s 295ms/step - loss: 1382.2467 - CHIPNexus.OCT4.logcount_loss: 0.3030 - CHIPNexus.OCT4.profile_loss: 1306.4958 - val_loss: 2240.1277 - val_CHIPNexus.OCT4.logcount_loss: 0.3118 - val_CHIPNexus.OCT4.profile_loss: 1205.2159\n",
      "Epoch 7/200\n",
      "276/276 [==============================] - 83s 301ms/step - loss: 1364.1275 - CHIPNexus.OCT4.logcount_loss: 0.2951 - CHIPNexus.OCT4.profile_loss: 1290.3466 - val_loss: 2102.5898 - val_CHIPNexus.OCT4.logcount_loss: 0.2978 - val_CHIPNexus.OCT4.profile_loss: 1190.3311\n",
      "Epoch 8/200\n",
      "276/276 [==============================] - 86s 310ms/step - loss: 1348.2012 - CHIPNexus.OCT4.logcount_loss: 0.2987 - CHIPNexus.OCT4.profile_loss: 1273.5143 - val_loss: 2181.9517 - val_CHIPNexus.OCT4.logcount_loss: 0.2752 - val_CHIPNexus.OCT4.profile_loss: 1180.4209\n",
      "Epoch 9/200\n",
      "276/276 [==============================] - 82s 296ms/step - loss: 1329.6020 - CHIPNexus.OCT4.logcount_loss: 0.2920 - CHIPNexus.OCT4.profile_loss: 1256.5966 - val_loss: 2090.4521 - val_CHIPNexus.OCT4.logcount_loss: 0.2764 - val_CHIPNexus.OCT4.profile_loss: 1166.5487\n",
      "Epoch 10/200\n",
      "276/276 [==============================] - 80s 291ms/step - loss: 1313.4501 - CHIPNexus.OCT4.logcount_loss: 0.2808 - CHIPNexus.OCT4.profile_loss: 1243.2395 - val_loss: 2061.0996 - val_CHIPNexus.OCT4.logcount_loss: 0.2742 - val_CHIPNexus.OCT4.profile_loss: 1164.7308\n",
      "Epoch 11/200\n",
      "276/276 [==============================] - 82s 299ms/step - loss: 1297.7516 - CHIPNexus.OCT4.logcount_loss: 0.2770 - CHIPNexus.OCT4.profile_loss: 1228.4966 - val_loss: 2064.6528 - val_CHIPNexus.OCT4.logcount_loss: 0.2648 - val_CHIPNexus.OCT4.profile_loss: 1149.8479\n",
      "Epoch 12/200\n",
      "276/276 [==============================] - 81s 295ms/step - loss: 1281.9024 - CHIPNexus.OCT4.logcount_loss: 0.2783 - CHIPNexus.OCT4.profile_loss: 1212.3375 - val_loss: 2146.0583 - val_CHIPNexus.OCT4.logcount_loss: 0.2958 - val_CHIPNexus.OCT4.profile_loss: 1144.2810\n",
      "Epoch 13/200\n",
      "276/276 [==============================] - 84s 304ms/step - loss: 1267.4073 - CHIPNexus.OCT4.logcount_loss: 0.2685 - CHIPNexus.OCT4.profile_loss: 1200.2933 - val_loss: 2037.7606 - val_CHIPNexus.OCT4.logcount_loss: 0.2556 - val_CHIPNexus.OCT4.profile_loss: 1137.1322\n",
      "Epoch 14/200\n",
      "276/276 [==============================] - 81s 293ms/step - loss: 1244.3171 - CHIPNexus.OCT4.logcount_loss: 0.2611 - CHIPNexus.OCT4.profile_loss: 1179.0411 - val_loss: 1982.8657 - val_CHIPNexus.OCT4.logcount_loss: 0.2764 - val_CHIPNexus.OCT4.profile_loss: 1131.5743\n",
      "Epoch 15/200\n",
      "276/276 [==============================] - 80s 292ms/step - loss: 1237.1153 - CHIPNexus.OCT4.logcount_loss: 0.2640 - CHIPNexus.OCT4.profile_loss: 1171.1042 - val_loss: 2061.2690 - val_CHIPNexus.OCT4.logcount_loss: 0.2638 - val_CHIPNexus.OCT4.profile_loss: 1139.8317\n",
      "Epoch 16/200\n",
      "276/276 [==============================] - 83s 300ms/step - loss: 1218.6692 - CHIPNexus.OCT4.logcount_loss: 0.2676 - CHIPNexus.OCT4.profile_loss: 1151.7789 - val_loss: 2078.4504 - val_CHIPNexus.OCT4.logcount_loss: 0.2494 - val_CHIPNexus.OCT4.profile_loss: 1132.1000\n",
      "Epoch 17/200\n",
      "276/276 [==============================] - 80s 291ms/step - loss: 1195.1404 - CHIPNexus.OCT4.logcount_loss: 0.2621 - CHIPNexus.OCT4.profile_loss: 1129.6204 - val_loss: 2046.2142 - val_CHIPNexus.OCT4.logcount_loss: 0.2475 - val_CHIPNexus.OCT4.profile_loss: 1115.1163\n",
      "Epoch 18/200\n",
      "276/276 [==============================] - 82s 296ms/step - loss: 1189.1326 - CHIPNexus.OCT4.logcount_loss: 0.2550 - CHIPNexus.OCT4.profile_loss: 1125.3844 - val_loss: 2459.5889 - val_CHIPNexus.OCT4.logcount_loss: 0.6007 - val_CHIPNexus.OCT4.profile_loss: 1182.1484\n",
      "Epoch 19/200\n",
      "276/276 [==============================] - 82s 298ms/step - loss: 1189.2712 - CHIPNexus.OCT4.logcount_loss: 0.2713 - CHIPNexus.OCT4.profile_loss: 1121.4479 - val_loss: 2147.5225 - val_CHIPNexus.OCT4.logcount_loss: 0.2732 - val_CHIPNexus.OCT4.profile_loss: 1119.9567\n",
      "Epoch 20/200\n",
      "276/276 [==============================] - 81s 293ms/step - loss: 1224.7711 - CHIPNexus.OCT4.logcount_loss: 0.2665 - CHIPNexus.OCT4.profile_loss: 1158.1506 - val_loss: 2050.8291 - val_CHIPNexus.OCT4.logcount_loss: 0.2473 - val_CHIPNexus.OCT4.profile_loss: 1111.6771\n",
      "Epoch 21/200\n",
      "276/276 [==============================] - 82s 298ms/step - loss: 1185.4195 - CHIPNexus.OCT4.logcount_loss: 0.2543 - CHIPNexus.OCT4.profile_loss: 1121.8333 - val_loss: 2050.0044 - val_CHIPNexus.OCT4.logcount_loss: 0.2500 - val_CHIPNexus.OCT4.profile_loss: 1110.3027\n",
      "Epoch 22/200\n",
      "276/276 [==============================] - 81s 293ms/step - loss: 1147.0850 - CHIPNexus.OCT4.logcount_loss: 0.2465 - CHIPNexus.OCT4.profile_loss: 1085.4529 - val_loss: 2053.1472 - val_CHIPNexus.OCT4.logcount_loss: 0.2405 - val_CHIPNexus.OCT4.profile_loss: 1107.9039\n",
      "Epoch 23/200\n",
      "276/276 [==============================] - 81s 293ms/step - loss: 1150.4821 - CHIPNexus.OCT4.logcount_loss: 0.2434 - CHIPNexus.OCT4.profile_loss: 1089.6350 - val_loss: 2176.0205 - val_CHIPNexus.OCT4.logcount_loss: 0.2653 - val_CHIPNexus.OCT4.profile_loss: 1149.1570\n",
      "Epoch 24/200\n",
      "276/276 [==============================] - 82s 299ms/step - loss: 1155.4564 - CHIPNexus.OCT4.logcount_loss: 0.2594 - CHIPNexus.OCT4.profile_loss: 1090.6167 - val_loss: 2100.2129 - val_CHIPNexus.OCT4.logcount_loss: 0.2494 - val_CHIPNexus.OCT4.profile_loss: 1100.4697\n",
      "val_loss 1982.86572265625\n",
      "val_CHIPNexus.OCT4.logcount_loss 0.27644625306129456\n",
      "val_CHIPNexus.OCT4.profile_loss 1131.5743408203125\n",
      "loss 1244.3787115278394\n",
      "CHIPNexus.OCT4.logcount_loss 0.26110256\n",
      "CHIPNexus.OCT4.profile_loss 1179.0411\n",
      "Seed: 1634\n",
      "LOG SPACE PREDS IS SET TO True\n",
      "Oct4only_ctaskweight500_seed1634.h5\n",
      "Model: \"model_28\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "sequence (InputLayer)           (None, 1346, 4)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_246 (Conv1D)             (None, 1326, 64)     5440        sequence[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_175 (Lambda)             (None, 1322, 64)     0           conv1d_246[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_247 (Conv1D)             (None, 1322, 64)     12352       conv1d_246[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_175 (Add)                   (None, 1322, 64)     0           lambda_175[0][0]                 \n",
      "                                                                 conv1d_247[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_176 (Lambda)             (None, 1314, 64)     0           add_175[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_248 (Conv1D)             (None, 1314, 64)     12352       add_175[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_176 (Add)                   (None, 1314, 64)     0           lambda_176[0][0]                 \n",
      "                                                                 conv1d_248[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_177 (Lambda)             (None, 1298, 64)     0           add_176[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_249 (Conv1D)             (None, 1298, 64)     12352       add_176[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_177 (Add)                   (None, 1298, 64)     0           lambda_177[0][0]                 \n",
      "                                                                 conv1d_249[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_178 (Lambda)             (None, 1266, 64)     0           add_177[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_250 (Conv1D)             (None, 1266, 64)     12352       add_177[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_178 (Add)                   (None, 1266, 64)     0           lambda_178[0][0]                 \n",
      "                                                                 conv1d_250[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_179 (Lambda)             (None, 1202, 64)     0           add_178[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_251 (Conv1D)             (None, 1202, 64)     12352       add_178[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_179 (Add)                   (None, 1202, 64)     0           lambda_179[0][0]                 \n",
      "                                                                 conv1d_251[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_180 (Lambda)             (None, 1074, 64)     0           add_179[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_252 (Conv1D)             (None, 1074, 64)     12352       add_179[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_180 (Add)                   (None, 1074, 64)     0           lambda_180[0][0]                 \n",
      "                                                                 conv1d_252[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_30 (Gl (None, 64)           0           add_180[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "patchcap.logcount (InputLayer)  (None, 2)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_253 (Conv1D)             (None, 1000, 2)      9602        add_180[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "patchcap.profile (InputLayer)   (None, 1000, 2)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_87 (Concatenate)    (None, 66)           0           global_average_pooling1d_30[0][0]\n",
      "                                                                 patchcap.logcount[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_88 (Concatenate)    (None, 1000, 4)      0           conv1d_253[0][0]                 \n",
      "                                                                 patchcap.profile[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "CHIPNexus.OCT4.logcount (Dense) (None, 2)            134         concatenate_87[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "CHIPNexus.OCT4.profile (Conv1D) (None, 1000, 2)      10          concatenate_88[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 89,298\n",
      "Trainable params: 89,298\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "276/276 [==============================] - 79s 286ms/step - loss: 1747.5200 - CHIPNexus.OCT4.logcount_loss: 0.5541 - CHIPNexus.OCT4.profile_loss: 1470.4456 - val_loss: 2622.7126 - val_CHIPNexus.OCT4.logcount_loss: 0.3084 - val_CHIPNexus.OCT4.profile_loss: 1335.1970\n",
      "Epoch 2/200\n",
      "276/276 [==============================] - 79s 288ms/step - loss: 1581.5524 - CHIPNexus.OCT4.logcount_loss: 0.3105 - CHIPNexus.OCT4.profile_loss: 1426.3162 - val_loss: 2628.6506 - val_CHIPNexus.OCT4.logcount_loss: 0.2983 - val_CHIPNexus.OCT4.profile_loss: 1304.7902\n",
      "Epoch 3/200\n",
      "276/276 [==============================] - 81s 295ms/step - loss: 1549.8091 - CHIPNexus.OCT4.logcount_loss: 0.3105 - CHIPNexus.OCT4.profile_loss: 1394.5746 - val_loss: 2596.1970 - val_CHIPNexus.OCT4.logcount_loss: 0.2923 - val_CHIPNexus.OCT4.profile_loss: 1277.9990\n",
      "Epoch 4/200\n",
      "276/276 [==============================] - 82s 298ms/step - loss: 1519.0027 - CHIPNexus.OCT4.logcount_loss: 0.3150 - CHIPNexus.OCT4.profile_loss: 1361.4995 - val_loss: 2631.5803 - val_CHIPNexus.OCT4.logcount_loss: 0.2914 - val_CHIPNexus.OCT4.profile_loss: 1252.0046_loss: 0.3171 - CHIPNexus.OCT4.profile_loss: 1383.28 - ETA: 4s - loss: 1540.2247 - CHIPNexus.OCT4.logcount_loss: 0.3168 - CHIP\n",
      "Epoch 5/200\n",
      "276/276 [==============================] - 81s 294ms/step - loss: 1481.4995 - CHIPNexus.OCT4.logcount_loss: 0.2948 - CHIPNexus.OCT4.profile_loss: 1334.0992 - val_loss: 2598.4595 - val_CHIPNexus.OCT4.logcount_loss: 0.3006 - val_CHIPNexus.OCT4.profile_loss: 1233.9271 \n",
      "Epoch 6/200\n",
      "276/276 [==============================] - 83s 299ms/step - loss: 1458.9475 - CHIPNexus.OCT4.logcount_loss: 0.2934 - CHIPNexus.OCT4.profile_loss: 1312.2235 - val_loss: 2535.7290 - val_CHIPNexus.OCT4.logcount_loss: 0.2840 - val_CHIPNexus.OCT4.profile_loss: 1220.6166\n",
      "Epoch 7/200\n",
      "276/276 [==============================] - 83s 299ms/step - loss: 1447.4034 - CHIPNexus.OCT4.logcount_loss: 0.3064 - CHIPNexus.OCT4.profile_loss: 1294.1912 - val_loss: 2496.2874 - val_CHIPNexus.OCT4.logcount_loss: 0.2767 - val_CHIPNexus.OCT4.profile_loss: 1210.7118\n",
      "Epoch 8/200\n",
      "276/276 [==============================] - 81s 294ms/step - loss: 1422.6912 - CHIPNexus.OCT4.logcount_loss: 0.2907 - CHIPNexus.OCT4.profile_loss: 1277.3417 - val_loss: 2327.2563 - val_CHIPNexus.OCT4.logcount_loss: 0.2748 - val_CHIPNexus.OCT4.profile_loss: 1188.1217\n",
      "Epoch 9/200\n",
      "276/276 [==============================] - 79s 286ms/step - loss: 1405.0326 - CHIPNexus.OCT4.logcount_loss: 0.2868 - CHIPNexus.OCT4.profile_loss: 1261.6105 - val_loss: 2379.3936 - val_CHIPNexus.OCT4.logcount_loss: 0.2636 - val_CHIPNexus.OCT4.profile_loss: 1176.0122oss: 0.2857 - CHIPNexu\n",
      "Epoch 10/200\n",
      "276/276 [==============================] - 83s 299ms/step - loss: 1385.2873 - CHIPNexus.OCT4.logcount_loss: 0.2814 - CHIPNexus.OCT4.profile_loss: 1244.6025 - val_loss: 2356.8254 - val_CHIPNexus.OCT4.logcount_loss: 0.2567 - val_CHIPNexus.OCT4.profile_loss: 1165.5714\n",
      "Epoch 11/200\n",
      "276/276 [==============================] - 86s 313ms/step - loss: 1360.8028 - CHIPNexus.OCT4.logcount_loss: 0.2655 - CHIPNexus.OCT4.profile_loss: 1228.0668 - val_loss: 2482.9868 - val_CHIPNexus.OCT4.logcount_loss: 0.2709 - val_CHIPNexus.OCT4.profile_loss: 1178.8871\n",
      "Epoch 12/200\n",
      "276/276 [==============================] - 82s 297ms/step - loss: 1345.3420 - CHIPNexus.OCT4.logcount_loss: 0.2645 - CHIPNexus.OCT4.profile_loss: 1213.0789 - val_loss: 2323.5576 - val_CHIPNexus.OCT4.logcount_loss: 0.2484 - val_CHIPNexus.OCT4.profile_loss: 1147.7539\n",
      "Epoch 13/200\n",
      "276/276 [==============================] - 82s 296ms/step - loss: 1331.6966 - CHIPNexus.OCT4.logcount_loss: 0.2548 - CHIPNexus.OCT4.profile_loss: 1204.2722 - val_loss: 2449.2207 - val_CHIPNexus.OCT4.logcount_loss: 0.2871 - val_CHIPNexus.OCT4.profile_loss: 1149.2339\n",
      "Epoch 14/200\n",
      "276/276 [==============================] - 83s 300ms/step - loss: 1310.1400 - CHIPNexus.OCT4.logcount_loss: 0.2542 - CHIPNexus.OCT4.profile_loss: 1183.0197 - val_loss: 2194.7859 - val_CHIPNexus.OCT4.logcount_loss: 0.2698 - val_CHIPNexus.OCT4.profile_loss: 1149.9290\n",
      "Epoch 15/200\n",
      "276/276 [==============================] - 82s 299ms/step - loss: 1289.8452 - CHIPNexus.OCT4.logcount_loss: 0.2418 - CHIPNexus.OCT4.profile_loss: 1168.9617 - val_loss: 2267.0229 - val_CHIPNexus.OCT4.logcount_loss: 0.2403 - val_CHIPNexus.OCT4.profile_loss: 1140.9769\n",
      "Epoch 16/200\n",
      "  3/276 [..............................] - ETA: 31s - loss: 1187.2441 - CHIPNexus.OCT4.logcount_loss: 0.2579 - CHIPNexus.OCT4.profile_loss: 1058.2943"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/avanti/anaconda3/envs/py376/lib/python3.7/site-packages/keras/callbacks/callbacks.py:95: RuntimeWarning: Method (on_train_batch_end) is slow compared to the batch update (0.116986). Check your callbacks.\n",
      "  % (hook_name, delta_t_median), RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "276/276 [==============================] - 81s 295ms/step - loss: 1276.0990 - CHIPNexus.OCT4.logcount_loss: 0.2411 - CHIPNexus.OCT4.profile_loss: 1155.5459 - val_loss: 2171.9546 - val_CHIPNexus.OCT4.logcount_loss: 0.2406 - val_CHIPNexus.OCT4.profile_loss: 1135.5243\n",
      "Epoch 17/200\n",
      "276/276 [==============================] - 85s 307ms/step - loss: 1264.5469 - CHIPNexus.OCT4.logcount_loss: 0.2444 - CHIPNexus.OCT4.profile_loss: 1142.3461 - val_loss: 2188.0686 - val_CHIPNexus.OCT4.logcount_loss: 0.2389 - val_CHIPNexus.OCT4.profile_loss: 1142.1041\n",
      "Epoch 18/200\n",
      "276/276 [==============================] - 83s 300ms/step - loss: 1256.0233 - CHIPNexus.OCT4.logcount_loss: 0.2478 - CHIPNexus.OCT4.profile_loss: 1132.1346 - val_loss: 2250.9917 - val_CHIPNexus.OCT4.logcount_loss: 0.2507 - val_CHIPNexus.OCT4.profile_loss: 1120.3491\n",
      "Epoch 19/200\n",
      "276/276 [==============================] - 83s 300ms/step - loss: 1265.0334 - CHIPNexus.OCT4.logcount_loss: 0.2638 - CHIPNexus.OCT4.profile_loss: 1133.1294 - val_loss: 2263.9766 - val_CHIPNexus.OCT4.logcount_loss: 0.2576 - val_CHIPNexus.OCT4.profile_loss: 1129.2653\n",
      "Epoch 20/200\n",
      "276/276 [==============================] - 83s 301ms/step - loss: 1238.9248 - CHIPNexus.OCT4.logcount_loss: 0.2586 - CHIPNexus.OCT4.profile_loss: 1109.6437 - val_loss: 2155.0330 - val_CHIPNexus.OCT4.logcount_loss: 0.2427 - val_CHIPNexus.OCT4.profile_loss: 1108.4838\n",
      "Epoch 21/200\n",
      "276/276 [==============================] - 83s 299ms/step - loss: 1260.1744 - CHIPNexus.OCT4.logcount_loss: 0.2478 - CHIPNexus.OCT4.profile_loss: 1136.2970 - val_loss: 2245.8828 - val_CHIPNexus.OCT4.logcount_loss: 0.2513 - val_CHIPNexus.OCT4.profile_loss: 1114.1147\n",
      "Epoch 22/200\n",
      "276/276 [==============================] - 80s 290ms/step - loss: 1224.0709 - CHIPNexus.OCT4.logcount_loss: 0.2514 - CHIPNexus.OCT4.profile_loss: 1098.3799 - val_loss: 2078.8235 - val_CHIPNexus.OCT4.logcount_loss: 0.2791 - val_CHIPNexus.OCT4.profile_loss: 1114.0250\n",
      "Epoch 23/200\n",
      "276/276 [==============================] - 79s 287ms/step - loss: 1221.6782 - CHIPNexus.OCT4.logcount_loss: 0.2633 - CHIPNexus.OCT4.profile_loss: 1090.0052 - val_loss: 2158.4382 - val_CHIPNexus.OCT4.logcount_loss: 0.2507 - val_CHIPNexus.OCT4.profile_loss: 1109.1836\n",
      "Epoch 24/200\n",
      "276/276 [==============================] - 80s 289ms/step - loss: 1227.0038 - CHIPNexus.OCT4.logcount_loss: 0.2461 - CHIPNexus.OCT4.profile_loss: 1103.9565 - val_loss: 2436.6775 - val_CHIPNexus.OCT4.logcount_loss: 0.3023 - val_CHIPNexus.OCT4.profile_loss: 1148.9548\n",
      "Epoch 25/200\n",
      "276/276 [==============================] - 80s 290ms/step - loss: 1210.7295 - CHIPNexus.OCT4.logcount_loss: 0.2474 - CHIPNexus.OCT4.profile_loss: 1087.0374 - val_loss: 2214.6355 - val_CHIPNexus.OCT4.logcount_loss: 0.2423 - val_CHIPNexus.OCT4.profile_loss: 1099.7786\n",
      "Epoch 26/200\n",
      "276/276 [==============================] - 84s 306ms/step - loss: 1200.8359 - CHIPNexus.OCT4.logcount_loss: 0.2398 - CHIPNexus.OCT4.profile_loss: 1080.9417 - val_loss: 2282.4277 - val_CHIPNexus.OCT4.logcount_loss: 0.2523 - val_CHIPNexus.OCT4.profile_loss: 1118.1364\n",
      "Epoch 27/200\n",
      "276/276 [==============================] - 85s 309ms/step - loss: 1191.6385 - CHIPNexus.OCT4.logcount_loss: 0.2357 - CHIPNexus.OCT4.profile_loss: 1073.7740 - val_loss: 2172.0732 - val_CHIPNexus.OCT4.logcount_loss: 0.2462 - val_CHIPNexus.OCT4.profile_loss: 1105.7836\n",
      "Epoch 28/200\n",
      "276/276 [==============================] - 81s 295ms/step - loss: 1208.7942 - CHIPNexus.OCT4.logcount_loss: 0.2387 - CHIPNexus.OCT4.profile_loss: 1089.4603 - val_loss: 2126.5942 - val_CHIPNexus.OCT4.logcount_loss: 0.2446 - val_CHIPNexus.OCT4.profile_loss: 1104.1541\n",
      "Epoch 29/200\n",
      "276/276 [==============================] - 82s 297ms/step - loss: 1191.5830 - CHIPNexus.OCT4.logcount_loss: 0.2396 - CHIPNexus.OCT4.profile_loss: 1071.7629 - val_loss: 2148.7627 - val_CHIPNexus.OCT4.logcount_loss: 0.2348 - val_CHIPNexus.OCT4.profile_loss: 1087.6825\n",
      "Epoch 30/200\n",
      "276/276 [==============================] - 82s 296ms/step - loss: 1184.5766 - CHIPNexus.OCT4.logcount_loss: 0.2335 - CHIPNexus.OCT4.profile_loss: 1067.8245 - val_loss: 2171.9961 - val_CHIPNexus.OCT4.logcount_loss: 0.2331 - val_CHIPNexus.OCT4.profile_loss: 1088.0851\n",
      "Epoch 31/200\n",
      "276/276 [==============================] - 82s 299ms/step - loss: 1167.4846 - CHIPNexus.OCT4.logcount_loss: 0.2309 - CHIPNexus.OCT4.profile_loss: 1052.0337 - val_loss: 2211.7883 - val_CHIPNexus.OCT4.logcount_loss: 0.2382 - val_CHIPNexus.OCT4.profile_loss: 1090.0375\n",
      "Epoch 32/200\n",
      "276/276 [==============================] - 83s 300ms/step - loss: 1167.2102 - CHIPNexus.OCT4.logcount_loss: 0.2329 - CHIPNexus.OCT4.profile_loss: 1050.7579 - val_loss: 2209.3608 - val_CHIPNexus.OCT4.logcount_loss: 0.2430 - val_CHIPNexus.OCT4.profile_loss: 1098.0964\n",
      "val_loss 2078.823486328125\n",
      "val_CHIPNexus.OCT4.logcount_loss 0.27910760045051575\n",
      "val_CHIPNexus.OCT4.profile_loss 1114.0250244140625\n",
      "loss 1224.1040856966083\n",
      "CHIPNexus.OCT4.logcount_loss 0.25138164\n",
      "CHIPNexus.OCT4.profile_loss 1098.3799\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "for i in range(5):\n",
    "    for c_task_weight in [0,125,250,500]:\n",
    "        counts_loss, log_space_preds, c_task_weight = (\"mse\", True, c_task_weight)\n",
    "        #counts_loss, log_space_preds, c_task_weight = (poisson_loglinear_loss, True, 1)\n",
    "        seed = 1234 + i*100\n",
    "        print(\"Seed:\",seed)\n",
    "        modelwrapper = BPnetArch(\n",
    "            counts_loss=counts_loss,\n",
    "            log_space_preds=log_space_preds,\n",
    "            input_seq_len=seq_len,\n",
    "            counts_hiddenlayer_size=None,\n",
    "            num_counts_hidden_layers=0,\n",
    "            c_task_weight=c_task_weight,\n",
    "            filters=64, n_dil_layers=6,\n",
    "            conv1_kernel_size=21,\n",
    "            dil_kernel_size=3,\n",
    "            outconv_kernel_size=75,\n",
    "            lr=0.001,\n",
    "            seed=1234*i)\n",
    "\n",
    "        \"\"\"class PlotScatter(keras.callbacks.Callback):\n",
    "\n",
    "            def __init__(self, takelogofpreds):\n",
    "                self.takelogofpreds = takelogofpreds\n",
    "\n",
    "            def on_epoch_begin(self, epoch, logs=None):\n",
    "                print(\"Scatterplots before epoch\",epoch)\n",
    "                valid_preds = dict(zip(model.output_names, model.predict(valid_concat_inputs)))\n",
    "                plot_scatter(preds=valid_preds, targets=valid_concat_targets,\n",
    "                             takelogofpreds=self.takelogofpreds,\n",
    "                             keys=['CHIPNexus.OCT4.logcount', 'CHIPNexus.SOX2.logcount',\n",
    "                                   'CHIPNexus.NANOG.logcount', 'CHIPNexus.KLF4.logcount'])\"\"\"\n",
    "\n",
    "        modelname = \"Oct4only_ctaskweight\"+str(c_task_weight)+\"_seed\"+str(seed)+\".h5\"\n",
    "        print(modelname)\n",
    "        model = modelwrapper.get_keras_model()\n",
    "        print(model.summary())\n",
    "        early_stopping_callback = keras.callbacks.EarlyStopping(\n",
    "                                    patience=10,\n",
    "                                    restore_best_weights=True)\n",
    "        loss_history = model.fit_generator(\n",
    "                            keras_train_batch_generator,\n",
    "                            epochs=200,\n",
    "                            validation_data=keras_valid_batch_generator,\n",
    "                            callbacks=[early_stopping_callback,\n",
    "                                       #PlotScatter(takelogofpreds=(log_space_preds==False))\n",
    "                                      ],\n",
    "                            workers=1)\n",
    "        best_epoch = np.argmin(loss_history.history['val_loss'])\n",
    "        for key in loss_history.history.keys():\n",
    "            print(key,loss_history.history[key][best_epoch])\n",
    "        model.set_weights(early_stopping_callback.best_weights)\n",
    "        model.save(modelname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
